{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.2.16 in ./.conda/lib/python3.12/site-packages (0.2.16)\n",
      "Requirement already satisfied: langchain_core==0.2.38 in ./.conda/lib/python3.12/site-packages (0.2.38)\n",
      "Requirement already satisfied: langchain_community==0.2.16 in ./.conda/lib/python3.12/site-packages (0.2.16)\n",
      "Requirement already satisfied: pymupdf in ./.conda/lib/python3.12/site-packages (1.24.10)\n",
      "Requirement already satisfied: openai in ./.conda/lib/python3.12/site-packages (1.46.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.12/site-packages (from langchain==0.2.16) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.conda/lib/python3.12/site-packages (from langchain==0.2.16) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.conda/lib/python3.12/site-packages (from langchain==0.2.16) (3.10.5)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.conda/lib/python3.12/site-packages (from langchain==0.2.16) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.conda/lib/python3.12/site-packages (from langchain==0.2.16) (0.1.121)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in ./.conda/lib/python3.12/site-packages (from langchain==0.2.16) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.conda/lib/python3.12/site-packages (from langchain==0.2.16) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.conda/lib/python3.12/site-packages (from langchain==0.2.16) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.conda/lib/python3.12/site-packages (from langchain==0.2.16) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.12/site-packages (from langchain_core==0.2.38) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/lib/python3.12/site-packages (from langchain_core==0.2.38) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.conda/lib/python3.12/site-packages (from langchain_core==0.2.38) (4.12.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.conda/lib/python3.12/site-packages (from langchain_community==0.2.16) (0.6.7)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.10 in ./.conda/lib/python3.12/site-packages (from pymupdf) (1.24.10)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.conda/lib/python3.12/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.conda/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/lib/python3.12/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.conda/lib/python3.12/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in ./.conda/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.conda/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (1.11.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.conda/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.conda/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.2.16) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.conda/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.2.16) (0.9.0)\n",
      "Requirement already satisfied: certifi in ./.conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain_core==0.2.38) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.16) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.conda/lib/python3.12/site-packages (from pydantic<3,>=1->langchain==0.2.16) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./.conda/lib/python3.12/site-packages (from pydantic<3,>=1->langchain==0.2.16) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.2.16) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.2.16) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.conda/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.16) (3.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.conda/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community==0.2.16) (1.0.0)\n",
      "Collecting langchain_openai==0.1.25\n",
      "  Downloading langchain_openai-0.1.25-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-qdrant in ./.conda/lib/python3.12/site-packages (0.1.4)\n",
      "Requirement already satisfied: qdrant_client in ./.conda/lib/python3.12/site-packages (1.11.2)\n",
      "Requirement already satisfied: asyncio in ./.conda/lib/python3.12/site-packages (3.4.3)\n",
      "Collecting ragas==0.1.14\n",
      "  Downloading ragas-0.1.14-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.40 (from langchain_openai==0.1.25)\n",
      "  Downloading langchain_core-0.2.40-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in ./.conda/lib/python3.12/site-packages (from langchain_openai==0.1.25) (1.46.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./.conda/lib/python3.12/site-packages (from langchain_openai==0.1.25) (0.7.0)\n",
      "Requirement already satisfied: numpy in ./.conda/lib/python3.12/site-packages (from ragas==0.1.14) (1.26.4)\n",
      "Requirement already satisfied: datasets in ./.conda/lib/python3.12/site-packages (from ragas==0.1.14) (3.0.0)\n",
      "Requirement already satisfied: langchain in ./.conda/lib/python3.12/site-packages (from ragas==0.1.14) (0.2.16)\n",
      "Requirement already satisfied: langchain-community in ./.conda/lib/python3.12/site-packages (from ragas==0.1.14) (0.2.16)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in ./.conda/lib/python3.12/site-packages (from ragas==0.1.14) (0.3.4)\n",
      "Requirement already satisfied: nest-asyncio in ./.conda/lib/python3.12/site-packages (from ragas==0.1.14) (1.6.0)\n",
      "Requirement already satisfied: appdirs in ./.conda/lib/python3.12/site-packages (from ragas==0.1.14) (1.4.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.conda/lib/python3.12/site-packages (from langchain-qdrant) (2.9.2)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in ./.conda/lib/python3.12/site-packages (from qdrant_client) (1.66.1)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in ./.conda/lib/python3.12/site-packages (from qdrant_client) (1.66.1)\n",
      "Requirement already satisfied: httpx>=0.20.0 in ./.conda/lib/python3.12/site-packages (from httpx[http2]>=0.20.0->qdrant_client) (0.27.2)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in ./.conda/lib/python3.12/site-packages (from qdrant_client) (2.10.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in ./.conda/lib/python3.12/site-packages (from qdrant_client) (2.2.3)\n",
      "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in ./.conda/lib/python3.12/site-packages (from grpcio-tools>=1.41.0->qdrant_client) (5.28.1)\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.12/site-packages (from grpcio-tools>=1.41.0->qdrant_client) (72.1.0)\n",
      "Requirement already satisfied: anyio in ./.conda/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (4.4.0)\n",
      "Requirement already satisfied: certifi in ./.conda/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.0.5)\n",
      "Requirement already satisfied: idna in ./.conda/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (3.10)\n",
      "Requirement already satisfied: sniffio in ./.conda/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in ./.conda/lib/python3.12/site-packages (from httpx[http2]>=0.20.0->qdrant_client) (4.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.40->langchain_openai==0.1.25) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.40->langchain_openai==0.1.25) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in ./.conda/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.40->langchain_openai==0.1.25) (0.1.121)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.conda/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.40->langchain_openai==0.1.25) (24.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.conda/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.40->langchain_openai==0.1.25) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.conda/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.40->langchain_openai==0.1.25) (4.12.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.conda/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai==0.1.25) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.conda/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai==0.1.25) (0.5.0)\n",
      "Requirement already satisfied: tqdm>4 in ./.conda/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai==0.1.25) (4.66.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-qdrant) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./.conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-qdrant) (2.23.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.conda/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain_openai==0.1.25) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.conda/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain_openai==0.1.25) (2.32.3)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.12/site-packages (from datasets->ragas==0.1.14) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.conda/lib/python3.12/site-packages (from datasets->ragas==0.1.14) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.conda/lib/python3.12/site-packages (from datasets->ragas==0.1.14) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.conda/lib/python3.12/site-packages (from datasets->ragas==0.1.14) (2.2.2)\n",
      "Requirement already satisfied: xxhash in ./.conda/lib/python3.12/site-packages (from datasets->ragas==0.1.14) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./.conda/lib/python3.12/site-packages (from datasets->ragas==0.1.14) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in ./.conda/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->ragas==0.1.14) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in ./.conda/lib/python3.12/site-packages (from datasets->ragas==0.1.14) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in ./.conda/lib/python3.12/site-packages (from datasets->ragas==0.1.14) (0.25.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.conda/lib/python3.12/site-packages (from langchain->ragas==0.1.14) (2.0.35)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.conda/lib/python3.12/site-packages (from langchain->ragas==0.1.14) (0.2.4)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.conda/lib/python3.12/site-packages (from langchain-community->ragas==0.1.14) (0.6.7)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.conda/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.1.14) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.1.14) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.1.14) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.1.14) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.1.14) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.12/site-packages (from aiohttp->datasets->ragas==0.1.14) (1.11.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.conda/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.1.14) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.conda/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.1.14) (0.9.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in ./.conda/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in ./.conda/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client) (4.0.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.40->langchain_openai==0.1.25) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.40->langchain_openai==0.1.25) (3.10.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai==0.1.25) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.conda/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas==0.1.14) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.12/site-packages (from pandas->datasets->ragas==0.1.14) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.12/site-packages (from pandas->datasets->ragas==0.1.14) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.conda/lib/python3.12/site-packages (from pandas->datasets->ragas==0.1.14) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas==0.1.14) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.conda/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.1.14) (1.0.0)\n",
      "Downloading langchain_openai-0.1.25-py3-none-any.whl (51 kB)\n",
      "Downloading ragas-0.1.14-py3-none-any.whl (163 kB)\n",
      "Downloading langchain_core-0.2.40-py3-none-any.whl (396 kB)\n",
      "Installing collected packages: langchain-core, langchain_openai, ragas\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.38\n",
      "    Uninstalling langchain-core-0.2.38:\n",
      "      Successfully uninstalled langchain-core-0.2.38\n",
      "  Attempting uninstall: langchain_openai\n",
      "    Found existing installation: langchain-openai 0.2.0\n",
      "    Uninstalling langchain-openai-0.2.0:\n",
      "      Successfully uninstalled langchain-openai-0.2.0\n",
      "  Attempting uninstall: ragas\n",
      "    Found existing installation: ragas 0.1.19\n",
      "    Uninstalling ragas-0.1.19:\n",
      "      Successfully uninstalled ragas-0.1.19\n",
      "Successfully installed langchain-core-0.2.40 langchain_openai-0.1.25 ragas-0.1.14\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.2.16 langchain_core==0.2.38 langchain_community==0.2.16 pymupdf openai \n",
    "!pip install langchain_openai==0.1.25 langchain-qdrant qdrant_client asyncio ragas==0.1.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Download and chunk the data**\n",
    "\n",
    "We are going to use the following docs as our knowledge base:\n",
    "1. Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People (PDF)\n",
    "2. National Institute of Standards and Technology (NIST) Artificial Intelligent Risk Management Framework \n",
    "\n",
    "Let's start with a simple fixed chunking strategy as a baseline, and later evaluate parent-doc retrieval if we have time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "CHUNK_SIZE = 1500\n",
    "OVERLAP = 150\n",
    "\n",
    "RAGAS_CHUNK_SIZE = 750\n",
    "RAGAS_OVERLAP = 75\n",
    "\n",
    "GENERATOR_LLM = \"gpt-4o-mini-2024-07-18\"\n",
    "CRITIC_LLM = \"gpt-4o-2024-08-06\"\n",
    "\n",
    "N_EVAL_QUESTIONS = 50\n",
    "\n",
    "PDFS = [\n",
    "    \"https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf\",\n",
    "    \"https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "# collect OpenAI key\n",
    "openai.api_key = getpass(\"OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf...\n",
      "Chunking...\n",
      "Loading https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf...\n",
      "Chunking...\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import vanilla_rag\n",
    "\n",
    "importlib.reload(vanilla_rag)\n",
    "for pdf in PDFS:\n",
    "    chunks = await vanilla_rag.load_and_chunk_pdf(pdf,CHUNK_SIZE,OVERLAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='GAI resources; Apply organizational risk tolerances to ﬁne-tuned third-party \n",
      "models; Apply organizational risk tolerance to existing third-party models \n",
      "adapted to a new domain; Reassess risk measurements after ﬁne-tuning third-\n",
      "party GAI models. \n",
      "Value Chain and Component \n",
      "Integration; Intellectual Property \n",
      "MG-3.1-002 \n",
      "Test GAI system value chain risks (e.g., data poisoning, malware, other software \n",
      "and hardware vulnerabilities; labor practices; data privacy and localization \n",
      "compliance; geopolitical alignment). \n",
      "Data Privacy; Information Security; \n",
      "Value Chain and Component \n",
      "Integration; Harmful Bias and \n",
      "Homogenization \n",
      "MG-3.1-003 \n",
      "Re-assess model risks after ﬁne-tuning or retrieval-augmented generation \n",
      "implementation and for any third-party GAI models deployed for applications \n",
      "and/or use cases that were not evaluated in initial testing. \n",
      "Value Chain and Component \n",
      "Integration \n",
      "MG-3.1-004 \n",
      "Take reasonable measures to review training data for CBRN information, and \n",
      "intellectual property, and where appropriate, remove it. Implement reasonable \n",
      "measures to prevent, ﬂag, or take other action in response to outputs that \n",
      "reproduce particular training data (e.g., plagiarized, trademarked, patented, \n",
      "licensed content or trade secret material). \n",
      "Intellectual Property; CBRN \n",
      "Information or Capabilities'\n"
     ]
    }
   ],
   "source": [
    "print(chunks[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Basic RAG Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created qdrant client\n",
      "populated vector db\n",
      "created chain\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(vanilla_rag)\n",
    "rag_chain = await vanilla_rag.vanilla_rag_chain(chunks, openai.api_key, \"AI-Risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': [Document(metadata={'_id': 'f9c13262335345adba20b565821c7ce7', '_collection_name': 'AI-Risk'}, page_content='with greater ease and scale than other technologies. LLMs have been reported to generate dangerous or \\nviolent recommendations, and some models have generated actionable instructions for dangerous or \\n \\n \\n9 Confabulations of falsehoods are most commonly a problem for text-based outputs; for audio, image, or video \\ncontent, creative generation of non-factual content can be a desired behavior.  \\n10 For example, legal confabulations have been shown to be pervasive in current state-of-the-art LLMs. See also, \\ne.g.,'),\n",
      "             Document(metadata={'_id': '874f59b2faa24d92bec35afe72b262c3', '_collection_name': 'AI-Risk'}, page_content='development, production, or use of CBRN weapons or other dangerous materials or agents. While \\nrelevant biological and chemical threat knowledge and information is often publicly accessible, LLMs \\ncould facilitate its analysis or synthesis, particularly by individuals without formal scientiﬁc training or \\nexpertise.  \\nRecent research on this topic found that LLM outputs regarding biological threat creation and attack \\nplanning provided minimal assistance beyond traditional search engine queries, suggesting that state-of-\\nthe-art LLMs at the time these studies were conducted do not substantially increase the operational \\nlikelihood of such an attack. The physical synthesis development, production, and use of chemical or \\nbiological agents will continue to require both applicable expertise and supporting materials and \\ninfrastructure. The impact of GAI on chemical or biological agent misuse will depend on what the key \\nbarriers for malicious actors are (e.g., whether information access is one such barrier), and how well GAI \\ncan help actors address those barriers.  \\nFurthermore, chemical and biological design tools (BDTs) – highly specialized AI systems trained on \\nscientiﬁc data that aid in chemical and biological design – may augment design capabilities in chemistry \\nand biology beyond what text-based LLMs are able to provide. As these models become more \\neﬃcacious, including for beneﬁcial uses, it will be important to assess their potential to be used for'),\n",
      "             Document(metadata={'_id': '8209a46c5a8e429893b150f051edf797', '_collection_name': 'AI-Risk'}, page_content='information reports could cause doctors to make incorrect diagnoses and/or recommend the wrong \\ntreatments. Risks of confabulated content may be especially important to monitor when integrating GAI \\ninto applications involving consequential decision making. \\nGAI outputs may also include confabulated logic or citations that purport to justify or explain the \\nsystem’s answer, which may further mislead humans into inappropriately trusting the system’s output. \\nFor instance, LLMs sometimes provide logical steps for how they arrived at an answer even when the \\nanswer itself is incorrect. Similarly, an LLM could falsely assert that it is human or has human traits, \\npotentially deceiving humans into believing they are speaking with another human. \\nThe extent to which humans can be deceived by LLMs, the mechanisms by which this may occur, and the \\npotential risks from adversarial prompting of such behavior are emerging areas of study. Given the wide \\nrange of downstream impacts of GAI, it is diﬃcult to estimate the downstream scale and impact of \\nconfabulations. \\nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Valid and Reliable, Explainable \\nand Interpretable \\n2.3. Dangerous, Violent, or Hateful Content \\nGAI systems can produce content that is inciting, radicalizing, or threatening, or that gloriﬁes violence, \\nwith greater ease and scale than other technologies. LLMs have been reported to generate dangerous or'),\n",
      "             Document(metadata={'_id': '6cf9f977a6844ea2824083c03224d0e7', '_collection_name': 'AI-Risk'}, page_content=\"Models. Project Zero. https://googleprojectzero.blogspot.com/2024/06/project-naptime.html \\nGreshake, K. et al. (2023) Not what you've signed up for: Compromising Real-World LLM-Integrated \\nApplications with Indirect Prompt Injection. arXiv. https://arxiv.org/abs/2302.12173 \\nHagan, M. (2024) Good AI Legal Help, Bad AI Legal Help: Establishing quality standards for responses to \\npeople’s legal problem stories. SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4696936 \\nHaran, R. (2023) Securing LLM Systems Against Prompt Injection. NVIDIA. \\nhttps://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/ \\nInformation Technology Industry Council (2024) Authenticating AI-Generated Content. \\nhttps://www.itic.org/policy/ITI_AIContentAuthorizationPolicy_122123.pdf \\nJain, S. et al. (2023) Algorithmic Pluralism: A Structural Approach To Equal Opportunity. arXiv. \\nhttps://arxiv.org/pdf/2305.08157 \\nJi, Z. et al (2023) Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55, 12, \\nArticle 248. https://doi.org/10.1145/3571730 \\nJones-Jang, S. et al. (2022) How do people react to AI failure? Automation bias, algorithmic aversion, and \\nperceived controllability. Oxford. https://academic.oup.com/jcmc/article/28/1/zmac029/6827859] \\nJussupow, E. et al. (2020) Why Are We Averse Towards Algorithms? A Comprehensive Literature Review \\non Algorithm Aversion. ECIS 2020. https://aisel.aisnet.org/ecis2020_rp/168/\")],\n",
      " 'response': AIMessage(content='Some key risks associated with modern LLMs include:\\n\\n1. **Generation of Dangerous Content**: LLMs can produce dangerous, violent, or hateful content, potentially inciting or radicalizing individuals.\\n\\n2. **Confabulation of Falsehoods**: LLMs may generate incorrect information or logical steps that mislead users, leading to incorrect diagnoses in medical contexts or other consequential decision-making scenarios.\\n\\n3. **Deception**: LLMs can falsely assert human-like traits, potentially deceiving users into believing they are interacting with a human.\\n\\n4. **Facilitation of Malicious Activities**: LLMs could assist individuals without formal training in analyzing or synthesizing dangerous knowledge, such as information related to chemical or biological threats.\\n\\n5. **Trust Issues**: The outputs of LLMs may include misleading citations or logic, which can lead users to inappropriately trust incorrect information.\\n\\nThese risks highlight the importance of monitoring and managing the outputs of LLMs, especially in sensitive applications.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 204, 'prompt_tokens': 1376, 'total_tokens': 1580, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-1ecc68a2-2a97-4421-aed1-dc3703637bb6-0', usage_metadata={'input_tokens': 1376, 'output_tokens': 204, 'total_tokens': 1580})}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "response = await rag_chain.ainvoke({\"input\":\"What are some key risks associated with modern LLMs?\"})\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Generate synthetic data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/Desktop/ai_makerspace/code/ai-risk-bot/.conda/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAIBase has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/Angela/Desktop/ai_makerspace/code/ai-risk-bot/.conda/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in _VertexAICommon has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Users/Angela/Desktop/ai_makerspace/code/ai-risk-bot/.conda/lib/python3.12/site-packages/ragas/metrics/__init__.py:4: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._answer_correctness import AnswerCorrectness, answer_correctness\n",
      "/Users/Angela/Desktop/ai_makerspace/code/ai-risk-bot/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/Angela/Desktop/ai_makerspace/code/ai-risk-bot/.conda/lib/python3.12/site-packages/ragas/metrics/__init__.py:8: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._context_entities_recall import (\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context, conditional\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "generator_llm = ChatOpenAI(model=GENERATOR_LLM)\n",
    "critic_llm = ChatOpenAI(model=CRITIC_LLM)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "distributions = {\n",
    "    simple: 0.5,\n",
    "    multi_context: 0.3,\n",
    "    reasoning: 0.1,\n",
    "    conditional: 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                   \r"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m testset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EVAL_QUESTIONS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_debugging_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ai_makerspace/code/ai-risk-bot/.conda/lib/python3.12/site-packages/ragas/testset/generator.py:206\u001b[0m, in \u001b[0;36mTestsetGenerator.generate_with_langchain_docs\u001b[0;34m(self, documents, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[1;32m    204\u001b[0m distributions \u001b[38;5;241m=\u001b[39m distributions \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# chunk documents and add to docstore\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_langchain_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    211\u001b[0m     test_size\u001b[38;5;241m=\u001b[39mtest_size,\n\u001b[1;32m    212\u001b[0m     distributions\u001b[38;5;241m=\u001b[39mdistributions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[1;32m    217\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/ai_makerspace/code/ai-risk-bot/.conda/lib/python3.12/site-packages/ragas/testset/docstore.py:214\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_documents\u001b[0;34m(self, docs, show_progress)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# split documents with self.splitter into smaller nodes\u001b[39;00m\n\u001b[1;32m    210\u001b[0m nodes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    211\u001b[0m     Node\u001b[38;5;241m.\u001b[39mfrom_langchain_document(d)\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39mtransform_documents(docs)\n\u001b[1;32m    213\u001b[0m ]\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ai_makerspace/code/ai-risk-bot/.conda/lib/python3.12/site-packages/ragas/testset/docstore.py:253\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_nodes\u001b[0;34m(self, nodes, show_progress)\u001b[0m\n\u001b[1;32m    251\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(nodes):\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m nodes_to_embed\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[0;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "importlib.reload(vanilla_rag)\n",
    "for pdf in PDFS:\n",
    "    ragas_chunks = await vanilla_rag.load_and_chunk_pdf(pdf,RAGAS_CHUNK_SIZE,RAGAS_OVERLAP)\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(ragas_chunks, N_EVAL_QUESTIONS, distributions, with_debugging_logs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
