{"questions": {"4e681090-8acf-4720-8788-dd338bd99391": "What criteria do the reporting channels use to decide which incidents to track?", "1d728082-4e33-4ba0-b779-999631b93fce": "How do publicly available databases document the occurrence of incidents?", "2459776d-4e76-4bba-bda7-842682878624": "How can documenting and reporting GAI incidents assist AI Actors in tracing impacts to their source?", "90209cd1-db62-49ef-b67c-b166a0ad205f": "What benefits could arise from greater awareness and standardization of GAI incident reporting in the AI ecosystem?", "4d152e90-377c-4096-8011-23b16b5fabe4": "What should AI Actors be aware of regarding their roles in reporting AI incidents?", "b8182f2a-a11d-4de2-9e41-f6e0a0a80145": "How can organizations develop guidelines to improve incident reporting for AI systems?", "7829faad-3281-4543-a7e8-facd04ccff12": "Why is documentation and review of third-party inputs and plugins important for AI Actors in the context of incident disclosure?", "57f98a14-6ac7-4406-84a8-01eeba4ff9c5": "How can logging, recording, and analyzing GAI incidents facilitate better practices for AI Actors?", "ed8db932-c2ec-4e2a-8a6f-17e2ea8ba57b": "How can regular information sharing enhance the effectiveness of AI Actors in managing incidents?", "b51ed897-601c-4e7a-a21b-5532a016b288": "What role do change management records and version history play in supporting AI Actors?", "50c291b9-6bfa-43a4-915a-9283ce0cc520": "What is the title of the paper by D Acemoglu published in 2024 regarding the macroeconomics of AI?", "fe94365e-298d-4851-9e32-fbf6b3478a24": "What topic does the 2024 survey by D Atherton focus on in relation to deepfakes?", "e8fecbf7-d183-41ed-afce-9a9ac66915a9": "What is the main focus of the blog post titled \"Bing Chat: Data Exfiltration Exploit Explained\"?", "3b2effc1-37d7-4632-891f-45eab9f4e11d": "How do the studies by Bommasani et al and Boyarskaya et al contribute to the understanding of AI system development and its challenges?", "f7183212-3bbf-4801-9583-04f741074c7b": "What are the main security concerns discussed in Browne et al (2023) regarding the AI pipeline?", "eb9cc74f-3546-4191-a9ca-1a52fb68ce7f": "According to Burgess (2024), what is identified as the biggest security flaw in generative AI?", "6c06db88-2219-4fe3-afea-d80edd5b5695": "What is the main focus of the article from the Canadian Centre for Cyber Security regarding generative artificial intelligence?", "f954916e-a75c-4d3a-9454-24a59c9eef0b": "Who are the authors of the paper discussing the extraction of training data from large language models, and where was it presented?", "c0e7d3c6-b2e7-443d-adce-8abdae8368a7": "What is the main focus of the research conducted by Carlini et al in their 2023 paper on neural language models?", "cf5d8c14-564c-4ac7-8db9-7621a3d8ee48": "What topic do Chandra et al address in their 2023 commentary published by RAND?", "1bfe7fa9-6fb3-4958-8b58-c548744eae19": "What are the ethical tensions discussed in Ciriello et al's inquiry into human-AI companionship with Replika?", "27291e37-f0a4-4941-9b0a-0bed5ae864ea": "What issues related to legal hallucinations in large language models are profiled in Dahl et al's research?", "5cdfeca9-52b4-41e9-aa77-20589eccc96b": "What are the short, mid, and long-term impacts of AI in cybersecurity as discussed by De Angelo in 2024?", "4f75eb42-e32e-4b27-a15e-130645249ffe": "What insights does the Harvard Business School publication by De Freitas et al provide regarding the safety of generative AI in relation to mental health?", "d2400e04-2b00-469d-a5fa-b2ad3f6503ed": "What is the main focus of the study conducted by Dietvorst et al (2014) regarding algorithm aversion?", "6242a094-6a22-471e-8144-a27910353610": "How do the findings of Elsayed et al (2024) relate to the influence of altered images on both machine vision and human perception?", "5eb16e63-b9dd-4257-839c-8bd99fb861d3": "What are the implications of altered images on machine vision and human perception as discussed by DeepMind?", "28cca4ca-7add-4835-9846-6dfef9c7488d": "How does the concept of red-teaming for generative AI address security concerns, according to Feffer et al (2024)?", "926a9dfd-84ba-4086-97fc-c173a377505a": "What are the main findings discussed in the Project Zero blog post regarding AI models?", "615b98a6-50b4-4d10-ba9c-deabbbbaf24d": "How do Greshake et al (2023) address the issue of indirect prompt injection in LLM-integrated applications?", "fb82c789-c8ef-410b-8668-c21769c58544": "What are the main strategies discussed by Haran (2023) for securing LLM systems against prompt injection?", "1d6c386f-7d6b-4128-8c47-2cc057d6857e": "How does the Information Technology Industry Council (2024) propose to authenticate AI-generated content?", "588f815c-4ed4-4757-9485-4f52b0441dba": "What are the main themes discussed in the survey by Ji et al (2023) regarding hallucination in natural language generation?", "2678339c-3ee4-4faf-9107-a73bbfa7fead": "How do the findings of Jones-Jang et al (2022) contribute to our understanding of public reactions to AI failures?", "d95e0d5b-3b1f-429b-a668-0f5d7c4f76ba": "What is the main focus of the research presented in the paper \"on Algorithm Aversion\" from ECIS 2020?", "39a7eaab-6ea8-47a6-8cd4-dbfefc8d57cf": "What is the significance of the findings in the paper \"Calibrated Language Models Must Hallucinate\" by Kalai et al (2024)?", "4a281421-16c7-4e0a-8a47-89ef982a4063": "What are the main predictors of non-consensual dissemination of intimate images as discussed by Karasavva et al (2021)?", "a9f18365-0607-4e1b-be1d-5255869c3d28": "How does Katzman et al (2023) approach the issue of representational harms in image tagging?", "e0d90118-d3b9-4e71-b4a3-d2464f26ee91": "What are the key players involved in the generative AI value chain as discussed in the Partnership on AI report?", "76d9f25e-c4bb-45f6-9ff6-f72cfe055bbc": "How does the concept of algorithmic monoculture relate to social welfare according to Kleinberg et al (2021)?", "f2c25b49-78ee-468a-8641-a24a14e3df4e": "What are the main AI privacy risks discussed in Lee et al (2024)?", "e04828e9-6c38-4e7e-a03d-31cccdcc5026": "How does data poisoning exploit generative AI according to Lenaerts-Bergmans (2024)?", "d0740eb8-11ac-455f-8272-b7ef86eaab42": "What are the main concerns discussed in Luccioni et al (2023) regarding the energy consumption of AI deployment?", "e5567e40-0dad-4593-af94-819400c063de": "How do Mouton et al (2024) address the operational risks associated with AI in the context of large-scale biological attacks?", "621209dc-bb02-4668-9683-c01d61e0cab8": "What is the focus of the National Institute of Standards and Technology's 2024 publication on adversarial machine learning?", "22a1d533-a1d9-4f0f-8a73-e76514c6fb93": "Where can one find the AI Risk Management Framework published by the National Institute of Standards and Technology in 2023?", "51322c37-0bad-4adb-af97-8f1bad27db29": "What are the key characteristics of risks and trustworthiness as outlined in the AI Risk Management Framework by the National Institute of Standards and Technology?", "c6dfbb32-31c8-4a6e-877b-8cbbd9086b7d": "How does Chapter 6 of the AI Risk Management Framework define AI RMF Profiles?", "4f66bc0e-5af4-4e54-bc8d-dc30f5fb45a4": "What are the different categories of AI actors mentioned in the context?", "abb9a0dd-86f9-4c3a-a368-1b800f15deff": "How do data providers and system funders fit into the roles of AI actors?", "072b4efd-bf03-41df-afde-e036906a5dca": "What is the focus of Appendix B in the National Institute of Standards and Technology's AI Risk Management Framework?", "c69bb720-e517-4ddf-9e36-30ba2cf5ed14": "Where can one find the AI RMF Playbook published by the National Institute of Standards and Technology?", "bba19ba0-7f4d-451e-9e9b-2f3a147b9146": "What is the purpose of the National Institute of Standards and Technology's glossary on trustworthy AI?", "3d31229a-ddbb-4d64-a16b-3ba1e9d06132": "How does the 2022 publication by the National Institute of Standards and Technology address bias in artificial intelligence?", "3028f26a-572d-4bb4-8a41-4d8d73c169a5": "What are the implications of pervasive label errors in test sets for machine learning benchmarks as discussed by Northcutt et al (2021)?", "ad91b88c-2967-4d04-a635-c900b0760683": "How does the OECD (2023) propose to advance accountability in AI throughout its lifecycle?", "5be4cf03-d772-46dc-8213-85dce1457704": "What is the title of the OECD publication that discusses defining AI incidents and related terms?", "19a1455f-2960-4601-ab9f-a72b1ce4e3a0": "In which year was the GPT-4 Technical Report published by OpenAI?", "1089b096-6082-401a-ae3b-7891f59cd4fa": "What are the main examples of AI deception discussed in the survey by Park et al (2024)?", "6a029e67-3116-47be-8fb2-d5e2dbf51764": "How does the Partnership on AI propose to enhance transparency in synthetic media according to their glossary?", "6a905e30-d177-4c20-b85f-ac5a2333f856": "What are the main findings of the study by Qu, Y et al (2023) regarding the generation of unsafe images and hateful memes from text-to-image models?", "b42e4e12-c98f-4a65-8f31-30d7abd032fc": "How does the research by Rafat, K et al (2023) address the issue of carbon footprint in deep learning model compression?", "ad204d84-d115-43b4-876a-be9deba08382": "What are the main risks associated with the misuse of artificial intelligence as discussed in Sandbrink's 2023 paper?", "9481e23f-fcdb-4bc0-afc6-10afea9d3f65": "How does the context of victimization and perpetration relate to the findings presented in the Sage journal article?", "e1a5915e-ab78-47ca-955e-8f3cb9e53efd": "What is the main focus of the article by Satariano et al published in the New York Times in 2023?", "679cd9bc-4632-42c2-ab86-4e409cc40528": "What topic do Schaul et al explore in their 2024 article in the Washington Post regarding AI and chatbots?", "47ce78bb-067a-40d3-b7cf-273a9008168e": "What are the main findings of Scheurer et al (2023) regarding the behavior of large language models under pressure?", "3be9d7c4-4a92-4269-b760-bce1d390b025": "How do Shelby et al (2023) propose to address the sociotechnical harms of algorithmic systems?", "654b6a61-8ee0-4405-9752-81a84b52445c": "What is the main focus of the research conducted by Shumailov et al (2023) regarding training on generated data?", "c49f4170-f8da-4b9b-9406-3548c90953e9": "How do Smith et al (2023) explore the concepts of hallucination and confabulation in relation to large language models?", "31d521be-6139-46af-93f6-e6f6398da37a": "What are the main methods and considerations discussed in Solaiman et al (2023) regarding the release of generative AI?", "cee67e62-a320-42fa-b15e-5d4bc4e706e2": "How do large language models potentially violate privacy according to Staab et al (2023)?", "8a84ea46-6958-4792-8497-d878f6322436": "What are the main energy and policy considerations discussed by Strubell et al (2019) in relation to deep learning in NLP?", "cdcd90d0-0a71-407c-927e-97077f09f724": "How does the 2023 Executive Order from The White House address the development and use of technology?", "0a342482-d23a-476c-98ea-536ba3b48381": "What are the key priorities outlined in the White House's 2022 Roadmap for Researchers regarding information integrity?", "00eb143f-3a22-4f57-88f3-9162bc24c331": "What is the purpose of the executive order on artificial intelligence issued by the White House on October 30, 2023?", "7e510aea-7917-4fa2-8632-1bf7ae4b9dee": "What does the investigation by Thiel (2023) reveal about AI image generation models?", "9bab97d3-0317-4987-bf8f-2fad44110c1f": "Where can the findings of the investigation conducted by Thiel (2023) be accessed?", "c7562d70-f85e-4565-8379-e8b891f9cf77": "What are the main themes discussed in Tirrell's article \"Toxic Speech: Toward an Epidemiology of Discursive Harm\"?", "53ed3e26-7510-4397-a0e2-d4f10a2d1423": "How does Tufekci's work address the challenges posed by algorithmic harms beyond major tech platforms like Facebook and Google?", "dec125d0-f252-4877-9c5e-b92d69223715": "What are the main topics discussed in the AAAI/ACM Conference on AI, Ethics, and Society?", "4a152187-d63e-4564-abd2-2ee015495f3d": "How does the paper by Urbina et al (2022) address the dual use of artificial intelligence in drug discovery?", "75cd125d-9f96-416b-a999-02275e1b86d1": "What is the main focus of the dataset created by Wang et al (2023) in their research on LLMs?", "1d11ad22-aecf-4b04-b333-1ed1110d1c8f": "How does the framework proposed by Wardle et al (2017) aim to address information disorder?", "e4bbf02f-0757-4bf9-b9d3-6fcfa7b49cca": "What are the ethical and social risks associated with language models as discussed by Weidinger et al (2021)?", "ed3638c3-ea90-4554-a74c-0cf1037453e0": "How does the research by Wei et al (2024) address the issue of factuality in large language models?", "7c41722e-53a4-42fc-a979-229f95d10ed2": "What are the main risks posed by language models as discussed in Weidinger et al (2022)?", "5323daf6-dee6-497e-b7a1-9b8a0325c1b9": "How does AI disproportionately affect women according to West (2023)?", "0272580d-fda4-4b84-8664-ba1fefde77f2": "What are the findings of Yin, L et al (2024) regarding racial bias in OpenAI's GPT as a recruitment tool?", "d81968e6-b2d9-4390-8859-ee76227a7520": "What is the focus of the research conducted by Yu, Z et al (March 2024) on jailbreak prompts of large language models?", "26fae4d0-c807-4632-ae10-823fbea07750": "What is the main focus of the study conducted by Zhang, Y et al (2023) regarding people's perceptions of generative AI and human experts?", "502037aa-c3eb-4894-b33b-4e5551878da9": "How do human favoritism and bias influence the collaboration between humans and generative AI in persuasive content generation, according to the research?", "73c49a61-2dab-4b02-b587-0c43b03aa3dc": "What is the main focus of the survey conducted by Zhang et al (2023) regarding large language models?", "02e3bf5b-9cfa-453f-90fe-244de495973f": "What is the purpose of the provable robust watermarking discussed by Zhao et al (2023) in relation to AI-generated text?"}, "relevant_contexts": {"4e681090-8acf-4720-8788-dd338bd99391": ["7bf21a59-a214-4b57-b758-9a81cb46a096"], "1d728082-4e33-4ba0-b779-999631b93fce": ["7bf21a59-a214-4b57-b758-9a81cb46a096"], "2459776d-4e76-4bba-bda7-842682878624": ["00e5e3aa-0530-444e-bc68-21290b2f473f"], "90209cd1-db62-49ef-b67c-b166a0ad205f": ["00e5e3aa-0530-444e-bc68-21290b2f473f"], "4d152e90-377c-4096-8011-23b16b5fabe4": ["90f15886-71fb-4bf0-850d-fad3f5dddc08"], "b8182f2a-a11d-4de2-9e41-f6e0a0a80145": ["90f15886-71fb-4bf0-850d-fad3f5dddc08"], "7829faad-3281-4543-a7e8-facd04ccff12": ["a54b205a-b8a9-4d2d-850a-ec792bb35a65"], "57f98a14-6ac7-4406-84a8-01eeba4ff9c5": ["a54b205a-b8a9-4d2d-850a-ec792bb35a65"], "ed8db932-c2ec-4e2a-8a6f-17e2ea8ba57b": ["d2f531cc-65e4-4526-9697-8261c4a75e34"], "b51ed897-601c-4e7a-a21b-5532a016b288": ["d2f531cc-65e4-4526-9697-8261c4a75e34"], "50c291b9-6bfa-43a4-915a-9283ce0cc520": ["02fa2e5c-6b97-4fb0-8ef3-a287958193ee"], "fe94365e-298d-4851-9e32-fbf6b3478a24": ["02fa2e5c-6b97-4fb0-8ef3-a287958193ee"], "e8fecbf7-d183-41ed-afce-9a9ac66915a9": ["dc0298fb-12d0-4091-9392-4b6c9e3a3c22"], "3b2effc1-37d7-4632-891f-45eab9f4e11d": ["dc0298fb-12d0-4091-9392-4b6c9e3a3c22"], "f7183212-3bbf-4801-9583-04f741074c7b": ["d005b361-a80b-4130-bc8d-ba5b2d521142"], "eb9cc74f-3546-4191-a9ca-1a52fb68ce7f": ["d005b361-a80b-4130-bc8d-ba5b2d521142"], "6c06db88-2219-4fe3-afea-d80edd5b5695": ["21423402-6662-42d1-a226-a970208a9a74"], "f954916e-a75c-4d3a-9454-24a59c9eef0b": ["21423402-6662-42d1-a226-a970208a9a74"], "c0e7d3c6-b2e7-443d-adce-8abdae8368a7": ["a97f9981-e912-4913-92a0-dd6e07007780"], "cf5d8c14-564c-4ac7-8db9-7621a3d8ee48": ["a97f9981-e912-4913-92a0-dd6e07007780"], "1bfe7fa9-6fb3-4958-8b58-c548744eae19": ["b3752ae1-c70b-46bb-893c-a55dd2529bbf"], "27291e37-f0a4-4941-9b0a-0bed5ae864ea": ["b3752ae1-c70b-46bb-893c-a55dd2529bbf"], "5cdfeca9-52b4-41e9-aa77-20589eccc96b": ["c4e9d43d-c5d6-4ae7-9f38-7d5a10ff32e0"], "4f75eb42-e32e-4b27-a15e-130645249ffe": ["c4e9d43d-c5d6-4ae7-9f38-7d5a10ff32e0"], "d2400e04-2b00-469d-a5fa-b2ad3f6503ed": ["99a3e49a-6655-4d35-8c91-a0ee1f0fb757"], "6242a094-6a22-471e-8144-a27910353610": ["99a3e49a-6655-4d35-8c91-a0ee1f0fb757"], "5eb16e63-b9dd-4257-839c-8bd99fb861d3": ["1a722ad4-c18f-46fc-bef0-be8c7fcdfd54"], "28cca4ca-7add-4835-9846-6dfef9c7488d": ["1a722ad4-c18f-46fc-bef0-be8c7fcdfd54"], "926a9dfd-84ba-4086-97fc-c173a377505a": ["320d8bb0-f2aa-4b65-ba24-8200f0dbd466"], "615b98a6-50b4-4d10-ba9c-deabbbbaf24d": ["320d8bb0-f2aa-4b65-ba24-8200f0dbd466"], "fb82c789-c8ef-410b-8668-c21769c58544": ["231301f5-3b8b-4f24-97d7-fe1b5888f914"], "1d6c386f-7d6b-4128-8c47-2cc057d6857e": ["231301f5-3b8b-4f24-97d7-fe1b5888f914"], "588f815c-4ed4-4757-9485-4f52b0441dba": ["f5c2f3fe-8330-4a97-8831-5393337edad3"], "2678339c-3ee4-4faf-9107-a73bbfa7fead": ["f5c2f3fe-8330-4a97-8831-5393337edad3"], "d95e0d5b-3b1f-429b-a668-0f5d7c4f76ba": ["2542ba17-1d49-405e-9ec7-f7bb9f3e0bf4"], "39a7eaab-6ea8-47a6-8cd4-dbfefc8d57cf": ["2542ba17-1d49-405e-9ec7-f7bb9f3e0bf4"], "4a281421-16c7-4e0a-8a47-89ef982a4063": ["b94361cb-b9b0-4176-bb33-2b12f49aa257"], "a9f18365-0607-4e1b-be1d-5255869c3d28": ["b94361cb-b9b0-4176-bb33-2b12f49aa257"], "e0d90118-d3b9-4e71-b4a3-d2464f26ee91": ["ff4345e7-16f8-4223-aa4b-4b93bfeba808"], "76d9f25e-c4bb-45f6-9ff6-f72cfe055bbc": ["ff4345e7-16f8-4223-aa4b-4b93bfeba808"], "f2c25b49-78ee-468a-8641-a24a14e3df4e": ["d46f31b1-4e8a-44b7-a120-cf14383211ba"], "e04828e9-6c38-4e7e-a03d-31cccdcc5026": ["d46f31b1-4e8a-44b7-a120-cf14383211ba"], "d0740eb8-11ac-455f-8272-b7ef86eaab42": ["67989833-5934-4ec2-87c0-0b28b6366200"], "e5567e40-0dad-4593-af94-819400c063de": ["67989833-5934-4ec2-87c0-0b28b6366200"], "621209dc-bb02-4668-9683-c01d61e0cab8": ["1fe4515f-361f-425c-81a0-a86ca1b2f0df"], "22a1d533-a1d9-4f0f-8a73-e76514c6fb93": ["1fe4515f-361f-425c-81a0-a86ca1b2f0df"], "51322c37-0bad-4adb-af97-8f1bad27db29": ["cbc7c103-c538-4d4e-a32c-8140905bbabb"], "c6dfbb32-31c8-4a6e-877b-8cbbd9086b7d": ["cbc7c103-c538-4d4e-a32c-8140905bbabb"], "4f66bc0e-5af4-4e54-bc8d-dc30f5fb45a4": ["5fd45d7c-b9bf-4348-8b27-686a58d5d1ac"], "abb9a0dd-86f9-4c3a-a368-1b800f15deff": ["5fd45d7c-b9bf-4348-8b27-686a58d5d1ac"], "072b4efd-bf03-41df-afde-e036906a5dca": ["6ae40f2e-b16d-4db4-9b4d-92dccd0f59cc"], "c69bb720-e517-4ddf-9e36-30ba2cf5ed14": ["6ae40f2e-b16d-4db4-9b4d-92dccd0f59cc"], "bba19ba0-7f4d-451e-9e9b-2f3a147b9146": ["c31c1037-681a-4847-9794-f2b8e76096f4"], "3d31229a-ddbb-4d64-a16b-3ba1e9d06132": ["c31c1037-681a-4847-9794-f2b8e76096f4"], "3028f26a-572d-4bb4-8a41-4d8d73c169a5": ["fbbb88f5-1e8d-4fa7-9e74-4bb9046a3d91"], "ad91b88c-2967-4d04-a635-c900b0760683": ["fbbb88f5-1e8d-4fa7-9e74-4bb9046a3d91"], "5be4cf03-d772-46dc-8213-85dce1457704": ["2460271a-468d-4f6b-95ee-effd4bcfd4ba"], "19a1455f-2960-4601-ab9f-a72b1ce4e3a0": ["2460271a-468d-4f6b-95ee-effd4bcfd4ba"], "1089b096-6082-401a-ae3b-7891f59cd4fa": ["c1bf8c64-ba9f-4306-bf98-fcda06fe2466"], "6a029e67-3116-47be-8fb2-d5e2dbf51764": ["c1bf8c64-ba9f-4306-bf98-fcda06fe2466"], "6a905e30-d177-4c20-b85f-ac5a2333f856": ["534bf276-7310-4123-b079-63582c85f764"], "b42e4e12-c98f-4a65-8f31-30d7abd032fc": ["534bf276-7310-4123-b079-63582c85f764"], "ad204d84-d115-43b4-876a-be9deba08382": ["49f2ba7b-951c-423f-ba78-d07ae542607d"], "9481e23f-fcdb-4bc0-afc6-10afea9d3f65": ["49f2ba7b-951c-423f-ba78-d07ae542607d"], "e1a5915e-ab78-47ca-955e-8f3cb9e53efd": ["ddeda98c-90a2-4462-9f76-c0988bea0818"], "679cd9bc-4632-42c2-ab86-4e409cc40528": ["ddeda98c-90a2-4462-9f76-c0988bea0818"], "47ce78bb-067a-40d3-b7cf-273a9008168e": ["a19e73be-c909-453e-967f-014176c948de"], "3be9d7c4-4a92-4269-b760-bce1d390b025": ["a19e73be-c909-453e-967f-014176c948de"], "654b6a61-8ee0-4405-9752-81a84b52445c": ["bcb6514c-0e67-4245-9fa8-3936fabd890e"], "c49f4170-f8da-4b9b-9406-3548c90953e9": ["bcb6514c-0e67-4245-9fa8-3936fabd890e"], "31d521be-6139-46af-93f6-e6f6398da37a": ["9a0b0b72-fb34-45a0-99bb-3f47cc46c7ae"], "cee67e62-a320-42fa-b15e-5d4bc4e706e2": ["9a0b0b72-fb34-45a0-99bb-3f47cc46c7ae"], "8a84ea46-6958-4792-8497-d878f6322436": ["19d66c5a-b1d6-4129-a30e-7f07a7682a13"], "cdcd90d0-0a71-407c-927e-97077f09f724": ["19d66c5a-b1d6-4129-a30e-7f07a7682a13"], "0a342482-d23a-476c-98ea-536ba3b48381": ["4119ccb7-39ee-4f96-9323-1a01a7318f8c"], "00eb143f-3a22-4f57-88f3-9162bc24c331": ["4119ccb7-39ee-4f96-9323-1a01a7318f8c"], "7e510aea-7917-4fa2-8632-1bf7ae4b9dee": ["fe4b7cb3-e9fa-41ff-a948-97274b7b1585"], "9bab97d3-0317-4987-bf8f-2fad44110c1f": ["fe4b7cb3-e9fa-41ff-a948-97274b7b1585"], "c7562d70-f85e-4565-8379-e8b891f9cf77": ["27042c69-8102-4454-b216-b19b4b43f447"], "53ed3e26-7510-4397-a0e2-d4f10a2d1423": ["27042c69-8102-4454-b216-b19b4b43f447"], "dec125d0-f252-4877-9c5e-b92d69223715": ["6705792e-b1c6-4c29-ad89-509028d9be9f"], "4a152187-d63e-4564-abd2-2ee015495f3d": ["6705792e-b1c6-4c29-ad89-509028d9be9f"], "75cd125d-9f96-416b-a999-02275e1b86d1": ["de003be6-b1ee-4f22-86ac-8a3abfa06f18"], "1d11ad22-aecf-4b04-b333-1ed1110d1c8f": ["de003be6-b1ee-4f22-86ac-8a3abfa06f18"], "e4bbf02f-0757-4bf9-b9d3-6fcfa7b49cca": ["7fd0a6b5-0b87-4362-86c1-fe87da9822a2"], "ed3638c3-ea90-4554-a74c-0cf1037453e0": ["7fd0a6b5-0b87-4362-86c1-fe87da9822a2"], "7c41722e-53a4-42fc-a979-229f95d10ed2": ["70dd7a42-62e1-4d18-96ca-118ee7108662"], "5323daf6-dee6-497e-b7a1-9b8a0325c1b9": ["70dd7a42-62e1-4d18-96ca-118ee7108662"], "0272580d-fda4-4b84-8664-ba1fefde77f2": ["b7e56616-cf4f-4748-9381-bb7f4856fac3"], "d81968e6-b2d9-4390-8859-ee76227a7520": ["b7e56616-cf4f-4748-9381-bb7f4856fac3"], "26fae4d0-c807-4632-ae10-823fbea07750": ["dbdef64f-db9a-42c5-8cf8-b8a984e4d60b"], "502037aa-c3eb-4894-b33b-4e5551878da9": ["dbdef64f-db9a-42c5-8cf8-b8a984e4d60b"], "73c49a61-2dab-4b02-b587-0c43b03aa3dc": ["916ec1a4-dd01-4b8c-8d32-1532400e43aa"], "02e3bf5b-9cfa-453f-90fe-244de495973f": ["916ec1a4-dd01-4b8c-8d32-1532400e43aa"]}, "corpus": {"7bf21a59-a214-4b57-b758-9a81cb46a096": "publicly available databases have been created to document their occurrence. These reporting channels \nmake decisions on an ad hoc basis about what kinds of incidents to track. Some, for example, track by \namount of media coverage.", "00e5e3aa-0530-444e-bc68-21290b2f473f": "53 \nDocumenting, reporting, and sharing information about GAI incidents can help mitigate and prevent \nharmful outcomes by assisting relevant AI Actors in tracing impacts to their source. Greater awareness \nand standardization of GAI incident reporting could promote this transparency and improve GAI risk \nmanagement across the AI ecosystem.  \nDocumentation and Involvement of AI Actors", "90f15886-71fb-4bf0-850d-fad3f5dddc08": "Documentation and Involvement of AI Actors \nAI Actors should be aware of their roles in reporting AI incidents. To better understand previous incidents \nand implement measures to prevent similar ones in the future, organizations could consider developing \nguidelines for publicly available incident reporting which include information about AI actor \nresponsibilities. These guidelines would help AI system operators identify GAI incidents across the AI", "a54b205a-b8a9-4d2d-850a-ec792bb35a65": "lifecycle and with AI Actors regardless of role. Documentation and review of third-party inputs and \nplugins for GAI systems is especially important for AI Actors in the context of incident disclosure; LLM \ninputs and content delivered through these plugins is often distributed, with inconsistent or insu\ufb03cient \naccess control. \nDocumentation practices including logging, recording, and analyzing GAI incidents can facilitate", "d2f531cc-65e4-4526-9697-8261c4a75e34": "smoother sharing of information with relevant AI Actors. Regular information sharing, change \nmanagement records, version history and metadata can also empower AI Actors responding to and \nmanaging AI incidents.", "02fa2e5c-6b97-4fb0-8ef3-a287958193ee": "54 \nAppendix B. References \nAcemoglu, D. (2024) The Simple Macroeconomics of AI https://www.nber.org/papers/w32487 \nAI Incident Database. https://incidentdatabase.ai/ \nAtherton, D. (2024) Deepfakes and Child Safety: A Survey and Analysis of 2023 Incidents and Responses. \nAI Incident Database. https://incidentdatabase.ai/blog/deepfakes-and-child-safety/ \nBadyal, N. et al. (2023) Intentional Biases in LLM Responses. arXiv. https://arxiv.org/pdf/2311.07611", "dc0298fb-12d0-4091-9392-4b6c9e3a3c22": "Bing Chat: Data Ex\ufb01ltration Exploit Explained. Embrace The Red. \nhttps://embracethered.com/blog/posts/2023/bing-chat-data-ex\ufb01ltration-poc-and-\ufb01x/ \nBommasani, R. et al. (2022) Picking on the Same Person: Does Algorithmic Monoculture lead to Outcome \nHomogenization? arXiv. https://arxiv.org/pdf/2211.13972 \nBoyarskaya, M. et al. (2020) Overcoming Failures of Imagination in AI Infused System Development and \nDeployment. arXiv. https://arxiv.org/pdf/2011.13416", "d005b361-a80b-4130-bc8d-ba5b2d521142": "Browne, D. et al. (2023) Securing the AI Pipeline. Mandiant. \nhttps://www.mandiant.com/resources/blog/securing-ai-pipeline \nBurgess, M. (2024) Generative AI\u2019s Biggest Security Flaw Is Not Easy to Fix. WIRED. \nhttps://www.wired.com/story/generative-ai-prompt-injection-hacking/ \nBurtell, M. et al. (2024) The Surprising Power of Next Word Prediction: Large Language Models \nExplained, Part 1. Georgetown Center for Security and Emerging Technology.", "21423402-6662-42d1-a226-a970208a9a74": "https://cset.georgetown.edu/article/the-surprising-power-of-next-word-prediction-large-language-\nmodels-explained-part-1/ \nCanadian Centre for Cyber Security (2023) Generative arti\ufb01cial intelligence (AI) - ITSAP.00.041. \nhttps://www.cyber.gc.ca/en/guidance/generative-arti\ufb01cial-intelligence-ai-itsap00041 \nCarlini, N., et al. (2021) Extracting Training Data from Large Language Models. Usenix. \nhttps://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting", "a97f9981-e912-4913-92a0-dd6e07007780": "Carlini, N. et al. (2023) Quantifying Memorization Across Neural Language Models. ICLR 2023. \nhttps://arxiv.org/pdf/2202.07646 \nCarlini, N. et al. (2024) Stealing Part of a Production Language Model. arXiv. \nhttps://arxiv.org/abs/2403.06634 \nChandra, B. et al. (2023) Dismantling the Disinformation Business of Chinese In\ufb02uence Operations. \nRAND. https://www.rand.org/pubs/commentary/2023/10/dismantling-the-disinformation-business-of-\nchinese.html", "b3752ae1-c70b-46bb-893c-a55dd2529bbf": "chinese.html \nCiriello, R. et al. (2024) Ethical Tensions in Human-AI Companionship: A Dialectical Inquiry into Replika. \nResearchGate. https://www.researchgate.net/publication/374505266_Ethical_Tensions_in_Human-\nAI_Companionship_A_Dialectical_Inquiry_into_Replika \nDahl, M. et al. (2024) Large Legal Fictions: Pro\ufb01ling Legal Hallucinations in Large Language Models. arXiv. \nhttps://arxiv.org/abs/2401.01301", "c4e9d43d-c5d6-4ae7-9f38-7d5a10ff32e0": "55 \nDe Angelo, D. (2024) Short, Mid and Long-Term Impacts of AI in Cybersecurity. Palo Alto Networks. \nhttps://www.paloaltonetworks.com/blog/2024/02/impacts-of-ai-in-cybersecurity/ \nDe Freitas, J. et al. (2023) Chatbots and Mental Health: Insights into the Safety of Generative AI. Harvard \nBusiness School. https://www.hbs.edu/ris/Publication%20Files/23-011_c1bdd417-f717-47b6-bccb-\n5438c6e65c1a_f6fd9798-3c2d-4932-b222-056231fe69d7.pdf", "99a3e49a-6655-4d35-8c91-a0ee1f0fb757": "Dietvorst, B. et al. (2014) Algorithm Aversion: People Erroneously Avoid Algorithms After Seeing Them \nErr. Journal of Experimental Psychology. https://marketing.wharton.upenn.edu/wp-\ncontent/uploads/2016/10/Dietvorst-Simmons-Massey-2014.pdf \nDuhigg, C. (2012) How Companies Learn Your Secrets. New York Times. \nhttps://www.nytimes.com/2012/02/19/magazine/shopping-habits.html \nElsayed, G. et al. (2024) Images altered to trick machine vision can in\ufb02uence humans too. Google", "1a722ad4-c18f-46fc-bef0-be8c7fcdfd54": "DeepMind. https://deepmind.google/discover/blog/images-altered-to-trick-machine-vision-can-\nin\ufb02uence-humans-too/ \nEpstein, Z. et al. (2023). Art and the science of generative AI. Science. \nhttps://www.science.org/doi/10.1126/science.adh4451 \nFe\ufb00er, M. et al. (2024) Red-Teaming for Generative AI: Silver Bullet or Security Theater? arXiv. \nhttps://arxiv.org/pdf/2401.15897 \nGlazunov, S. et al. (2024) Project Naptime: Evaluating O\ufb00ensive Security Capabilities of Large Language", "320d8bb0-f2aa-4b65-ba24-8200f0dbd466": "Models. Project Zero. https://googleprojectzero.blogspot.com/2024/06/project-naptime.html \nGreshake, K. et al. (2023) Not what you've signed up for: Compromising Real-World LLM-Integrated \nApplications with Indirect Prompt Injection. arXiv. https://arxiv.org/abs/2302.12173 \nHagan, M. (2024) Good AI Legal Help, Bad AI Legal Help: Establishing quality standards for responses to \npeople\u2019s legal problem stories. SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4696936", "231301f5-3b8b-4f24-97d7-fe1b5888f914": "Haran, R. (2023) Securing LLM Systems Against Prompt Injection. NVIDIA. \nhttps://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/ \nInformation Technology Industry Council (2024) Authenticating AI-Generated Content. \nhttps://www.itic.org/policy/ITI_AIContentAuthorizationPolicy_122123.pdf \nJain, S. et al. (2023) Algorithmic Pluralism: A Structural Approach To Equal Opportunity. arXiv. \nhttps://arxiv.org/pdf/2305.08157", "f5c2f3fe-8330-4a97-8831-5393337edad3": "https://arxiv.org/pdf/2305.08157 \nJi, Z. et al (2023) Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55, 12, \nArticle 248. https://doi.org/10.1145/3571730 \nJones-Jang, S. et al. (2022) How do people react to AI failure? Automation bias, algorithmic aversion, and \nperceived controllability. Oxford. https://academic.oup.com/jcmc/article/28/1/zmac029/6827859] \nJussupow, E. et al. (2020) Why Are We Averse Towards Algorithms? A Comprehensive Literature Review", "2542ba17-1d49-405e-9ec7-f7bb9f3e0bf4": "on Algorithm Aversion. ECIS 2020. https://aisel.aisnet.org/ecis2020_rp/168/ \nKalai, A., et al. (2024) Calibrated Language Models Must Hallucinate. arXiv. \nhttps://arxiv.org/pdf/2311.14648", "b94361cb-b9b0-4176-bb33-2b12f49aa257": "56 \nKarasavva, V. et al. (2021) Personality, Attitudinal, and Demographic Predictors of Non-consensual \nDissemination of Intimate Images. NIH. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9554400/ \nKatzman, J., et al. (2023) Taxonomizing and measuring representational harms: a look at image tagging. \nAAAI. https://dl.acm.org/doi/10.1609/aaai.v37i12.26670 \nKhan, T. et al. (2024) From Code to Consumer: PAI\u2019s Value Chain Analysis Illuminates Generative AI\u2019s Key", "ff4345e7-16f8-4223-aa4b-4b93bfeba808": "Players. AI. https://partnershiponai.org/from-code-to-consumer-pais-value-chain-analysis-illuminates-\ngenerative-ais-key-players/ \nKirchenbauer, J. et al. (2023) A Watermark for Large Language Models. OpenReview. \nhttps://openreview.net/forum?id=aX8ig9X2a7 \nKleinberg, J. et al. (May 2021) Algorithmic monoculture and social welfare. PNAS. \nhttps://www.pnas.org/doi/10.1073/pnas.2018340118 \nLakatos, S. (2023) A Revealing Picture. Graphika. https://graphika.com/reports/a-revealing-picture", "d46f31b1-4e8a-44b7-a120-cf14383211ba": "Lee, H. et al. (2024) Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI Privacy Risks. \narXiv. https://arxiv.org/pdf/2310.07879 \nLenaerts-Bergmans, B. (2024) Data Poisoning: The Exploitation of Generative AI. Crowdstrike. \nhttps://www.crowdstrike.com/cybersecurity-101/cyberattacks/data-poisoning/ \nLiang, W. et al. (2023) GPT detectors are biased against non-native English writers. arXiv. \nhttps://arxiv.org/abs/2304.02819", "67989833-5934-4ec2-87c0-0b28b6366200": "https://arxiv.org/abs/2304.02819 \nLuccioni, A. et al. (2023) Power Hungry Processing: Watts Driving the Cost of AI Deployment? arXiv. \nhttps://arxiv.org/pdf/2311.16863 \nMouton, C. et al. (2024) The Operational Risks of AI in Large-Scale Biological Attacks. RAND. \nhttps://www.rand.org/pubs/research_reports/RRA2977-2.html. \nNicoletti, L. et al. (2023) Humans Are Biased. Generative Ai Is Even Worse. Bloomberg. \nhttps://www.bloomberg.com/graphics/2023-generative-ai-bias/.", "1fe4515f-361f-425c-81a0-a86ca1b2f0df": "National Institute of Standards and Technology (2024) Adversarial Machine Learning: A Taxonomy and \nTerminology of Attacks and Mitigations https://csrc.nist.gov/pubs/ai/100/2/e2023/\ufb01nal \nNational Institute of Standards and Technology (2023) AI Risk Management Framework. \nhttps://www.nist.gov/itl/ai-risk-management-framework \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Chapter 3: AI \nRisks and Trustworthiness.", "cbc7c103-c538-4d4e-a32c-8140905bbabb": "Risks and Trustworthiness. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Foundational_Information/3-sec-characteristics \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Chapter 6: AI \nRMF Pro\ufb01les. https://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Core_And_Pro\ufb01les/6-sec-pro\ufb01le \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Appendix A: \nDescriptions of AI Actor Tasks.", "5fd45d7c-b9bf-4348-8b27-686a58d5d1ac": "Descriptions of AI Actor Tasks. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_A#:~:text=AI%20actors%\n20in%20this%20category,data%20providers%2C%20system%20funders%2C%20product", "6ae40f2e-b16d-4db4-9b4d-92dccd0f59cc": "57 \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Appendix B: \nHow AI Risks Di\ufb00er from Traditional Software Risks. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_B \nNational Institute of Standards and Technology (2023) AI RMF Playbook. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook \nNational Institue of Standards and Technology (2023) Framing Risk", "c31c1037-681a-4847-9794-f2b8e76096f4": "https://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Foundational_Information/1-sec-risk \nNational Institute of Standards and Technology (2023) The Language of Trustworthy AI: An In-Depth \nGlossary of Terms https://airc.nist.gov/AI_RMF_Knowledge_Base/Glossary \nNational Institue of Standards and Technology (2022) Towards a Standard for Identifying and Managing \nBias in Arti\ufb01cial Intelligence https://www.nist.gov/publications/towards-standard-identifying-and-\nmanaging-bias-arti\ufb01cial-intelligence", "fbbb88f5-1e8d-4fa7-9e74-4bb9046a3d91": "managing-bias-arti\ufb01cial-intelligence \nNorthcutt, C. et al. (2021) Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks. \narXiv. https://arxiv.org/pdf/2103.14749 \nOECD (2023) \"Advancing accountability in AI: Governing and managing risks throughout the lifecycle for \ntrustworthy AI\", OECD Digital Economy Papers, No. 349, OECD Publishing, Paris. \nhttps://doi.org/10.1787/2448f04b-en", "2460271a-468d-4f6b-95ee-effd4bcfd4ba": "https://doi.org/10.1787/2448f04b-en \nOECD (2024) \"De\ufb01ning AI incidents and related terms\" OECD Arti\ufb01cial Intelligence Papers, No. 16, OECD \nPublishing, Paris. https://doi.org/10.1787/d1a8d965-en \nOpenAI (2023) GPT-4 System Card. https://cdn.openai.com/papers/gpt-4-system-card.pdf \nOpenAI (2024) GPT-4 Technical Report. https://arxiv.org/pdf/2303.08774 \nPadmakumar, V. et al. (2024) Does writing with language models reduce content diversity? ICLR. \nhttps://arxiv.org/pdf/2309.05196", "c1bf8c64-ba9f-4306-bf98-fcda06fe2466": "https://arxiv.org/pdf/2309.05196 \nPark, P. et. al. (2024) AI deception: A survey of examples, risks, and potential solutions. Patterns, 5(5). \narXiv. https://arxiv.org/pdf/2308.14752 \nPartnership on AI (2023) Building a Glossary for Synthetic Media Transparency Methods, Part 1: Indirect \nDisclosure. https://partnershiponai.org/glossary-for-synthetic-media-transparency-methods-part-1-\nindirect-disclosure/", "534bf276-7310-4123-b079-63582c85f764": "indirect-disclosure/ \nQu, Y. et al. (2023) Unsafe Di\ufb00usion: On the Generation of Unsafe Images and Hateful Memes From Text-\nTo-Image Models. arXiv. https://arxiv.org/pdf/2305.13873 \nRafat, K. et al. (2023) Mitigating carbon footprint for knowledge distillation based deep learning model \ncompression. PLOS One. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0285668 \nSaid, I. et al. (2022) Nonconsensual Distribution of Intimate Images: Exploring the Role of Legal Attitudes", "49f2ba7b-951c-423f-ba78-d07ae542607d": "in Victimization and Perpetration. Sage. \nhttps://journals.sagepub.com/doi/full/10.1177/08862605221122834#bibr47-08862605221122834 \nSandbrink, J. (2023) Arti\ufb01cial intelligence and biological misuse: Di\ufb00erentiating risks of language models \nand biological design tools. arXiv. https://arxiv.org/pdf/2306.13952", "ddeda98c-90a2-4462-9f76-c0988bea0818": "58 \nSatariano, A. et al. (2023) The People Onscreen Are Fake. The Disinformation Is Real. New York Times. \nhttps://www.nytimes.com/2023/02/07/technology/arti\ufb01cial-intelligence-training-deepfake.html \nSchaul, K. et al. (2024) Inside the secret list of websites that make AI like ChatGPT sound smart. \nWashington Post. https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/", "a19e73be-c909-453e-967f-014176c948de": "Scheurer, J. et al. (2023) Technical report: Large language models can strategically deceive their users \nwhen put under pressure. arXiv. https://arxiv.org/abs/2311.07590 \nShelby, R. et al. (2023) Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for Harm \nReduction. arXiv. https://arxiv.org/pdf/2210.05791 \nShevlane, T. et al. (2023) Model evaluation for extreme risks. arXiv. https://arxiv.org/pdf/2305.15324", "bcb6514c-0e67-4245-9fa8-3936fabd890e": "Shumailov, I. et al. (2023) The curse of recursion: training on generated data makes models forget. arXiv. \nhttps://arxiv.org/pdf/2305.17493v2 \nSmith, A. et al. (2023) Hallucination or Confabulation? Neuroanatomy as metaphor in Large Language \nModels. PLOS Digital Health. \nhttps://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000388 \nSoice, E. et al. (2023) Can large language models democratize access to dual-use biotechnology? arXiv. \nhttps://arxiv.org/abs/2306.03809", "9a0b0b72-fb34-45a0-99bb-3f47cc46c7ae": "https://arxiv.org/abs/2306.03809 \nSolaiman, I. et al. (2023) The Gradient of Generative AI Release: Methods and Considerations. arXiv. \nhttps://arxiv.org/abs/2302.04844 \nStaab, R. et al. (2023) Beyond Memorization: Violating Privacy via Inference With Large Language \nModels. arXiv. https://arxiv.org/pdf/2310.07298 \nStanford, S. et al. (2023) Whose Opinions Do Language Models Re\ufb02ect? arXiv. \nhttps://arxiv.org/pdf/2303.17548", "19d66c5a-b1d6-4129-a30e-7f07a7682a13": "https://arxiv.org/pdf/2303.17548 \nStrubell, E. et al. (2019) Energy and Policy Considerations for Deep Learning in NLP. arXiv. \nhttps://arxiv.org/pdf/1906.02243 \nThe White House (2016) Circular No. A-130, Managing Information as a Strategic Resource. \nhttps://www.whitehouse.gov/wp-\ncontent/uploads/legacy_drupal_\ufb01les/omb/circulars/A130/a130revised.pdf \nThe White House (2023) Executive Order on the Safe, Secure, and Trustworthy Development and Use of", "4119ccb7-39ee-4f96-9323-1a01a7318f8c": "Arti\ufb01cial Intelligence. https://www.whitehouse.gov/brie\ufb01ng-room/presidential-\nactions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-\narti\ufb01cial-intelligence/ \nThe White House (2022) Roadmap for Researchers on Priorities Related to Information Integrity \nResearch and Development. https://www.whitehouse.gov/wp-content/uploads/2022/12/Roadmap-\nInformation-Integrity-RD-2022.pdf?", "fe4b7cb3-e9fa-41ff-a948-97274b7b1585": "Information-Integrity-RD-2022.pdf? \nThiel, D. (2023) Investigation Finds AI Image Generation Models Trained on Child Abuse. Stanford Cyber \nPolicy Center. https://cyber.fsi.stanford.edu/news/investigation-\ufb01nds-ai-image-generation-models-\ntrained-child-abuse", "27042c69-8102-4454-b216-b19b4b43f447": "59 \nTirrell, L. (2017) Toxic Speech: Toward an Epidemiology of Discursive Harm. Philosophical Topics, 45(2), \n139-162. https://www.jstor.org/stable/26529441  \nTufekci, Z. (2015) Algorithmic Harms Beyond Facebook and Google: Emergent Challenges of \nComputational Agency. Colorado Technology Law Journal. https://ctlj.colorado.edu/wp-\ncontent/uploads/2015/08/Tufekci-\ufb01nal.pdf \nTurri, V. et al. (2023) Why We Need to Know More: Exploring the State of AI Incident Documentation", "6705792e-b1c6-4c29-ad89-509028d9be9f": "Practices. AAAI/ACM Conference on AI, Ethics, and Society. \nhttps://dl.acm.org/doi/fullHtml/10.1145/3600211.3604700 \nUrbina, F. et al. (2022) Dual use of arti\ufb01cial-intelligence-powered drug discovery. Nature Machine \nIntelligence. https://www.nature.com/articles/s42256-022-00465-9 \nWang, X. et al. (2023) Energy and Carbon Considerations of Fine-Tuning BERT. ACL Anthology. \nhttps://aclanthology.org/2023.\ufb01ndings-emnlp.607.pdf", "de003be6-b1ee-4f22-86ac-8a3abfa06f18": "Wang, Y. et al. (2023) Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. arXiv. \nhttps://arxiv.org/pdf/2308.13387 \nWardle, C. et al. (2017) Information Disorder: Toward an interdisciplinary framework for research and \npolicy making. Council of Europe. https://rm.coe.int/information-disorder-toward-an-interdisciplinary-\nframework-for-researc/168076277c \nWeatherbed, J. (2024) Trolls have \ufb02ooded X with graphic Taylor Swift AI fakes. The Verge.", "7fd0a6b5-0b87-4362-86c1-fe87da9822a2": "https://www.theverge.com/2024/1/25/24050334/x-twitter-taylor-swift-ai-fake-images-trending \nWei, J. et al. (2024) Long Form Factuality in Large Language Models. arXiv. \nhttps://arxiv.org/pdf/2403.18802 \nWeidinger, L. et al. (2021) Ethical and social risks of harm from Language Models. arXiv. \nhttps://arxiv.org/pdf/2112.04359 \nWeidinger, L. et al. (2023) Sociotechnical Safety Evaluation of Generative AI Systems. arXiv. \nhttps://arxiv.org/pdf/2310.11986", "70dd7a42-62e1-4d18-96ca-118ee7108662": "https://arxiv.org/pdf/2310.11986 \nWeidinger, L. et al. (2022) Taxonomy of Risks posed by Language Models. FAccT \u201922. \nhttps://dl.acm.org/doi/pdf/10.1145/3531146.3533088 \nWest, D. (2023) AI poses disproportionate risks to women. Brookings. \nhttps://www.brookings.edu/articles/ai-poses-disproportionate-risks-to-women/ \nWu, K. et al. (2024) How well do LLMs cite relevant medical references? An evaluation framework and \nanalyses. arXiv. https://arxiv.org/pdf/2402.02008", "b7e56616-cf4f-4748-9381-bb7f4856fac3": "Yin, L. et al. (2024) OpenAI\u2019s GPT Is A Recruiter\u2019s Dream Tool. Tests Show There\u2019s Racial Bias. Bloomberg. \nhttps://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/ \nYu, Z. et al. (March 2024) Don\u2019t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large \nLanguage Models. arXiv. https://arxiv.org/html/2403.17336v1 \nZaugg, I. et al. (2022) Digitally-disadvantaged languages. Policy Review. \nhttps://policyreview.info/pdf/policyreview-2022-2-1654.pdf", "dbdef64f-db9a-42c5-8cf8-b8a984e4d60b": "60 \nZhang, Y. et al. (2023) Human favoritism, not AI aversion: People\u2019s perceptions (and bias) toward \ngenerative AI, human experts, and human\u2013GAI collaboration in persuasive content generation. Judgment \nand Decision Making. https://www.cambridge.org/core/journals/judgment-and-decision-\nmaking/article/human-favoritism-not-ai-aversion-peoples-perceptions-and-bias-toward-generative-ai-\nhuman-experts-and-humangai-collaboration-in-persuasive-content-\ngeneration/419C4BD9CE82673EAF1D8F6C350C4FA8", "916ec1a4-dd01-4b8c-8d32-1532400e43aa": "generation/419C4BD9CE82673EAF1D8F6C350C4FA8 \nZhang, Y. et al. (2023) Siren\u2019s Song in the AI Ocean: A Survey on Hallucination in Large Language Models. \narXiv. https://arxiv.org/pdf/2309.01219 \nZhao, X. et al. (2023) Provable Robust Watermarking for AI-Generated Text. Semantic Scholar. \nhttps://www.semanticscholar.org/paper/Provable-Robust-Watermarking-for-AI-Generated-Text-Zhao-\nAnanth/75b68d0903af9d9f6e47ce3cf7e1a7d27ec811dc"}}