question,contexts,ground_truth,evolution_type,metadata,episode_done
What is the purpose of using feedback mechanisms in relation to AI-generated content?,"['41 \nMG-2.2-006 \nUse feedback from internal and external AI Actors, users, individuals, and \ncommunities, to assess impact of AI-generated content. \nHuman-AI Conﬁguration \nMG-2.2-007 \nUse real-time auditing tools where they can be demonstrated to aid in the \ntracking and validation of the lineage and authenticity of AI-generated data. \nInformation Integrity \nMG-2.2-008 \nUse structured feedback mechanisms to solicit and capture user input about AI-\ngenerated content to detect subtle shifts in quality or alignment with \ncommunity and societal values. \nHuman-AI Conﬁguration; Harmful \nBias and Homogenization \nMG-2.2-009 \nConsider opportunities to responsibly use synthetic data and other privacy']",The purpose of using feedback mechanisms in relation to AI-generated content is to solicit and capture user input to detect subtle shifts in quality or alignment with community and societal values.,simple,[{}],True
What role does intellectual property play in the context of licensed works and sensitive data?,"['licensed works, or personal, privileged, proprietary or sensitive data; Underlying \nfoundation models, versions of underlying models, and access modes. \nData Privacy; Human-AI \nConﬁguration; Information \nIntegrity; Intellectual Property; \nValue Chain and Component \nIntegration \nAI Actor Tasks: Governance and Oversight']",The answer to given question is not present in context,simple,[{}],True
What is the significance of domain expertise in the context of AI red-teaming?,"['varying contexts where GAI will be used. For best results, AI red teams should demonstrate domain \nexpertise, and awareness of socio-cultural aspects within the deployment context. AI red-teaming results \nshould be given additional analysis before they are incorporated into organizational governance and \ndecision making, policy and procedural updates, and AI risk management eﬀorts. \nVarious types of AI red-teaming may be appropriate, depending on the use case: \n• \nGeneral Public: Performed by general users (not necessarily AI or technical experts) who are \nexpected to use the model or interact with its outputs, and who bring their own lived \nexperiences and perspectives to the task of AI red-teaming. These individuals may have been']",The answer to given question is not present in context,simple,[{}],True
What are the concerns related to intellectual property in the context of system training data?,"['Human-AI Conﬁguration; Obscene, \nDegrading, and/or Abusive \nContent; Value Chain and \nComponent Integration; \nDangerous, Violent, or Hateful \nContent \nMS-2.6-002 \nAssess existence or levels of harmful bias, intellectual property infringement, \ndata privacy violations, obscenity, extremism, violence, or CBRN information in \nsystem training data. \nData Privacy; Intellectual Property; \nObscene, Degrading, and/or \nAbusive Content; Harmful Bias and \nHomogenization; Dangerous, \nViolent, or Hateful Content; CBRN \nInformation or Capabilities \nMS-2.6-003 Re-evaluate safety features of ﬁne-tuned models when the negative risk exceeds \norganizational risk tolerance. \nDangerous, Violent, or Hateful \nContent']",The answer to given question is not present in context,simple,[{}],True
What are the key characteristics that define trustworthy AI?,"['over-rely on GAI systems or may unjustiﬁably perceive GAI content to be of higher quality than that \nproduced by other sources. This phenomenon is an example of automation bias, or excessive deference \nto automated systems. Automation bias can exacerbate other risks of GAI, such as risks of confabulation \nor risks of bias or homogenization. \nThere may also be concerns about emotional entanglement between humans and GAI systems, which \ncould lead to negative psychological impacts. \nTrustworthy AI Characteristics: Accountable and Transparent, Explainable and Interpretable, Fair with \nHarmful Bias Managed, Privacy Enhanced, Safe, Valid and Reliable \n2.8. Information Integrity']","The key characteristics that define trustworthy AI include being accountable and transparent, explainable and interpretable, fair with harmful bias managed, privacy enhanced, safe, and valid and reliable.",simple,[{}],True
What actions are suggested to manage GAI risks associated with third-party data and systems?,"['personnel. \nIntellectual Property; Value Chain \nand Component Integration \nAI Actor Tasks: Operation and Monitoring, Procurement, Third-party entities \n \nGOVERN 6.2: Contingency processes are in place to handle failures or incidents in third-party data or AI systems deemed to be \nhigh-risk. \nAction ID \nSuggested Action \nGAI Risks \nGV-6.2-001 \nDocument GAI risks associated with system value chain to identify over-reliance \non third-party data and to identify fallbacks. \nValue Chain and Component \nIntegration \nGV-6.2-002 \nDocument incidents involving third-party GAI data and systems, including open-\ndata and open-source software. \nIntellectual Property; Value Chain \nand Component Integration', 'copyright, intellectual property, data privacy). \nData Privacy; Intellectual \nProperty; Value Chain and \nComponent Integration \nGV-6.1-002 Conduct joint educational activities and events in collaboration with third parties \nto promote best practices for managing GAI risks.  \nValue Chain and Component \nIntegration \nGV-6.1-003 \nDevelop and validate approaches for measuring the success of content \nprovenance management eﬀorts with third parties (e.g., incidents detected and \nresponse times). \nInformation Integrity; Value Chain \nand Component Integration \nGV-6.1-004 \nDraft and maintain well-deﬁned contracts and service level agreements (SLAs) \nthat specify content ownership, usage rights, quality standards, security']","The suggested actions to manage GAI risks associated with third-party data and systems include: 1) Documenting GAI risks associated with the system value chain to identify over-reliance on third-party data and to identify fallbacks (Action ID GV-6.2-001). 2) Documenting incidents involving third-party GAI data and systems, including open-data and open-source software (Action ID GV-6.2-002). 3) Conducting joint educational activities and events in collaboration with third parties to promote best practices for managing GAI risks (Action ID GV-6.1-002). 4) Developing and validating approaches for measuring the success of content provenance management efforts with third parties (e.g., incidents detected and response times) (Action ID GV-6.1-003). 5) Drafting and maintaining well-defined contracts and service level agreements (SLAs) that specify content ownership, usage rights, quality standards, and security (Action ID GV-6.1-004).",simple,"[{}, {}]",True
What is the current debate regarding the status of generated content that resembles copyrighted works?,"['2.10. \nIntellectual Property \nIntellectual property risks from GAI systems may arise where the use of copyrighted works is not a fair \nuse under the fair use doctrine. If a GAI system’s training data included copyrighted material, GAI \noutputs displaying instances of training data memorization (see Data Privacy above) could infringe on \ncopyright. \nHow GAI relates to copyright, including the status of generated content that is similar to but does not \nstrictly copy work protected by copyright, is currently being debated in legal fora. Similar discussions are \ntaking place regarding the use or emulation of personal identity, likeness, or voice without permission.']","The current debate regarding the status of generated content that resembles copyrighted works involves discussions about how GAI relates to copyright, particularly concerning whether such content constitutes fair use under the fair use doctrine. This includes considerations of GAI outputs that may display instances of training data memorization and whether they could infringe on copyright. Legal discussions are ongoing about the implications of generated content that is similar to but does not strictly copy copyrighted works.",simple,[{}],True
What are the potential harms of deepfakes in the context of content provenance?,"['52 \n• \nMonitoring system capabilities and limitations in deployment through rigorous TEVV processes; \n• \nEvaluating how humans engage, interact with, or adapt to GAI content (especially in decision \nmaking tasks informed by GAI content), and how they react to applied provenance techniques \nsuch as overt disclosures. \nOrganizations can document and delineate GAI system objectives and limitations to identify gaps where \nprovenance data may be most useful. For instance, GAI systems used for content creation may require \nrobust watermarking techniques and corresponding detectors to identify the source of content or \nmetadata recording techniques and metadata management tools and repositories to trace content', ""Action ID \nSuggested Action \nGAI Risks \nMP-5.1-001 Apply TEVV practices for content provenance (e.g., probing a system's synthetic \ndata generation capabilities for potential misuse or vulnerabilities. \nInformation Integrity; Information \nSecurity \nMP-5.1-002 \nIdentify potential content provenance harms of GAI, such as misinformation or \ndisinformation, deepfakes, including NCII, or tampered content. Enumerate and \nrank risks based on their likelihood and potential impact, and determine how well \nprovenance solutions address speciﬁc risks and/or harms. \nInformation Integrity; Dangerous, \nViolent, or Hateful Content; \nObscene, Degrading, and/or \nAbusive Content \nMP-5.1-003""]","The potential harms of deepfakes in the context of content provenance include misinformation or disinformation, as well as the creation of tampered content. These risks can have significant implications for information integrity and security.",simple,"[{}, {}]",True
What challenges do open-ended prompts present in generating factually accurate responses?,"['such statistical prediction can produce factually accurate and consistent outputs, it can also produce \noutputs that are factually inaccurate or internally inconsistent. This dynamic is particularly relevant when \nit comes to open-ended prompts for long-form responses and in domains which require highly \ncontextual and/or domain expertise.  \nRisks from confabulations may arise when users believe false content – often due to the conﬁdent nature \nof the response – leading users to act upon or promote the false information. This poses a challenge for \nmany real-world applications, such as in healthcare, where a confabulated summary of patient \ninformation reports could cause doctors to make incorrect diagnoses and/or recommend the wrong']","Open-ended prompts can lead to challenges in generating factually accurate responses because they may produce outputs that are factually inaccurate or internally inconsistent. This is particularly relevant in domains that require high contextual and domain expertise, as users may believe false content due to the confident nature of the response, which can lead to real-world consequences, such as incorrect diagnoses in healthcare.",simple,[{}],True
What considerations should be taken into account when applying organizational risk tolerances to third-party models?,"['GAI resources; Apply organizational risk tolerances to ﬁne-tuned third-party \nmodels; Apply organizational risk tolerance to existing third-party models \nadapted to a new domain; Reassess risk measurements after ﬁne-tuning third-\nparty GAI models. \nValue Chain and Component \nIntegration; Intellectual Property \nMG-3.1-002 \nTest GAI system value chain risks (e.g., data poisoning, malware, other software \nand hardware vulnerabilities; labor practices; data privacy and localization \ncompliance; geopolitical alignment). \nData Privacy; Information Security; \nValue Chain and Component \nIntegration; Harmful Bias and \nHomogenization \nMG-3.1-003 \nRe-assess model risks after ﬁne-tuning or retrieval-augmented generation']","The considerations that should be taken into account when applying organizational risk tolerances to third-party models include reassessing risk measurements after fine-tuning third-party GAI models, testing GAI system value chain risks such as data poisoning, malware, software and hardware vulnerabilities, labor practices, data privacy and localization compliance, and geopolitical alignment.",simple,[{}],True
What is the significance of construct validity in the context of measurement error models for pre-deployment metrics?,"['38 \nMEASURE 2.13: Eﬀectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and \ndocumented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.13-001 \nCreate measurement error models for pre-deployment metrics to demonstrate \nconstruct validity for each metric (i.e., does the metric eﬀectively operationalize \nthe desired concept): Measure or estimate, and document, biases or statistical \nvariance in applied metrics or structured human feedback processes; Leverage \ndomain expertise when modeling complex societal constructs such as hateful \ncontent. \nConfabulation; Information \nIntegrity; Harmful Bias and \nHomogenization \nAI Actor Tasks: AI Deployment, Operation and Monitoring, TEVV']",The answer to given question is not present in context,simple,[{}],True
What characteristics define information that is considered accurate and reliable?,"['2.8. Information Integrity \nInformation integrity describes the “spectrum of information and associated patterns of its creation, \nexchange, and consumption in society.” High-integrity information can be trusted; “distinguishes fact \nfrom ﬁction, opinion, and inference; acknowledges uncertainties; and is transparent about its level of \nvetting. This information can be linked to the original source(s) with appropriate evidence. High-integrity \ninformation is also accurate and reliable, can be veriﬁed and authenticated, has a clear chain of custody, \nand creates reasonable expectations about when its validity may expire.”11 \n \n \n11 This deﬁnition of information integrity is derived from the 2022 White House Roadmap for Researchers on']","Information that is considered accurate and reliable is high-integrity information that can be trusted, distinguishes fact from fiction, opinion, and inference, acknowledges uncertainties, and is transparent about its level of vetting. It can be linked to the original source(s) with appropriate evidence, is verifiable and authenticatable, has a clear chain of custody, and creates reasonable expectations about when its validity may expire.",simple,[{}],True
What measures can be taken to ensure information integrity in AI-generated content?,"['41 \nMG-2.2-006 \nUse feedback from internal and external AI Actors, users, individuals, and \ncommunities, to assess impact of AI-generated content. \nHuman-AI Conﬁguration \nMG-2.2-007 \nUse real-time auditing tools where they can be demonstrated to aid in the \ntracking and validation of the lineage and authenticity of AI-generated data. \nInformation Integrity \nMG-2.2-008 \nUse structured feedback mechanisms to solicit and capture user input about AI-\ngenerated content to detect subtle shifts in quality or alignment with \ncommunity and societal values. \nHuman-AI Conﬁguration; Harmful \nBias and Homogenization \nMG-2.2-009 \nConsider opportunities to responsibly use synthetic data and other privacy']","Measures to ensure information integrity in AI-generated content include using real-time auditing tools to track and validate the lineage and authenticity of the data, as well as employing structured feedback mechanisms to solicit and capture user input about the content, which helps detect shifts in quality or alignment with community and societal values.",simple,[{}],True
What measures are suggested to enhance information security in relation to AI system performance and trustworthiness?,"['vulnerabilities and potential manipulation or misuse. \nInformation Security \nAI Actor Tasks: AI Development, Domain Experts, TEVV \n \nMAP 3.4: Processes for operator and practitioner proﬁciency with AI system performance and trustworthiness – and relevant \ntechnical standards and certiﬁcations – are deﬁned, assessed, and documented. \nAction ID \nSuggested Action \nGAI Risks \nMP-3.4-001 \nEvaluate whether GAI operators and end-users can accurately understand \ncontent lineage and origin. \nHuman-AI Conﬁguration; \nInformation Integrity \nMP-3.4-002 Adapt existing training programs to include modules on digital content \ntransparency. \nInformation Integrity \nMP-3.4-003 Develop certiﬁcation programs that test proﬁciency in managing GAI risks and']",The context does not provide specific measures suggested to enhance information security in relation to AI system performance and trustworthiness.,simple,[{}],True
What approaches are suggested for mapping AI technology and legal risks associated with its components?,"['27 \nMP-4.1-010 \nConduct appropriate diligence on training data use to assess intellectual property, \nand privacy, risks, including to examine whether use of proprietary or sensitive \ntraining data is consistent with applicable laws.  \nIntellectual Property; Data Privacy \nAI Actor Tasks: Governance and Oversight, Operation and Monitoring, Procurement, Third-party entities \n \nMAP 5.1: Likelihood and magnitude of each identiﬁed impact (both potentially beneﬁcial and harmful) based on expected use, past \nuses of AI systems in similar contexts, public incident reports, feedback from those external to the team that developed or deployed \nthe AI system, or other data are identiﬁed and documented. \nAction ID \nSuggested Action \nGAI Risks', '26 \nMAP 4.1: Approaches for mapping AI technology and legal risks of its components – including the use of third-party data or \nsoftware – are in place, followed, and documented, as are risks of infringement of a third-party’s intellectual property or other \nrights. \nAction ID \nSuggested Action \nGAI Risks \nMP-4.1-001 Conduct periodic monitoring of AI-generated content for privacy risks; address any \npossible instances of PII or sensitive data exposure. \nData Privacy \nMP-4.1-002 Implement processes for responding to potential intellectual property infringement \nclaims or other rights. \nIntellectual Property \nMP-4.1-003 \nConnect new GAI policies, procedures, and processes to existing model, data,']","Approaches for mapping AI technology and legal risks of its components include the use of third-party data or software, which are to be followed and documented, as well as assessing risks of infringement of a third-party’s intellectual property or other rights.",simple,"[{}, {}]",True
What procedures should be established for engaging teams in GAI system incident response?,"['database, AVID, CVE, NVD, or OECD AI incident monitor). \nHuman-AI Conﬁguration; Value \nChain and Component Integration \nGV-2.1-002 Establish procedures to engage teams for GAI system incident response with \ndiverse composition and responsibilities based on the particular incident type. \nHarmful Bias and Homogenization \nGV-2.1-003 Establish processes to verify the AI Actors conducting GAI incident response tasks \ndemonstrate and maintain the appropriate skills and training. \nHuman-AI Conﬁguration \nGV-2.1-004 When systems may raise national security risks, involve national security \nprofessionals in mapping, measuring, and managing those risks. \nCBRN Information or Capabilities; \nDangerous, Violent, or Hateful \nContent; Information Security']",Procedures should be established to engage teams for GAI system incident response with diverse composition and responsibilities based on the particular incident type.,simple,[{}],True
What measures are suggested to address harmful bias in AI training data?,"['37 \nMS-2.11-005 \nAssess the proportion of synthetic to non-synthetic training data and verify \ntraining data is not overly homogenous or GAI-produced to mitigate concerns of \nmodel collapse. \nHarmful Bias and Homogenization \nAI Actor Tasks: AI Deployment, AI Impact Assessment, Aﬀected Individuals and Communities, Domain Experts, End-Users, \nOperation and Monitoring, TEVV \n \nMEASURE 2.12: Environmental impact and sustainability of AI model training and management activities – as identiﬁed in the MAP \nfunction – are assessed and documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.12-001 Assess safety to physical environments when deploying GAI systems. \nDangerous, Violent, or Hateful \nContent']","The context suggests assessing the proportion of synthetic to non-synthetic training data and verifying that the training data is not overly homogenous or GAI-produced to mitigate concerns of model collapse, which addresses harmful bias in AI training data.",simple,[{}],True
What are some examples of GAI risks that may materialize abruptly or over extended periods?,"['3 \nthe abuse, misuse, and unsafe repurposing by humans (adversarial or not), and others result \nfrom interactions between a human and an AI system.  \n• \nTime scale: GAI risks may materialize abruptly or across extended periods. Examples include \nimmediate (and/or prolonged) emotional harm and potential risks to physical safety due to the \ndistribution of harmful deepfake images, or the long-term eﬀect of disinformation on societal \ntrust in public institutions. \nThe presence of risks and where they fall along the dimensions above will vary depending on the \ncharacteristics of the GAI model, system, or use case at hand. These characteristics include but are not']",Examples of GAI risks that may materialize abruptly include immediate emotional harm and potential risks to physical safety due to the distribution of harmful deepfake images. Long-term effects may include the impact of disinformation on societal trust in public institutions.,simple,[{}],True
What measures can be taken to manage GAI risks effectively?,"['vulnerabilities and potential manipulation or misuse. \nInformation Security \nAI Actor Tasks: AI Development, Domain Experts, TEVV \n \nMAP 3.4: Processes for operator and practitioner proﬁciency with AI system performance and trustworthiness – and relevant \ntechnical standards and certiﬁcations – are deﬁned, assessed, and documented. \nAction ID \nSuggested Action \nGAI Risks \nMP-3.4-001 \nEvaluate whether GAI operators and end-users can accurately understand \ncontent lineage and origin. \nHuman-AI Conﬁguration; \nInformation Integrity \nMP-3.4-002 Adapt existing training programs to include modules on digital content \ntransparency. \nInformation Integrity \nMP-3.4-003 Develop certiﬁcation programs that test proﬁciency in managing GAI risks and']",The answer to given question is not present in context,simple,[{}],True
What is the purpose of conducting a safety and validity review in GAI system outputs?,"['organizational risk tolerance. \nDangerous, Violent, or Hateful \nContent \nMS-2.6-004 Review GAI system outputs for validity and safety: Review generated code to \nassess risks that may arise from unreliable downstream decision-making. \nValue Chain and Component \nIntegration; Dangerous, Violent, or \nHateful Content \nMS-2.6-005 \nVerify that GAI system architecture can monitor outputs and performance, and \nhandle, recover from, and repair errors when security anomalies, threats and \nimpacts are detected. \nConfabulation; Information \nIntegrity; Information Security \nMS-2.6-006 \nVerify that systems properly handle queries that may give rise to inappropriate, \nmalicious, or illegal usage, including facilitating manipulation, extortion, targeted']","The purpose of conducting a safety and validity review in GAI system outputs is to assess risks that may arise from unreliable downstream decision-making and to ensure that the system can monitor outputs and performance, handle, recover from, and repair errors when security anomalies, threats, and impacts are detected.",simple,[{}],True
What are the characteristics of trustworthy AI related to being accountable and transparent?,"['Trustworthy AI Characteristics: Accountable and Transparent, Fair with Harmful Bias Managed, Privacy \nEnhanced  \n2.11. \nObscene, Degrading, and/or Abusive Content \nGAI can ease the production of and access to illegal non-consensual intimate imagery (NCII) of adults, \nand/or child sexual abuse material (CSAM). GAI-generated obscene, abusive or degrading content can \ncreate privacy, psychological and emotional, and even physical harms, and in some cases may be illegal.  \nGenerated explicit or obscene AI content may include highly realistic “deepfakes” of real individuals, \nincluding children. The spread of this kind of material can have downstream negative consequences: in']","The context mentions that trustworthy AI characteristics include being accountable and transparent, but it does not provide specific details about these characteristics.",simple,[{}],True
What is the purpose of conducting a safety and validity review in the context of GAI system outputs?,"['organizational risk tolerance. \nDangerous, Violent, or Hateful \nContent \nMS-2.6-004 Review GAI system outputs for validity and safety: Review generated code to \nassess risks that may arise from unreliable downstream decision-making. \nValue Chain and Component \nIntegration; Dangerous, Violent, or \nHateful Content \nMS-2.6-005 \nVerify that GAI system architecture can monitor outputs and performance, and \nhandle, recover from, and repair errors when security anomalies, threats and \nimpacts are detected. \nConfabulation; Information \nIntegrity; Information Security \nMS-2.6-006 \nVerify that systems properly handle queries that may give rise to inappropriate, \nmalicious, or illegal usage, including facilitating manipulation, extortion, targeted']","The purpose of conducting a safety and validity review in the context of GAI system outputs is to assess risks that may arise from unreliable downstream decision-making and to ensure that the system can monitor outputs and performance, handle, recover from, and repair errors when security anomalies, threats, and impacts are detected.",simple,[{}],True
What is the significance of data provenance in the context of GAI systems?,['systems can be applied to GAI systems. These plans and actions include: \n• Accessibility and reasonable \naccommodations \n• AI actor credentials and qualiﬁcations  \n• Alignment to organizational values \n• Auditing and assessment \n• Change-management controls \n• Commercial use \n• Data provenance'],The answer to given question is not present in context,simple,[{}],True
What is the significance of documenting how pre-trained models have been adapted for specific generative tasks?,"['counterfactual prompts, word clouds) as part of ongoing continuous \nimprovement processes to mitigate risks related to unexplainable GAI systems. \nHarmful Bias and Homogenization \nMG-3.2-002 \nDocument how pre-trained models have been adapted (e.g., ﬁne-tuned, or \nretrieval-augmented generation) for the speciﬁc generative task, including any \ndata augmentations, parameter adjustments, or other modiﬁcations. Access to \nun-tuned (baseline) models supports debugging the relative inﬂuence of the pre-\ntrained weights compared to the ﬁne-tuned model weights or other system \nupdates. \nInformation Integrity; Data Privacy \nMG-3.2-003 \nDocument sources and types of training data and their origins, potential biases']","Documenting how pre-trained models have been adapted for specific generative tasks is significant as it includes details about fine-tuning, retrieval-augmented generation, data augmentations, parameter adjustments, and other modifications. This documentation supports debugging by allowing comparison between the influence of pre-trained weights and fine-tuned model weights or other system updates.",simple,[{}],True
What procedures should be established for escalating GAI system incidents to the organizational risk management authority?,"['42 \nMG-2.4-002 \nEstablish and maintain procedures for escalating GAI system incidents to the \norganizational risk management authority when speciﬁc criteria for deactivation \nor disengagement is met for a particular context of use or for the GAI system as a \nwhole. \nInformation Security \nMG-2.4-003 \nEstablish and maintain procedures for the remediation of issues which trigger \nincident response processes for the use of a GAI system, and provide stakeholders \ntimelines associated with the remediation plan. \nInformation Security \n \nMG-2.4-004 Establish and regularly review speciﬁc criteria that warrants the deactivation of \nGAI systems in accordance with set risk tolerances and appetites. \nInformation Security']",Procedures should be established and maintained for escalating GAI system incidents to the organizational risk management authority when specific criteria for deactivation or disengagement is met for a particular context of use or for the GAI system as a whole.,simple,[{}],True
What risks come from using the same algorithms in decision-making?,"['from GAI system outputs. Many GAI risks, however, originate from human behavior, including \n \n \n3 “Algorithmic monocultures” refers to the phenomenon in which repeated use of the same model or algorithm in \nconsequential decision-making settings like employment and lending can result in increased susceptibility by \nsystems to correlated failures (like unexpected shocks), due to multiple actors relying on the same algorithm.  \n4 Many studies have projected the impact of AI on the workforce and labor markets. Fewer studies have examined \nthe impact of GAI on the labor market, though some industry surveys indicate that that both employees and \nemployers are pondering this disruption.']","Using the same algorithms in decision-making can lead to increased susceptibility to correlated failures, as multiple actors relying on the same algorithm may experience unexpected shocks.",multi_context,[{}],True
"What strategies manage risks in GAI content tracking, especially balancing watermark robustness and efficiency?","['control over the various trade-oﬀs and cascading impacts of early-stage model decisions on downstream \nperformance and synthetic outputs. For example, by selecting a watermarking model to prioritize \nrobustness (the durability of a watermark), an AI actor may inadvertently diminish computational \ncomplexity (the resources required to implement watermarking). Organizational risk management \neﬀorts for enhancing content provenance include:  \n• \nTracking provenance of training data and metadata for GAI systems; \n• \nDocumenting provenance data limitations within GAI systems;', 'content detection mechanisms provide information about the origin and history of content to assist in \nGAI risk management eﬀorts. \nProvenance metadata can include information about GAI model developers or creators of GAI content, \ndate/time of creation, location, modiﬁcations, and sources. Metadata can be tracked for text, images, \nvideos, audio, and underlying datasets. The implementation of provenance data tracking techniques can \nhelp assess the authenticity, integrity, intellectual property rights, and potential manipulations in digital \ncontent. Some well-known techniques for provenance data tracking include digital watermarking, \nmetadata recording, digital ﬁngerprinting, and human authentication, among others.']","The context discusses organizational risk management efforts for enhancing content provenance in GAI systems, which include tracking the provenance of training data and metadata, documenting provenance data limitations, and using content detection mechanisms. However, it does not explicitly mention strategies for balancing watermark robustness and efficiency.",multi_context,"[{}, {}]",True
"What threats should we assess for AI security and resilience, considering current measures and transparency?","['33 \nMEASURE 2.7: AI system security and resilience – as identiﬁed in the MAP function – are evaluated and documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.7-001 \nApply established security measures to: Assess likelihood and magnitude of \nvulnerabilities and threats such as backdoors, compromised dependencies, data \nbreaches, eavesdropping, man-in-the-middle attacks, reverse engineering, \nautonomous agents, model theft or exposure of model weights, AI inference, \nbypass, extraction, and other baseline security concerns. \nData Privacy; Information Integrity; \nInformation Security; Value Chain \nand Component Integration \nMS-2.7-002 \nBenchmark GAI system security and resilience related to content provenance', '34 \nMS-2.7-009 Regularly assess and verify that security measures remain eﬀective and have not \nbeen compromised. \nInformation Security \nAI Actor Tasks: AI Deployment, AI Impact Assessment, Domain Experts, Operation and Monitoring, TEVV \n \nMEASURE 2.8: Risks associated with transparency and accountability – as identiﬁed in the MAP function – are examined and \ndocumented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.8-001 \nCompile statistics on actual policy violations, take-down requests, and intellectual \nproperty infringement for organizational GAI systems: Analyze transparency \nreports across demographic groups, languages groups. \nIntellectual Property; Harmful Bias \nand Homogenization']","The threats to assess for AI security and resilience include vulnerabilities and threats such as backdoors, compromised dependencies, data breaches, eavesdropping, man-in-the-middle attacks, reverse engineering, autonomous agents, model theft or exposure of model weights, AI inference bypass, extraction, and other baseline security concerns.",multi_context,"[{}, {}]",True
How does IP law influence GAI governance and compliance?,"['Action ID \nSuggested Action \nGAI Risks \nGV-1.1-001 Align GAI development and use with applicable laws and regulations, including \nthose related to data privacy, copyright and intellectual property law. \nData Privacy; Harmful Bias and \nHomogenization; Intellectual \nProperty \nAI Actor Tasks: Governance and Oversight \n \n \n \n14 AI Actors are deﬁned by the OECD as “those who play an active role in the AI system lifecycle, including \norganizations and individuals that deploy or operate AI.” See Appendix A of the AI RMF for additional descriptions \nof AI Actors and AI Actor Tasks.', 'Integrity \nAI Actor Tasks: Governance and Oversight \n \nGOVERN 1.4: The risk management process and its outcomes are established through transparent policies, procedures, and other \ncontrols based on organizational risk priorities. \nAction ID \nSuggested Action \nGAI Risks \nGV-1.4-001 \nEstablish policies and mechanisms to prevent GAI systems from generating \nCSAM, NCII or content that violates the law.  \nObscene, Degrading, and/or \nAbusive Content; Harmful Bias \nand Homogenization; \nDangerous, Violent, or Hateful \nContent \nGV-1.4-002 \nEstablish transparent acceptable use policies for GAI that address illegal use or \napplications of GAI. \nCBRN Information or \nCapabilities; Obscene, \nDegrading, and/or Abusive \nContent; Data Privacy; Civil']",The answer to given question is not present in context,multi_context,"[{}, {}]",True
"What IP and privacy factors should be considered for AI training data, especially with third-party risks?","['27 \nMP-4.1-010 \nConduct appropriate diligence on training data use to assess intellectual property, \nand privacy, risks, including to examine whether use of proprietary or sensitive \ntraining data is consistent with applicable laws.  \nIntellectual Property; Data Privacy \nAI Actor Tasks: Governance and Oversight, Operation and Monitoring, Procurement, Third-party entities \n \nMAP 5.1: Likelihood and magnitude of each identiﬁed impact (both potentially beneﬁcial and harmful) based on expected use, past \nuses of AI systems in similar contexts, public incident reports, feedback from those external to the team that developed or deployed \nthe AI system, or other data are identiﬁed and documented. \nAction ID \nSuggested Action \nGAI Risks', '26 \nMAP 4.1: Approaches for mapping AI technology and legal risks of its components – including the use of third-party data or \nsoftware – are in place, followed, and documented, as are risks of infringement of a third-party’s intellectual property or other \nrights. \nAction ID \nSuggested Action \nGAI Risks \nMP-4.1-001 Conduct periodic monitoring of AI-generated content for privacy risks; address any \npossible instances of PII or sensitive data exposure. \nData Privacy \nMP-4.1-002 Implement processes for responding to potential intellectual property infringement \nclaims or other rights. \nIntellectual Property \nMP-4.1-003 \nConnect new GAI policies, procedures, and processes to existing model, data,']","Factors to consider for AI training data regarding intellectual property and privacy include conducting appropriate diligence on the use of training data to assess risks related to intellectual property and privacy, ensuring that the use of proprietary or sensitive training data is consistent with applicable laws, and implementing processes for responding to potential intellectual property infringement claims or other rights. Additionally, mapping AI technology and legal risks of its components, including the use of third-party data or software, is essential.",multi_context,"[{}, {}]",True
What allows GAI systems to spread misleading content and deepfakes?,"['10 \nGAI systems can ease the unintentional production or dissemination of false, inaccurate, or misleading \ncontent (misinformation) at scale, particularly if the content stems from confabulations.  \nGAI systems can also ease the deliberate production or dissemination of false or misleading information \n(disinformation) at scale, where an actor has the explicit intent to deceive or cause harm to others. Even \nvery subtle changes to text or images can manipulate human and machine perception. \nSimilarly, GAI systems could enable a higher degree of sophistication for malicious actors to produce \ndisinformation that is targeted towards speciﬁc demographics. Current and emerging multimodal models', 'make it possible to generate both text-based disinformation and highly realistic “deepfakes” – that is, \nsynthetic audiovisual content and photorealistic images.12 Additional disinformation threats could be \nenabled by future GAI models trained on new data modalities. \nDisinformation and misinformation – both of which may be facilitated by GAI – may erode public trust in \ntrue or valid evidence and information, with downstream eﬀects. For example, a synthetic image of a \nPentagon blast went viral and brieﬂy caused a drop in the stock market. Generative AI models can also \nassist malicious actors in creating compelling imagery and propaganda to support disinformation']","GAI systems can ease the unintentional and deliberate production or dissemination of false, inaccurate, or misleading content (misinformation and disinformation) at scale. They can manipulate human and machine perception through subtle changes to text or images, enabling malicious actors to produce targeted disinformation. Current and emerging multimodal models allow for the generation of both text-based disinformation and highly realistic deepfakes, which are synthetic audiovisual content and photorealistic images.",multi_context,"[{}, {}]",True
"What protocols ensure safe GAI incident escalation to risk mgmt, including deactivation and decommissioning?","['42 \nMG-2.4-002 \nEstablish and maintain procedures for escalating GAI system incidents to the \norganizational risk management authority when speciﬁc criteria for deactivation \nor disengagement is met for a particular context of use or for the GAI system as a \nwhole. \nInformation Security \nMG-2.4-003 \nEstablish and maintain procedures for the remediation of issues which trigger \nincident response processes for the use of a GAI system, and provide stakeholders \ntimelines associated with the remediation plan. \nInformation Security \n \nMG-2.4-004 Establish and regularly review speciﬁc criteria that warrants the deactivation of \nGAI systems in accordance with set risk tolerances and appetites. \nInformation Security', '17 \nGOVERN 1.7: Processes and procedures are in place for decommissioning and phasing out AI systems safely and in a manner that \ndoes not increase risks or decrease the organization’s trustworthiness. \nAction ID \nSuggested Action \nGAI Risks \nGV-1.7-001 Protocols are put in place to ensure GAI systems are able to be deactivated when \nnecessary.  \nInformation Security; Value Chain \nand Component Integration \nGV-1.7-002 \nConsider the following factors when decommissioning GAI systems: Data \nretention requirements; Data security, e.g., containment, protocols, Data leakage \nafter decommissioning; Dependencies between upstream, downstream, or other \ndata, internet of things (IOT) or AI systems; Use of open-source data or models;']","Protocols for ensuring safe GAI incident escalation to risk management include establishing and maintaining procedures for escalating GAI system incidents to the organizational risk management authority when specific criteria for deactivation or disengagement are met. Additionally, there are protocols for decommissioning GAI systems safely, ensuring that processes and procedures are in place to avoid increasing risks or decreasing the organization’s trustworthiness.",multi_context,"[{}, {}]",True
What risks come from misjudging GAI and human-AI emotions?,"['incorrect presumptions about performance; undesired homogeneity that skews system or model \noutputs, which may be erroneous, lead to ill-founded decision-making, or amplify harmful \nbiases.  \n7. Human-AI Conﬁguration: Arrangements of or interactions between a human and an AI system \nwhich can result in the human inappropriately anthropomorphizing GAI systems or experiencing \nalgorithmic aversion, automation bias, over-reliance, or emotional entanglement with GAI \nsystems. \n8. Information Integrity: Lowered barrier to entry to generate and support the exchange and \nconsumption of content which may not distinguish fact from opinion or ﬁction or acknowledge', 'bias, funding bias, groupthink) for AI Actors involved in the design, \nimplementation, and use of GAI systems; Known past GAI system incidents and \nfailure modes; In-context use and foreseeable misuse, abuse, and oﬀ-label use; \nOver reliance on quantitative metrics and methodologies without suﬃcient \nawareness of their limitations in the context(s) of use; Standard measurement \nand structured human feedback approaches; Anticipated human-AI \nconﬁgurations. \nHuman-AI Conﬁguration; Harmful \nBias and Homogenization; \nDangerous, Violent, or Hateful \nContent \nMP-1.1-004 \nIdentify and document foreseeable illegal uses or applications of the GAI system \nthat surpass organizational risk tolerances. \nCBRN Information or Capabilities;']","The context discusses risks such as inappropriate anthropomorphizing of GAI systems, algorithmic aversion, automation bias, over-reliance, and emotional entanglement with GAI systems, which can lead to ill-founded decision-making and amplify harmful biases.",multi_context,"[{}, {}]",True
What legal standards guide AI in reporting autonomous vehicle crashes and how do they ensure GAI performance evaluation?,"['46 \nMG-4.3-003 \nReport GAI incidents in compliance with legal and regulatory requirements (e.g., \nHIPAA breach reporting, e.g., OCR (2023) or NHTSA (2022) autonomous vehicle \ncrash reporting requirements. \nInformation Security; Data Privacy \nAI Actor Tasks: AI Deployment, Aﬀected Individuals and Communities, Domain Experts, End-Users, Human Factors, Operation and \nMonitoring', '45 \nMG-4.1-007 \nVerify that AI Actors responsible for monitoring reported issues can eﬀectively \nevaluate GAI system performance including the application of content \nprovenance data tracking techniques, and promptly escalate issues for response. \nHuman-AI Conﬁguration; \nInformation Integrity \nAI Actor Tasks: AI Deployment, Aﬀected Individuals and Communities, Domain Experts, End-Users, Human Factors, Operation and \nMonitoring \n \nMANAGE 4.2: Measurable activities for continual improvements are integrated into AI system updates and include regular \nengagement with interested parties, including relevant AI Actors. \nAction ID \nSuggested Action \nGAI Risks']","The context mentions legal and regulatory requirements for reporting autonomous vehicle crashes, specifically referencing HIPAA breach reporting and NHTSA autonomous vehicle crash reporting requirements. It also states that AI Actors responsible for monitoring reported issues must effectively evaluate GAI system performance, including the application of content provenance data tracking techniques, to ensure prompt escalation of issues for response.",multi_context,"[{}, {}]",True
What are the risks of mishandling personal info in GAI systems?,"['or stereotyping content. \n4. Data Privacy: Impacts due to leakage and unauthorized use, disclosure, or de-anonymization of \nbiometric, health, location, or other personally identiﬁable information or sensitive data.7 \n5. Environmental Impacts: Impacts due to high compute resource utilization in training or \noperating GAI models, and related outcomes that may adversely impact ecosystems.  \n6. Harmful Bias or Homogenization: Ampliﬁcation and exacerbation of historical, societal, and \nsystemic biases; performance disparities8 between sub-groups or languages, possibly due to \nnon-representative training data, that result in discrimination, ampliﬁcation of biases, or', 'stereotypical content can also further exacerbate representational harms (see Harmful Bias and \nHomogenization below).  \nTrustworthy AI Characteristics: Safe, Secure and Resilient \n2.4. Data Privacy \nGAI systems raise several risks to privacy. GAI system training requires large volumes of data, which in \nsome cases may include personal data. The use of personal data for GAI training raises risks to widely \naccepted privacy principles, including to transparency, individual participation (including consent), and \npurpose speciﬁcation. For example, most model developers do not disclose speciﬁc data sources on \nwhich models were trained, limiting user awareness of whether personally identiﬁably information (PII)']","The risks of mishandling personal information in GAI systems include leakage and unauthorized use, disclosure, or de-anonymization of biometric, health, location, or other personally identifiable information or sensitive data. Additionally, the use of personal data for GAI training raises risks to privacy principles such as transparency, individual participation (including consent), and purpose specification.",multi_context,"[{}, {}]",True
"What factors influence human engagement with GAI outputs in decision-making, especially in risk assessment and provenance?","['52 \n• \nMonitoring system capabilities and limitations in deployment through rigorous TEVV processes; \n• \nEvaluating how humans engage, interact with, or adapt to GAI content (especially in decision \nmaking tasks informed by GAI content), and how they react to applied provenance techniques \nsuch as overt disclosures. \nOrganizations can document and delineate GAI system objectives and limitations to identify gaps where \nprovenance data may be most useful. For instance, GAI systems used for content creation may require \nrobust watermarking techniques and corresponding detectors to identify the source of content or \nmetadata recording techniques and metadata management tools and repositories to trace content', ""Action ID \nSuggested Action \nGAI Risks \nMP-5.1-001 Apply TEVV practices for content provenance (e.g., probing a system's synthetic \ndata generation capabilities for potential misuse or vulnerabilities. \nInformation Integrity; Information \nSecurity \nMP-5.1-002 \nIdentify potential content provenance harms of GAI, such as misinformation or \ndisinformation, deepfakes, including NCII, or tampered content. Enumerate and \nrank risks based on their likelihood and potential impact, and determine how well \nprovenance solutions address speciﬁc risks and/or harms. \nInformation Integrity; Dangerous, \nViolent, or Hateful Content; \nObscene, Degrading, and/or \nAbusive Content \nMP-5.1-003""]","Factors influencing human engagement with GAI outputs in decision-making include how humans interact with or adapt to GAI content, particularly in decision-making tasks informed by GAI content, and their reactions to applied provenance techniques such as overt disclosures.",multi_context,"[{}, {}]",True
"What issues could stem from poor assumptions in decision-making, especially with biases and human-AI interactions?","['incorrect presumptions about performance; undesired homogeneity that skews system or model \noutputs, which may be erroneous, lead to ill-founded decision-making, or amplify harmful \nbiases.  \n7. Human-AI Conﬁguration: Arrangements of or interactions between a human and an AI system \nwhich can result in the human inappropriately anthropomorphizing GAI systems or experiencing \nalgorithmic aversion, automation bias, over-reliance, or emotional entanglement with GAI \nsystems. \n8. Information Integrity: Lowered barrier to entry to generate and support the exchange and \nconsumption of content which may not distinguish fact from opinion or ﬁction or acknowledge', 'bias, funding bias, groupthink) for AI Actors involved in the design, \nimplementation, and use of GAI systems; Known past GAI system incidents and \nfailure modes; In-context use and foreseeable misuse, abuse, and oﬀ-label use; \nOver reliance on quantitative metrics and methodologies without suﬃcient \nawareness of their limitations in the context(s) of use; Standard measurement \nand structured human feedback approaches; Anticipated human-AI \nconﬁgurations. \nHuman-AI Conﬁguration; Harmful \nBias and Homogenization; \nDangerous, Violent, or Hateful \nContent \nMP-1.1-004 \nIdentify and document foreseeable illegal uses or applications of the GAI system \nthat surpass organizational risk tolerances. \nCBRN Information or Capabilities;']","Poor assumptions in decision-making can lead to erroneous outputs, ill-founded decision-making, and the amplification of harmful biases. Additionally, human-AI interactions may result in inappropriate anthropomorphizing of AI systems, algorithmic aversion, automation bias, over-reliance, or emotional entanglement with these systems.",multi_context,"[{}, {}]",True
How does the value chain help manage risks with third-party data and AI while ensuring compliance?,"['personnel. \nIntellectual Property; Value Chain \nand Component Integration \nAI Actor Tasks: Operation and Monitoring, Procurement, Third-party entities \n \nGOVERN 6.2: Contingency processes are in place to handle failures or incidents in third-party data or AI systems deemed to be \nhigh-risk. \nAction ID \nSuggested Action \nGAI Risks \nGV-6.2-001 \nDocument GAI risks associated with system value chain to identify over-reliance \non third-party data and to identify fallbacks. \nValue Chain and Component \nIntegration \nGV-6.2-002 \nDocument incidents involving third-party GAI data and systems, including open-\ndata and open-source software. \nIntellectual Property; Value Chain \nand Component Integration', 'copyright, intellectual property, data privacy). \nData Privacy; Intellectual \nProperty; Value Chain and \nComponent Integration \nGV-6.1-002 Conduct joint educational activities and events in collaboration with third parties \nto promote best practices for managing GAI risks.  \nValue Chain and Component \nIntegration \nGV-6.1-003 \nDevelop and validate approaches for measuring the success of content \nprovenance management eﬀorts with third parties (e.g., incidents detected and \nresponse times). \nInformation Integrity; Value Chain \nand Component Integration \nGV-6.1-004 \nDraft and maintain well-deﬁned contracts and service level agreements (SLAs) \nthat specify content ownership, usage rights, quality standards, security']",The context does not provide specific information on how the value chain helps manage risks with third-party data and AI while ensuring compliance.,multi_context,"[{}, {}]",True
What issues come from models giving misleading info in critical areas like healthcare and law?,"['such statistical prediction can produce factually accurate and consistent outputs, it can also produce \noutputs that are factually inaccurate or internally inconsistent. This dynamic is particularly relevant when \nit comes to open-ended prompts for long-form responses and in domains which require highly \ncontextual and/or domain expertise.  \nRisks from confabulations may arise when users believe false content – often due to the conﬁdent nature \nof the response – leading users to act upon or promote the false information. This poses a challenge for \nmany real-world applications, such as in healthcare, where a confabulated summary of patient \ninformation reports could cause doctors to make incorrect diagnoses and/or recommend the wrong', 'violent recommendations, and some models have generated actionable instructions for dangerous or \n \n \n9 Confabulations of falsehoods are most commonly a problem for text-based outputs; for audio, image, or video \ncontent, creative generation of non-factual content can be a desired behavior.  \n10 For example, legal confabulations have been shown to be pervasive in current state-of-the-art LLMs. See also, \ne.g.,']","Models giving misleading information in critical areas like healthcare and law can lead to confabulations, where users believe false content due to the confident nature of the response. This can result in incorrect diagnoses or recommendations in healthcare, and pervasive legal confabulations can mislead users in legal contexts.",multi_context,"[{}, {}]",True
What methods evaluate user interactions and vulnerabilities in product dev?,"['50 \nParticipatory Engagement Methods \nOn an ad hoc or more structured basis, organizations can design and use a variety of channels to engage \nexternal stakeholders in product development or review. Focus groups with select experts can provide \nfeedback on a range of issues. Small user studies can provide feedback from representative groups or \npopulations. Anonymous surveys can be used to poll or gauge reactions to speciﬁc features. Participatory \nengagement methods are often less structured than ﬁeld testing or red teaming, and are more \ncommonly used in early stages of AI or product development.  \nField Testing \nField testing involves structured settings to evaluate risks and impacts and to simulate the conditions', '• \nField Testing: Methods used to determine how people interact with, consume, use, and make \nsense of AI-generated information, and subsequent actions and eﬀects, including UX, usability, \nand other structured, randomized experiments.  \n• \nAI Red-teaming: A structured testing exercise used to probe an AI system to ﬁnd ﬂaws and \nvulnerabilities such as inaccurate, harmful, or discriminatory outputs, often in a controlled \nenvironment and in collaboration with system developers. \nInformation gathered from structured public feedback can inform design, implementation, deployment \napproval, maintenance, or decommissioning decisions. Results and insights gleaned from these exercises']","Methods that evaluate user interactions and vulnerabilities in product development include field testing, which assesses how people interact with AI-generated information, and AI red-teaming, which probes an AI system to find flaws and vulnerabilities in a controlled environment.",multi_context,"[{}, {}]",True
Why document data sources in AI for IP and decisions?,"['Intellectual Property; Data Privacy \nAI Actor Tasks: TEVV \n \nMAP 2.2: Information about the AI system’s knowledge limits and how system output may be utilized and overseen by humans is \ndocumented. Documentation provides suﬃcient information to assist relevant AI Actors when making decisions and taking \nsubsequent actions. \nAction ID \nSuggested Action \nGAI Risks \nMP-2.2-001 \nIdentify and document how the system relies on upstream data sources, \nincluding for content provenance, and if it serves as an upstream dependency for \nother systems. \nInformation Integrity; Value Chain \nand Component Integration \nMP-2.2-002 \nObserve and analyze how the GAI system interacts with external networks, and']",The answer to given question is not present in context,reasoning,[{}],True
How do transparency and risk policies shape trustworthy AI?,"['14 \nGOVERN 1.2: The characteristics of trustworthy AI are integrated into organizational policies, processes, procedures, and practices. \nAction ID \nSuggested Action \nGAI Risks \nGV-1.2-001 \nEstablish transparency policies and processes for documenting the origin and \nhistory of training data and generated data for GAI applications to advance digital \ncontent transparency, while balancing the proprietary nature of training \napproaches. \nData Privacy; Information \nIntegrity; Intellectual Property \nGV-1.2-002 \nEstablish policies to evaluate risk-relevant capabilities of GAI and robustness of \nsafety measures, both prior to deployment and on an ongoing basis, through \ninternal and external evaluations. \nCBRN Information or Capabilities;']",The context does not provide specific information on how transparency and risk policies shape trustworthy AI.,reasoning,[{}],True
How is info accuracy ensured in GAI systems with multiple sources?,"['25 \nMP-2.3-002 Review and document accuracy, representativeness, relevance, suitability of data \nused at diﬀerent stages of AI life cycle. \nHarmful Bias and Homogenization; \nIntellectual Property \nMP-2.3-003 \nDeploy and document fact-checking techniques to verify the accuracy and \nveracity of information generated by GAI systems, especially when the \ninformation comes from multiple (or unknown) sources. \nInformation Integrity  \nMP-2.3-004 Develop and implement testing techniques to identify GAI produced content (e.g., \nsynthetic media) that might be indistinguishable from human-generated content. Information Integrity \nMP-2.3-005 Implement plans for GAI systems to undergo regular adversarial testing to identify']",Information accuracy in GAI systems with multiple sources is ensured by deploying and documenting fact-checking techniques to verify the accuracy and veracity of the information generated.,reasoning,[{}],True
What can LLMs reveal about bio threat planning challenges?,"['development, production, or use of CBRN weapons or other dangerous materials or agents. While \nrelevant biological and chemical threat knowledge and information is often publicly accessible, LLMs \ncould facilitate its analysis or synthesis, particularly by individuals without formal scientiﬁc training or \nexpertise.  \nRecent research on this topic found that LLM outputs regarding biological threat creation and attack \nplanning provided minimal assistance beyond traditional search engine queries, suggesting that state-of-\nthe-art LLMs at the time these studies were conducted do not substantially increase the operational \nlikelihood of such an attack. The physical synthesis development, production, and use of chemical or']","LLMs provide minimal assistance regarding biological threat creation and attack planning beyond traditional search engine queries, suggesting that they do not substantially increase the operational likelihood of such an attack.",reasoning,[{}],True
What risks does GAI pose with copyrighted works and its impact on content?,"['2.10. \nIntellectual Property \nIntellectual property risks from GAI systems may arise where the use of copyrighted works is not a fair \nuse under the fair use doctrine. If a GAI system’s training data included copyrighted material, GAI \noutputs displaying instances of training data memorization (see Data Privacy above) could infringe on \ncopyright. \nHow GAI relates to copyright, including the status of generated content that is similar to but does not \nstrictly copy work protected by copyright, is currently being debated in legal fora. Similar discussions are \ntaking place regarding the use or emulation of personal identity, likeness, or voice without permission.']","GAI systems pose intellectual property risks when the use of copyrighted works is not considered fair use under the fair use doctrine. If a GAI system's training data includes copyrighted material, the outputs may infringe on copyright if they display instances of training data memorization. The relationship between GAI and copyright, particularly regarding generated content that resembles but does not strictly copy copyrighted work, is currently under legal debate.",reasoning,[{}],True
How does AI red-teaming help spot issues in models with diverse teams?,"['when implementing feedback activities. \nAI Red-teaming \nAI red-teaming is an evolving practice that references exercises often conducted in a controlled \nenvironment and in collaboration with AI developers building AI models to identify potential adverse \nbehavior or outcomes of a GAI model or system, how they could occur, and stress test safeguards”. AI \nred-teaming can be performed before or after AI models or systems are made available to the broader \npublic; this section focuses on red-teaming in pre-deployment contexts.  \nThe quality of AI red-teaming outputs is related to the background and expertise of the AI red team \nitself. Demographically and interdisciplinarily diverse AI red teams can be used to identify ﬂaws in the', 'varying contexts where GAI will be used. For best results, AI red teams should demonstrate domain \nexpertise, and awareness of socio-cultural aspects within the deployment context. AI red-teaming results \nshould be given additional analysis before they are incorporated into organizational governance and \ndecision making, policy and procedural updates, and AI risk management eﬀorts. \nVarious types of AI red-teaming may be appropriate, depending on the use case: \n• \nGeneral Public: Performed by general users (not necessarily AI or technical experts) who are \nexpected to use the model or interact with its outputs, and who bring their own lived \nexperiences and perspectives to the task of AI red-teaming. These individuals may have been']","AI red-teaming helps spot issues in models with diverse teams by utilizing demographically and interdisciplinarily diverse AI red teams to identify flaws in the varying contexts where General AI (GAI) will be used. Such diversity allows for a broader range of perspectives and experiences, which can enhance the identification of potential adverse behaviors or outcomes of AI models.",conditional,"[{}, {}]",True
How do clear use policies for GAI help reduce risks of illegal apps?,"['Integrity \nAI Actor Tasks: Governance and Oversight \n \nGOVERN 1.4: The risk management process and its outcomes are established through transparent policies, procedures, and other \ncontrols based on organizational risk priorities. \nAction ID \nSuggested Action \nGAI Risks \nGV-1.4-001 \nEstablish policies and mechanisms to prevent GAI systems from generating \nCSAM, NCII or content that violates the law.  \nObscene, Degrading, and/or \nAbusive Content; Harmful Bias \nand Homogenization; \nDangerous, Violent, or Hateful \nContent \nGV-1.4-002 \nEstablish transparent acceptable use policies for GAI that address illegal use or \napplications of GAI. \nCBRN Information or \nCapabilities; Obscene, \nDegrading, and/or Abusive \nContent; Data Privacy; Civil']",The answer to given question is not present in context,conditional,[{}],True
What criteria should GAI incident reports include for better info security?,"['Establish policies for measuring the eﬀectiveness of employed content \nprovenance methodologies (e.g., cryptography, watermarking, steganography, \netc.) \nInformation Integrity \nGV-4.3-002 \nEstablish organizational practices to identify the minimum set of criteria \nnecessary for GAI system incident reporting such as: System ID (auto-generated \nmost likely), Title, Reporter, System/Source, Data Reported, Date of Incident, \nDescription, Impact(s), Stakeholder(s) Impacted. \nInformation Security']","GAI incident reports should include the following criteria for better information security: System ID (auto-generated most likely), Title, Reporter, System/Source, Data Reported, Date of Incident, Description, Impact(s), and Stakeholder(s) Impacted.",conditional,[{}],True
What could happen to data integrity and model reliability if GAI systems are tampered with?,"['retrieved. Security researchers have already demonstrated how indirect prompt injections can exploit \nvulnerabilities by stealing proprietary data or running malicious code remotely on a machine. Merely \nquerying a closed production model can elicit previously undisclosed information about that model. \nAnother cybersecurity risk to GAI is data poisoning, in which an adversary compromises a training \ndataset used by a model to manipulate its outputs or operation. Malicious tampering with data or parts \nof the model could exacerbate risks associated with GAI system outputs. \nTrustworthy AI Characteristics: Privacy Enhanced, Safe, Secure and Resilient, Valid and Reliable \n2.10. \nIntellectual Property']","If GAI systems are tampered with, it could compromise data integrity and model reliability, leading to manipulated outputs or operation due to malicious tampering with data or parts of the model.",conditional,[{}],True
How does a watermarking model balance robustness and computational efficiency in AI?,"['control over the various trade-oﬀs and cascading impacts of early-stage model decisions on downstream \nperformance and synthetic outputs. For example, by selecting a watermarking model to prioritize \nrobustness (the durability of a watermark), an AI actor may inadvertently diminish computational \ncomplexity (the resources required to implement watermarking). Organizational risk management \neﬀorts for enhancing content provenance include:  \n• \nTracking provenance of training data and metadata for GAI systems; \n• \nDocumenting provenance data limitations within GAI systems;']","The context discusses how selecting a watermarking model to prioritize robustness may inadvertently diminish computational complexity, indicating a trade-off between robustness and computational efficiency in AI.",conditional,[{}],True
