question,contexts,ground_truth,evolution_type,metadata,episode_done
What is the significance of identifying adverse behavior in AI models during the red-teaming process?,"['when implementing feedback activities. \nAI Red-teaming \nAI red-teaming is an evolving practice that references exercises often conducted in a controlled \nenvironment and in collaboration with AI developers building AI models to identify potential adverse \nbehavior or outcomes of a GAI model or system, how they could occur, and stress test safeguards”. AI \nred-teaming can be performed before or after AI models or systems are made available to the broader \npublic; this section focuses on red-teaming in pre-deployment contexts.  \nThe quality of AI red-teaming outputs is related to the background and expertise of the AI red team \nitself. Demographically and interdisciplinarily diverse AI red teams can be used to identify ﬂaws in the']","The significance of identifying adverse behavior in AI models during the red-teaming process lies in the ability to uncover potential flaws and adverse outcomes of a GAI model or system, which can then be addressed to improve the safety and effectiveness of the AI before it is made available to the public.",simple,[{}],True
What privacy risks are associated with data memorization in GAI models?,"['was trained on and, if so, how it was collected.  \nModels may leak, generate, or correctly infer sensitive information about individuals. For example, \nduring adversarial attacks, LLMs have revealed sensitive information (from the public domain) that was \nincluded in their training data. This problem has been referred to as data memorization, and may pose \nexacerbated privacy risks even for data present only in a small number of training samples.  \nIn addition to revealing sensitive information in GAI training data, GAI models may be able to correctly \ninfer PII or sensitive data that was not in their training data nor disclosed by the user by stitching']","Data memorization in GAI models poses privacy risks by potentially leaking, generating, or correctly inferring sensitive information about individuals. This includes revealing sensitive information from the public domain that was included in their training data, which may exacerbate privacy risks even for data present only in a small number of training samples.",simple,[{}],True
What is the significance of digital content transparency in the context of information integrity?,"['vulnerabilities and potential manipulation or misuse. \nInformation Security \nAI Actor Tasks: AI Development, Domain Experts, TEVV \n \nMAP 3.4: Processes for operator and practitioner proﬁciency with AI system performance and trustworthiness – and relevant \ntechnical standards and certiﬁcations – are deﬁned, assessed, and documented. \nAction ID \nSuggested Action \nGAI Risks \nMP-3.4-001 \nEvaluate whether GAI operators and end-users can accurately understand \ncontent lineage and origin. \nHuman-AI Conﬁguration; \nInformation Integrity \nMP-3.4-002 Adapt existing training programs to include modules on digital content \ntransparency. \nInformation Integrity \nMP-3.4-003 Develop certiﬁcation programs that test proﬁciency in managing GAI risks and']",The answer to given question is not present in context,simple,[{}],True
What characteristics define information that is considered accurate and reliable?,"['2.8. Information Integrity \nInformation integrity describes the “spectrum of information and associated patterns of its creation, \nexchange, and consumption in society.” High-integrity information can be trusted; “distinguishes fact \nfrom ﬁction, opinion, and inference; acknowledges uncertainties; and is transparent about its level of \nvetting. This information can be linked to the original source(s) with appropriate evidence. High-integrity \ninformation is also accurate and reliable, can be veriﬁed and authenticated, has a clear chain of custody, \nand creates reasonable expectations about when its validity may expire.”11 \n \n \n11 This deﬁnition of information integrity is derived from the 2022 White House Roadmap for Researchers on']","Information that is considered accurate and reliable is high-integrity information that can be trusted, distinguishes fact from fiction, opinion, and inference, acknowledges uncertainties, and is transparent about its level of vetting. It can be linked to the original source(s) with appropriate evidence, is verifiable and authenticatable, has a clear chain of custody, and creates reasonable expectations about when its validity may expire.",simple,[{}],True
What are the potential consequences of algorithmic aversion in human-AI interactions?,"['incorrect presumptions about performance; undesired homogeneity that skews system or model \noutputs, which may be erroneous, lead to ill-founded decision-making, or amplify harmful \nbiases.  \n7. Human-AI Conﬁguration: Arrangements of or interactions between a human and an AI system \nwhich can result in the human inappropriately anthropomorphizing GAI systems or experiencing \nalgorithmic aversion, automation bias, over-reliance, or emotional entanglement with GAI \nsystems. \n8. Information Integrity: Lowered barrier to entry to generate and support the exchange and \nconsumption of content which may not distinguish fact from opinion or ﬁction or acknowledge']",The answer to given question is not present in context,simple,[{}],True
What are the concerns associated with harmful bias in the context of AI systems?,"['25 \nMP-2.3-002 Review and document accuracy, representativeness, relevance, suitability of data \nused at diﬀerent stages of AI life cycle. \nHarmful Bias and Homogenization; \nIntellectual Property \nMP-2.3-003 \nDeploy and document fact-checking techniques to verify the accuracy and \nveracity of information generated by GAI systems, especially when the \ninformation comes from multiple (or unknown) sources. \nInformation Integrity  \nMP-2.3-004 Develop and implement testing techniques to identify GAI produced content (e.g., \nsynthetic media) that might be indistinguishable from human-generated content. Information Integrity \nMP-2.3-005 Implement plans for GAI systems to undergo regular adversarial testing to identify']",The answer to given question is not present in context,simple,[{}],True
What processes should be implemented to respond to potential intellectual property infringement claims?,"['26 \nMAP 4.1: Approaches for mapping AI technology and legal risks of its components – including the use of third-party data or \nsoftware – are in place, followed, and documented, as are risks of infringement of a third-party’s intellectual property or other \nrights. \nAction ID \nSuggested Action \nGAI Risks \nMP-4.1-001 Conduct periodic monitoring of AI-generated content for privacy risks; address any \npossible instances of PII or sensitive data exposure. \nData Privacy \nMP-4.1-002 Implement processes for responding to potential intellectual property infringement \nclaims or other rights. \nIntellectual Property \nMP-4.1-003 \nConnect new GAI policies, procedures, and processes to existing model, data,']","Processes for responding to potential intellectual property infringement claims should be implemented as suggested in the action MP-4.1-002, which involves addressing such claims or other rights.",simple,[{}],True
What are the concerns associated with harmful bias in generated content?,"['biases in the generated content. \nInformation Security; Harmful Bias \nand Homogenization \nMG-2.2-005 \nEngage in due diligence to analyze GAI output for harmful content, potential \nmisinformation, and CBRN-related or NCII content. \nCBRN Information or Capabilities; \nObscene, Degrading, and/or \nAbusive Content; Harmful Bias and \nHomogenization; Dangerous, \nViolent, or Hateful Content']","The concerns associated with harmful bias in generated content include the potential for misinformation, the presence of obscene, degrading, and abusive content, and the risk of dangerous, violent, or hateful content.",simple,[{}],True
What are the requirements for reporting incidents related to NHTSA autonomous vehicle crash reporting?,"['46 \nMG-4.3-003 \nReport GAI incidents in compliance with legal and regulatory requirements (e.g., \nHIPAA breach reporting, e.g., OCR (2023) or NHTSA (2022) autonomous vehicle \ncrash reporting requirements. \nInformation Security; Data Privacy \nAI Actor Tasks: AI Deployment, Aﬀected Individuals and Communities, Domain Experts, End-Users, Human Factors, Operation and \nMonitoring']",The answer to given question is not present in context,simple,[{}],True
What is the significance of data provenance in the context of GAI systems?,['systems can be applied to GAI systems. These plans and actions include: \n• Accessibility and reasonable \naccommodations \n• AI actor credentials and qualiﬁcations  \n• Alignment to organizational values \n• Auditing and assessment \n• Change-management controls \n• Commercial use \n• Data provenance'],The answer to given question is not present in context,simple,[{}],True
What considerations should be taken into account when applying organizational risk tolerances to third-party models?,"['GAI resources; Apply organizational risk tolerances to ﬁne-tuned third-party \nmodels; Apply organizational risk tolerance to existing third-party models \nadapted to a new domain; Reassess risk measurements after ﬁne-tuning third-\nparty GAI models. \nValue Chain and Component \nIntegration; Intellectual Property \nMG-3.1-002 \nTest GAI system value chain risks (e.g., data poisoning, malware, other software \nand hardware vulnerabilities; labor practices; data privacy and localization \ncompliance; geopolitical alignment). \nData Privacy; Information Security; \nValue Chain and Component \nIntegration; Harmful Bias and \nHomogenization \nMG-3.1-003 \nRe-assess model risks after ﬁne-tuning or retrieval-augmented generation']","The considerations that should be taken into account when applying organizational risk tolerances to third-party models include reassessing risk measurements after fine-tuning third-party GAI models, testing GAI system value chain risks such as data poisoning, malware, software and hardware vulnerabilities, labor practices, data privacy and localization compliance, and geopolitical alignment.",simple,[{}],True
What methods can be used to analyze user perceptions of content authenticity?,"['against industry standards and best practices. Compare GAI system security \nfeatures and content provenance methods against industry state-of-the-art. \nInformation Integrity; Information \nSecurity \nMS-2.7-003 \nConduct user surveys to gather user satisfaction with the AI-generated content \nand user perceptions of content authenticity. Analyze user feedback to identify \nconcerns and/or current literacy levels related to content provenance and \nunderstanding of labels on content. \nHuman-AI Conﬁguration; \nInformation Integrity \nMS-2.7-004 \nIdentify metrics that reﬂect the eﬀectiveness of security measures, such as data \nprovenance, the number of unauthorized access attempts, inference, bypass,']",User surveys can be conducted to gather user satisfaction with AI-generated content and user perceptions of content authenticity. Analyzing user feedback can help identify concerns and current literacy levels related to content provenance and understanding of labels on content.,simple,[{}],True
What is the purpose of service level agreements (SLAs) in managing content ownership and usage rights?,"['copyright, intellectual property, data privacy). \nData Privacy; Intellectual \nProperty; Value Chain and \nComponent Integration \nGV-6.1-002 Conduct joint educational activities and events in collaboration with third parties \nto promote best practices for managing GAI risks.  \nValue Chain and Component \nIntegration \nGV-6.1-003 \nDevelop and validate approaches for measuring the success of content \nprovenance management eﬀorts with third parties (e.g., incidents detected and \nresponse times). \nInformation Integrity; Value Chain \nand Component Integration \nGV-6.1-004 \nDraft and maintain well-deﬁned contracts and service level agreements (SLAs) \nthat specify content ownership, usage rights, quality standards, security']",The answer to given question is not present in context,simple,[{}],True
What is the significance of the fair use doctrine in relation to intellectual property risks from GAI systems?,"['2.10. \nIntellectual Property \nIntellectual property risks from GAI systems may arise where the use of copyrighted works is not a fair \nuse under the fair use doctrine. If a GAI system’s training data included copyrighted material, GAI \noutputs displaying instances of training data memorization (see Data Privacy above) could infringe on \ncopyright. \nHow GAI relates to copyright, including the status of generated content that is similar to but does not \nstrictly copy work protected by copyright, is currently being debated in legal fora. Similar discussions are \ntaking place regarding the use or emulation of personal identity, likeness, or voice without permission.']","The significance of the fair use doctrine in relation to intellectual property risks from GAI systems lies in determining whether the use of copyrighted works in training data is considered fair use. If a GAI system's training data includes copyrighted material and the outputs display instances of training data memorization, it could potentially infringe on copyright. The fair use doctrine is crucial in assessing these risks and the legal status of generated content that resembles copyrighted work.",simple,[{}],True
What are the considerations involved in Human-AI Configuration regarding harmful content?,"['Human-AI Conﬁguration; Obscene, \nDegrading, and/or Abusive \nContent; Value Chain and \nComponent Integration; \nDangerous, Violent, or Hateful \nContent \nMS-2.6-002 \nAssess existence or levels of harmful bias, intellectual property infringement, \ndata privacy violations, obscenity, extremism, violence, or CBRN information in \nsystem training data. \nData Privacy; Intellectual Property; \nObscene, Degrading, and/or \nAbusive Content; Harmful Bias and \nHomogenization; Dangerous, \nViolent, or Hateful Content; CBRN \nInformation or Capabilities \nMS-2.6-003 Re-evaluate safety features of ﬁne-tuned models when the negative risk exceeds \norganizational risk tolerance. \nDangerous, Violent, or Hateful \nContent']","Considerations involved in Human-AI Configuration regarding harmful content include assessing the existence or levels of harmful bias, intellectual property infringement, data privacy violations, obscenity, extremism, violence, or CBRN information in system training data. Additionally, it involves re-evaluating safety features of fine-tuned models when the negative risk exceeds organizational risk tolerance, particularly concerning dangerous, violent, or hateful content.",simple,[{}],True
"What harms from GAI, like misinformation, can regular adversarial testing reveal, and how can we mitigate them?","[""Action ID \nSuggested Action \nGAI Risks \nMP-5.1-001 Apply TEVV practices for content provenance (e.g., probing a system's synthetic \ndata generation capabilities for potential misuse or vulnerabilities. \nInformation Integrity; Information \nSecurity \nMP-5.1-002 \nIdentify potential content provenance harms of GAI, such as misinformation or \ndisinformation, deepfakes, including NCII, or tampered content. Enumerate and \nrank risks based on their likelihood and potential impact, and determine how well \nprovenance solutions address speciﬁc risks and/or harms. \nInformation Integrity; Dangerous, \nViolent, or Hateful Content; \nObscene, Degrading, and/or \nAbusive Content \nMP-5.1-003"", 'Action ID \nSuggested Action \nGAI Risks \nMS-4.2-001 \nConduct adversarial testing at a regular cadence to map and measure GAI risks, \nincluding tests to address attempts to deceive or manipulate the application of \nprovenance techniques or other misuses. Identify vulnerabilities and \nunderstand potential misuse scenarios and unintended outputs. \nInformation Integrity; Information \nSecurity \nMS-4.2-002 \nEvaluate GAI system performance in real-world scenarios to observe its \nbehavior in practical environments and reveal issues that might not surface in \ncontrolled and optimized testing environments. \nHuman-AI Conﬁguration; \nConfabulation; Information \nSecurity \nMS-4.2-003']","The context mentions that potential content provenance harms of GAI include misinformation or disinformation, deepfakes, and tampered content. Regular adversarial testing can help identify vulnerabilities and understand potential misuse scenarios and unintended outputs, which can aid in mitigating these harms by mapping and measuring GAI risks.",multi_context,"[{}, {}]",True
How do AI red teams' expertise and diversity impact governance and decision-making?,"['varying contexts where GAI will be used. For best results, AI red teams should demonstrate domain \nexpertise, and awareness of socio-cultural aspects within the deployment context. AI red-teaming results \nshould be given additional analysis before they are incorporated into organizational governance and \ndecision making, policy and procedural updates, and AI risk management eﬀorts. \nVarious types of AI red-teaming may be appropriate, depending on the use case: \n• \nGeneral Public: Performed by general users (not necessarily AI or technical experts) who are \nexpected to use the model or interact with its outputs, and who bring their own lived \nexperiences and perspectives to the task of AI red-teaming. These individuals may have been', 'when implementing feedback activities. \nAI Red-teaming \nAI red-teaming is an evolving practice that references exercises often conducted in a controlled \nenvironment and in collaboration with AI developers building AI models to identify potential adverse \nbehavior or outcomes of a GAI model or system, how they could occur, and stress test safeguards”. AI \nred-teaming can be performed before or after AI models or systems are made available to the broader \npublic; this section focuses on red-teaming in pre-deployment contexts.  \nThe quality of AI red-teaming outputs is related to the background and expertise of the AI red team \nitself. Demographically and interdisciplinarily diverse AI red teams can be used to identify ﬂaws in the']","The context mentions that AI red teams should demonstrate domain expertise and awareness of socio-cultural aspects, which are important for the quality of AI red-teaming outputs. Additionally, it states that demographically and interdisciplinarily diverse AI red teams can be used to identify flaws, suggesting that their expertise and diversity positively impact governance and decision-making.",multi_context,"[{}, {}]",True
"What issues could stem from poor assumptions in decision-making, especially with biases and human-AI interactions?","['incorrect presumptions about performance; undesired homogeneity that skews system or model \noutputs, which may be erroneous, lead to ill-founded decision-making, or amplify harmful \nbiases.  \n7. Human-AI Conﬁguration: Arrangements of or interactions between a human and an AI system \nwhich can result in the human inappropriately anthropomorphizing GAI systems or experiencing \nalgorithmic aversion, automation bias, over-reliance, or emotional entanglement with GAI \nsystems. \n8. Information Integrity: Lowered barrier to entry to generate and support the exchange and \nconsumption of content which may not distinguish fact from opinion or ﬁction or acknowledge', 'bias, funding bias, groupthink) for AI Actors involved in the design, \nimplementation, and use of GAI systems; Known past GAI system incidents and \nfailure modes; In-context use and foreseeable misuse, abuse, and oﬀ-label use; \nOver reliance on quantitative metrics and methodologies without suﬃcient \nawareness of their limitations in the context(s) of use; Standard measurement \nand structured human feedback approaches; Anticipated human-AI \nconﬁgurations. \nHuman-AI Conﬁguration; Harmful \nBias and Homogenization; \nDangerous, Violent, or Hateful \nContent \nMP-1.1-004 \nIdentify and document foreseeable illegal uses or applications of the GAI system \nthat surpass organizational risk tolerances. \nCBRN Information or Capabilities;']","Poor assumptions in decision-making can lead to erroneous outputs, ill-founded decision-making, and the amplification of harmful biases. Additionally, human-AI interactions may result in inappropriate anthropomorphizing of AI systems, algorithmic aversion, automation bias, over-reliance, or emotional entanglement with these systems.",multi_context,"[{}, {}]",True
"What protocols ensure GAI can monitor, recover, and document security issues with AI involvement?","['organizational risk tolerance. \nDangerous, Violent, or Hateful \nContent \nMS-2.6-004 Review GAI system outputs for validity and safety: Review generated code to \nassess risks that may arise from unreliable downstream decision-making. \nValue Chain and Component \nIntegration; Dangerous, Violent, or \nHateful Content \nMS-2.6-005 \nVerify that GAI system architecture can monitor outputs and performance, and \nhandle, recover from, and repair errors when security anomalies, threats and \nimpacts are detected. \nConfabulation; Information \nIntegrity; Information Security \nMS-2.6-006 \nVerify that systems properly handle queries that may give rise to inappropriate, \nmalicious, or illegal usage, including facilitating manipulation, extortion, targeted', 'Intellectual Property; Dangerous, \nViolent, or Hateful Content; \nObscene, Degrading, and/or \nAbusive Content \nGV-4.2-002 \nInclude relevant AI Actors in the GAI system risk identiﬁcation process. \nHuman-AI Conﬁguration \nGV-4.2-003 \nVerify that downstream GAI system impacts (such as the use of third-party \nplugins) are included in the impact documentation process. \nValue Chain and Component \nIntegration \nAI Actor Tasks: AI Deployment, AI Design, AI Development, Operation and Monitoring \n \nGOVERN 4.3: Organizational practices are in place to enable AI testing, identiﬁcation of incidents, and information sharing. \nAction ID \nSuggested Action \nGAI Risks \nGV4.3--001 \nEstablish policies for measuring the eﬀectiveness of employed content']","The protocols ensuring GAI can monitor, recover, and document security issues with AI involvement include verifying that the GAI system architecture can monitor outputs and performance, handle, recover from, and repair errors when security anomalies, threats, and impacts are detected. Additionally, it involves including relevant AI Actors in the GAI system risk identification process and ensuring that downstream GAI system impacts, such as the use of third-party plugins, are included in the impact documentation process.",multi_context,"[{}, {}]",True
How do diverse AI red teams help spot issues in models before deployment?,"['when implementing feedback activities. \nAI Red-teaming \nAI red-teaming is an evolving practice that references exercises often conducted in a controlled \nenvironment and in collaboration with AI developers building AI models to identify potential adverse \nbehavior or outcomes of a GAI model or system, how they could occur, and stress test safeguards”. AI \nred-teaming can be performed before or after AI models or systems are made available to the broader \npublic; this section focuses on red-teaming in pre-deployment contexts.  \nThe quality of AI red-teaming outputs is related to the background and expertise of the AI red team \nitself. Demographically and interdisciplinarily diverse AI red teams can be used to identify ﬂaws in the', 'varying contexts where GAI will be used. For best results, AI red teams should demonstrate domain \nexpertise, and awareness of socio-cultural aspects within the deployment context. AI red-teaming results \nshould be given additional analysis before they are incorporated into organizational governance and \ndecision making, policy and procedural updates, and AI risk management eﬀorts. \nVarious types of AI red-teaming may be appropriate, depending on the use case: \n• \nGeneral Public: Performed by general users (not necessarily AI or technical experts) who are \nexpected to use the model or interact with its outputs, and who bring their own lived \nexperiences and perspectives to the task of AI red-teaming. These individuals may have been']","Diverse AI red teams help spot issues in models before deployment by bringing a range of backgrounds and expertise, which allows them to identify flaws in the varying contexts where General AI (GAI) will be used. Their demographic and interdisciplinary diversity enhances the identification of potential adverse behaviors or outcomes, ensuring a more comprehensive evaluation of the AI models.",multi_context,"[{}, {}]",True
How do clear roles impact GAI oversight in risk management?,"['16 \nGOVERN 1.5: Ongoing monitoring and periodic review of the risk management process and its outcomes are planned, and \norganizational roles and responsibilities are clearly deﬁned, including determining the frequency of periodic review. \nAction ID \nSuggested Action \nGAI Risks \nGV-1.5-001 Deﬁne organizational responsibilities for periodic review of content provenance \nand incident monitoring for GAI systems. \nInformation Integrity \nGV-1.5-002 \nEstablish organizational policies and procedures for after action reviews of GAI \nsystem incident response and incident disclosures, to identify gaps; Update \nincident response and incident disclosure processes as required. \nHuman-AI Conﬁguration; \nInformation Security \nGV-1.5-003', '18 \nGOVERN 3.2: Policies and procedures are in place to deﬁne and diﬀerentiate roles and responsibilities for human-AI conﬁgurations \nand oversight of AI systems. \nAction ID \nSuggested Action \nGAI Risks \nGV-3.2-001 \nPolicies are in place to bolster oversight of GAI systems with independent \nevaluations or assessments of GAI models or systems where the type and \nrobustness of evaluations are proportional to the identiﬁed risks. \nCBRN Information or Capabilities; \nHarmful Bias and Homogenization \nGV-3.2-002 \nConsider adjustment of organizational roles and components across lifecycle \nstages of large or complex GAI systems, including: Test and evaluation, validation, \nand red-teaming of GAI systems; GAI content moderation; GAI system']","Clear roles and responsibilities are essential for effective oversight of GAI systems in risk management, as they help define and differentiate the responsibilities for human-AI configurations and ensure that policies and procedures are in place to bolster oversight through independent evaluations or assessments proportional to identified risks.",multi_context,"[{}, {}]",True
How do measurement error models help validate pre-deployment metrics in TEVV for AI risks and biases?,"['38 \nMEASURE 2.13: Eﬀectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and \ndocumented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.13-001 \nCreate measurement error models for pre-deployment metrics to demonstrate \nconstruct validity for each metric (i.e., does the metric eﬀectively operationalize \nthe desired concept): Measure or estimate, and document, biases or statistical \nvariance in applied metrics or structured human feedback processes; Leverage \ndomain expertise when modeling complex societal constructs such as hateful \ncontent. \nConfabulation; Information \nIntegrity; Harmful Bias and \nHomogenization \nAI Actor Tasks: AI Deployment, Operation and Monitoring, TEVV']",The context does not provide information on how measurement error models help validate pre-deployment metrics in TEVV for AI risks and biases.,multi_context,[{}],True
How can user feedback improve AI content authenticity?,"['41 \nMG-2.2-006 \nUse feedback from internal and external AI Actors, users, individuals, and \ncommunities, to assess impact of AI-generated content. \nHuman-AI Conﬁguration \nMG-2.2-007 \nUse real-time auditing tools where they can be demonstrated to aid in the \ntracking and validation of the lineage and authenticity of AI-generated data. \nInformation Integrity \nMG-2.2-008 \nUse structured feedback mechanisms to solicit and capture user input about AI-\ngenerated content to detect subtle shifts in quality or alignment with \ncommunity and societal values. \nHuman-AI Conﬁguration; Harmful \nBias and Homogenization \nMG-2.2-009 \nConsider opportunities to responsibly use synthetic data and other privacy', '39 \nMS-3.3-004 \nProvide input for training materials about the capabilities and limitations of GAI \nsystems related to digital content transparency for AI Actors, other \nprofessionals, and the public about the societal impacts of AI and the role of \ndiverse and inclusive content generation. \nHuman-AI Conﬁguration; \nInformation Integrity; Harmful Bias \nand Homogenization \nMS-3.3-005 \nRecord and integrate structured feedback about content provenance from \noperators, users, and potentially impacted communities through the use of \nmethods such as user research studies, focus groups, or community forums. \nActively seek feedback on generated content quality and potential biases.']","User feedback can improve AI content authenticity by using structured feedback mechanisms to solicit and capture user input about AI-generated content, which helps detect subtle shifts in quality or alignment with community and societal values. Additionally, real-time auditing tools can aid in tracking and validating the lineage and authenticity of AI-generated data.",multi_context,"[{}, {}]",True
What copyright and cybersecurity risks might GAI systems pose?,"['2.10. \nIntellectual Property \nIntellectual property risks from GAI systems may arise where the use of copyrighted works is not a fair \nuse under the fair use doctrine. If a GAI system’s training data included copyrighted material, GAI \noutputs displaying instances of training data memorization (see Data Privacy above) could infringe on \ncopyright. \nHow GAI relates to copyright, including the status of generated content that is similar to but does not \nstrictly copy work protected by copyright, is currently being debated in legal fora. Similar discussions are \ntaking place regarding the use or emulation of personal identity, likeness, or voice without permission.', 'retrieved. Security researchers have already demonstrated how indirect prompt injections can exploit \nvulnerabilities by stealing proprietary data or running malicious code remotely on a machine. Merely \nquerying a closed production model can elicit previously undisclosed information about that model. \nAnother cybersecurity risk to GAI is data poisoning, in which an adversary compromises a training \ndataset used by a model to manipulate its outputs or operation. Malicious tampering with data or parts \nof the model could exacerbate risks associated with GAI system outputs. \nTrustworthy AI Characteristics: Privacy Enhanced, Safe, Secure and Resilient, Valid and Reliable \n2.10. \nIntellectual Property']","GAI systems may pose copyright risks if their training data includes copyrighted material, leading to potential infringement through outputs that display memorization of training data. Additionally, there are cybersecurity risks such as indirect prompt injections that can exploit vulnerabilities, steal proprietary data, or run malicious code. Data poisoning is another risk, where an adversary compromises a training dataset to manipulate a model's outputs or operation.",multi_context,"[{}, {}]",True
What causes measurement gaps in GAI reliability between real-world and lab settings?,"['humans (e.g., intelligence tests, professional licensing exams) does not guarantee GAI system validity or \nreliability in those domains. Similarly, jailbreaking or prompt engineering tests may not systematically \nassess validity or reliability risks.  \nMeasurement gaps can arise from mismatches between laboratory and real-world settings. Current \ntesting approaches often remain focused on laboratory conditions or restricted to benchmark test \ndatasets and in silico techniques that may not extrapolate well to—or directly assess GAI impacts in real-\nworld conditions. For example, current measurement gaps for GAI make it diﬃcult to precisely estimate']","Measurement gaps in GAI reliability arise from mismatches between laboratory and real-world settings. Current testing approaches often focus on laboratory conditions or are restricted to benchmark test datasets and in silico techniques, which may not extrapolate well to or directly assess GAI impacts in real-world conditions.",reasoning,[{}],True
What do LLMs reveal about bio threats vs. search engines?,"['development, production, or use of CBRN weapons or other dangerous materials or agents. While \nrelevant biological and chemical threat knowledge and information is often publicly accessible, LLMs \ncould facilitate its analysis or synthesis, particularly by individuals without formal scientiﬁc training or \nexpertise.  \nRecent research on this topic found that LLM outputs regarding biological threat creation and attack \nplanning provided minimal assistance beyond traditional search engine queries, suggesting that state-of-\nthe-art LLMs at the time these studies were conducted do not substantially increase the operational \nlikelihood of such an attack. The physical synthesis development, production, and use of chemical or']","LLMs provide minimal assistance regarding biological threat creation and attack planning compared to traditional search engine queries, suggesting that they do not substantially increase the operational likelihood of such an attack.",reasoning,[{}],True
What biases could stem from faulty performance assumptions in decisions?,"['incorrect presumptions about performance; undesired homogeneity that skews system or model \noutputs, which may be erroneous, lead to ill-founded decision-making, or amplify harmful \nbiases.  \n7. Human-AI Conﬁguration: Arrangements of or interactions between a human and an AI system \nwhich can result in the human inappropriately anthropomorphizing GAI systems or experiencing \nalgorithmic aversion, automation bias, over-reliance, or emotional entanglement with GAI \nsystems. \n8. Information Integrity: Lowered barrier to entry to generate and support the exchange and \nconsumption of content which may not distinguish fact from opinion or ﬁction or acknowledge']","Faulty performance assumptions can lead to erroneous outputs, ill-founded decision-making, and the amplification of harmful biases.",reasoning,[{}],True
"What IP risks come from GAI using copyrighted material without fair use, especially in data memorization and identity emulation?","['2.10. \nIntellectual Property \nIntellectual property risks from GAI systems may arise where the use of copyrighted works is not a fair \nuse under the fair use doctrine. If a GAI system’s training data included copyrighted material, GAI \noutputs displaying instances of training data memorization (see Data Privacy above) could infringe on \ncopyright. \nHow GAI relates to copyright, including the status of generated content that is similar to but does not \nstrictly copy work protected by copyright, is currently being debated in legal fora. Similar discussions are \ntaking place regarding the use or emulation of personal identity, likeness, or voice without permission.']","Intellectual property risks from GAI systems may arise where the use of copyrighted works is not a fair use under the fair use doctrine. If a GAI system’s training data included copyrighted material, GAI outputs displaying instances of training data memorization could infringe on copyright. Additionally, there are discussions regarding the use or emulation of personal identity, likeness, or voice without permission.",conditional,[{}],True
What are the criteria for escalating GAI incidents to risk mgmt if deactivation is needed?,"['42 \nMG-2.4-002 \nEstablish and maintain procedures for escalating GAI system incidents to the \norganizational risk management authority when speciﬁc criteria for deactivation \nor disengagement is met for a particular context of use or for the GAI system as a \nwhole. \nInformation Security \nMG-2.4-003 \nEstablish and maintain procedures for the remediation of issues which trigger \nincident response processes for the use of a GAI system, and provide stakeholders \ntimelines associated with the remediation plan. \nInformation Security \n \nMG-2.4-004 Establish and regularly review speciﬁc criteria that warrants the deactivation of \nGAI systems in accordance with set risk tolerances and appetites. \nInformation Security']",The specific criteria for escalating GAI system incidents to the organizational risk management authority when deactivation or disengagement is needed are not detailed in the provided context.,conditional,[{}],True
How does the AI RMF support trust in voluntary Generative AI adoption?,"['1 \n1. \nIntroduction \nThis document is a cross-sectoral proﬁle of and companion resource for the AI Risk Management \nFramework (AI RMF 1.0) for Generative AI,1 pursuant to President Biden’s Executive Order (EO) 14110 on \nSafe, Secure, and Trustworthy Artiﬁcial Intelligence.2 The AI RMF was released in January 2023, and is \nintended for voluntary use and to improve the ability of organizations to incorporate trustworthiness \nconsiderations into the design, development, use, and evaluation of AI products, services, and systems.  \nA proﬁle is an implementation of the AI RMF functions, categories, and subcategories for a speciﬁc \nsetting, application, or technology – in this case, Generative AI (GAI) – based on the requirements, risk']",The answer to given question is not present in context,conditional,[{}],True
