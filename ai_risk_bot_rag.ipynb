{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0: Imports, constants, and API Keys!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain==0.2.16 langchain_core==0.2.38 langchain_community==0.2.16 pymupdf openai \n",
    "!pip install -q langchain_openai==0.1.23 langchain-qdrant qdrant_client asyncio ragas==0.1.14 pandas\n",
    "!pip install -q transformers sentence-transformers langchain_huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall\n",
    ")\n",
    "RAGAS_METRICS = [ faithfulness, answer_relevancy, context_precision, context_recall ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "# collect OpenAI key\n",
    "openai.api_key = getpass(\"OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Download and chunk the data**\n",
    "\n",
    "We are going to use the following docs as our knowledge base:\n",
    "1. Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People (PDF)\n",
    "2. National Institute of Standards and Technology (NIST) Artificial Intelligent Risk Management Framework \n",
    "\n",
    "Let's start with a simple fixed chunking strategy as a baseline, and later evaluate parent-doc retrieval if we have time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/Desktop/ai_makerspace/code/ai-risk-bot/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf...\n",
      "Chunking...\n",
      "Loading https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf...\n",
      "Chunking...\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import utils\n",
    "from vars import PDFS, CHUNK_SIZE, OVERLAP\n",
    "\n",
    "importlib.reload(utils)\n",
    "for pdf in PDFS:\n",
    "    chunks = await utils.load_and_chunk_pdf(pdf,CHUNK_SIZE,OVERLAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Construct and test baseline RAG Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created qdrant client\n",
      "created embeddings\n",
      "populated vector db\n",
      "created chain\n"
     ]
    }
   ],
   "source": [
    "from vars import BASELINE_CHAT_MODEL, BASELINE_EMBEDDING_MODEL\n",
    "\n",
    "importlib.reload(utils)\n",
    "rag_chain = await utils.vanilla_openai_rag_chain(texts=chunks, \n",
    "                                            openai_key=openai.api_key, \n",
    "                                            embedding_model=BASELINE_EMBEDDING_MODEL,\n",
    "                                            chat_model=BASELINE_CHAT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': [Document(metadata={'_id': 'fa369ee3fbb5442d89cbc2cb81c8c414', '_collection_name': 'default'}, page_content='with greater ease and scale than other technologies. LLMs have been reported to generate dangerous or \\nviolent recommendations, and some models have generated actionable instructions for dangerous or \\n \\n \\n9 Confabulations of falsehoods are most commonly a problem for text-based outputs; for audio, image, or video \\ncontent, creative generation of non-factual content can be a desired behavior.  \\n10 For example, legal confabulations have been shown to be pervasive in current state-of-the-art LLMs. See also, \\ne.g.,'),\n",
      "             Document(metadata={'_id': '3b69051d246044dca5186263d701224b', '_collection_name': 'default'}, page_content='development, production, or use of CBRN weapons or other dangerous materials or agents. While \\nrelevant biological and chemical threat knowledge and information is often publicly accessible, LLMs \\ncould facilitate its analysis or synthesis, particularly by individuals without formal scientiﬁc training or \\nexpertise.  \\nRecent research on this topic found that LLM outputs regarding biological threat creation and attack \\nplanning provided minimal assistance beyond traditional search engine queries, suggesting that state-of-\\nthe-art LLMs at the time these studies were conducted do not substantially increase the operational \\nlikelihood of such an attack. The physical synthesis development, production, and use of chemical or \\nbiological agents will continue to require both applicable expertise and supporting materials and \\ninfrastructure. The impact of GAI on chemical or biological agent misuse will depend on what the key \\nbarriers for malicious actors are (e.g., whether information access is one such barrier), and how well GAI \\ncan help actors address those barriers.  \\nFurthermore, chemical and biological design tools (BDTs) – highly specialized AI systems trained on \\nscientiﬁc data that aid in chemical and biological design – may augment design capabilities in chemistry \\nand biology beyond what text-based LLMs are able to provide. As these models become more \\neﬃcacious, including for beneﬁcial uses, it will be important to assess their potential to be used for'),\n",
      "             Document(metadata={'_id': '8e81fd2724fc452f88840100287ecbe7', '_collection_name': 'default'}, page_content='information reports could cause doctors to make incorrect diagnoses and/or recommend the wrong \\ntreatments. Risks of confabulated content may be especially important to monitor when integrating GAI \\ninto applications involving consequential decision making. \\nGAI outputs may also include confabulated logic or citations that purport to justify or explain the \\nsystem’s answer, which may further mislead humans into inappropriately trusting the system’s output. \\nFor instance, LLMs sometimes provide logical steps for how they arrived at an answer even when the \\nanswer itself is incorrect. Similarly, an LLM could falsely assert that it is human or has human traits, \\npotentially deceiving humans into believing they are speaking with another human. \\nThe extent to which humans can be deceived by LLMs, the mechanisms by which this may occur, and the \\npotential risks from adversarial prompting of such behavior are emerging areas of study. Given the wide \\nrange of downstream impacts of GAI, it is diﬃcult to estimate the downstream scale and impact of \\nconfabulations. \\nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Valid and Reliable, Explainable \\nand Interpretable \\n2.3. Dangerous, Violent, or Hateful Content \\nGAI systems can produce content that is inciting, radicalizing, or threatening, or that gloriﬁes violence, \\nwith greater ease and scale than other technologies. LLMs have been reported to generate dangerous or'),\n",
      "             Document(metadata={'_id': '64be5d02a3a74979981fc3c8a9c20aeb', '_collection_name': 'default'}, page_content=\"Models. Project Zero. https://googleprojectzero.blogspot.com/2024/06/project-naptime.html \\nGreshake, K. et al. (2023) Not what you've signed up for: Compromising Real-World LLM-Integrated \\nApplications with Indirect Prompt Injection. arXiv. https://arxiv.org/abs/2302.12173 \\nHagan, M. (2024) Good AI Legal Help, Bad AI Legal Help: Establishing quality standards for responses to \\npeople’s legal problem stories. SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4696936 \\nHaran, R. (2023) Securing LLM Systems Against Prompt Injection. NVIDIA. \\nhttps://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/ \\nInformation Technology Industry Council (2024) Authenticating AI-Generated Content. \\nhttps://www.itic.org/policy/ITI_AIContentAuthorizationPolicy_122123.pdf \\nJain, S. et al. (2023) Algorithmic Pluralism: A Structural Approach To Equal Opportunity. arXiv. \\nhttps://arxiv.org/pdf/2305.08157 \\nJi, Z. et al (2023) Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55, 12, \\nArticle 248. https://doi.org/10.1145/3571730 \\nJones-Jang, S. et al. (2022) How do people react to AI failure? Automation bias, algorithmic aversion, and \\nperceived controllability. Oxford. https://academic.oup.com/jcmc/article/28/1/zmac029/6827859] \\nJussupow, E. et al. (2020) Why Are We Averse Towards Algorithms? A Comprehensive Literature Review \\non Algorithm Aversion. ECIS 2020. https://aisel.aisnet.org/ecis2020_rp/168/\")],\n",
      " 'response': AIMessage(content='Modern large language models (LLMs) present several key risks that are important to understand, especially as they become more integrated into various applications. One significant risk is the potential for these models to generate dangerous or violent content. LLMs can produce outputs that incite violence, radicalize individuals, or glorify harmful actions, which can have serious real-world implications. This capability to generate such content is more pronounced than in many other technologies, raising concerns about their use in sensitive contexts.\\n\\nAnother critical risk is the phenomenon known as \"confabulation,\" where LLMs produce false or misleading information that appears plausible. This can lead to serious consequences, particularly in fields like medicine, where incorrect diagnoses or treatment recommendations could arise from trusting an LLM\\'s output. Additionally, LLMs may provide fabricated citations or logical reasoning to support their incorrect answers, which can mislead users into believing the information is accurate. This issue is compounded by the fact that LLMs can sometimes present themselves as human-like, potentially deceiving users about the nature of their interactions.\\n\\nMoreover, LLMs can facilitate the analysis or synthesis of sensitive information, such as knowledge related to biological or chemical threats, which could be misused by individuals lacking formal expertise. While the actual physical creation of harmful agents still requires significant expertise and resources, the accessibility of information through LLMs could lower barriers for malicious actors. As these models evolve, it becomes increasingly important to assess their potential for misuse and to implement safeguards that mitigate these risks. Overall, understanding and addressing these risks is crucial for ensuring the safe and responsible deployment of LLM technology.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 323, 'prompt_tokens': 1472, 'total_tokens': 1795, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-f7aa014b-48f4-4d1a-97b0-7386b1637f56-0', usage_metadata={'input_tokens': 1472, 'output_tokens': 323, 'total_tokens': 1795})}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "response = await rag_chain.ainvoke({\"input\":\"What are some key risks associated with modern LLMs?\"})\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Evaluate baseline RAG system**\n",
    "\n",
    "This assumes that gen_synthetic_data.ipynb has already been run to generate some test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read test questions\n",
      "generating responses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|██████████| 30/30 [01:39<00:00,  3.33s/it]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 30/30 [00:00<00:00, 3530.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from vars import N_EVAL_QUESTIONS\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "#  Load the dataset and run the RAG pipeline\n",
    "response_dataset = await utils.gen_rag_responses(rag_chain)\n",
    "response_dataset.save_to_disk(f\"baseline_response_dataset_{N_EVAL_QUESTIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ragas to evaluate\n",
    "from datasets import load_from_disk\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from vars import N_EVAL_QUESTIONS, EVALUATION_MODEL\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.run_config import RunConfig\n",
    "\n",
    "# uncomment this line to load the responses from disk\n",
    "#response_dataset = load_from_disk(f\"baseline_response_dataset_{N_EVAL_QUESTIONS}\") \n",
    "\n",
    "results = evaluate(response_dataset, \n",
    "                   RAGAS_METRICS, \n",
    "                   #run_config=RunConfig(max_workers=2), # uncomment if we need to slow it down to avoid rate limit errors\n",
    "                   llm=ChatOpenAI(model_name=EVALUATION_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.8833, 'answer_relevancy': 0.9104, 'context_precision': 0.7444, 'context_recall': 0.9389}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check out the results, save them to disk\n",
    "print(results)\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv(\"baseline_ragas_evaluation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Evaluate the same system, but with the fine-tuned embedding model we uploaded to huggingface**\n",
    "This assumes that we have successfully run the notebook in the fine_tuning_arctic directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Angela/Desktop/ai_makerspace/code/ai-risk-bot/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/qk/rnr0wf6j3xj452rpryymzb200000gs/T/ipykernel_2029/2751114291.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  hf_embeddings = HuggingFaceEmbeddings(model_name=f\"{HF_USERNAME}/{FT_MODEL_NAME}\")\n",
      "Some weights of BertModel were not initialized from the model checkpoint at achapman/finetuned_arctic_ai_risk and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from vars import HF_USERNAME, FT_MODEL_NAME\n",
    "\n",
    "# Load the finetuned embeddings\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=f\"{HF_USERNAME}/{FT_MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created qdrant client\n",
      "populated vector db\n",
      "created chain\n"
     ]
    }
   ],
   "source": [
    "from vars import BASELINE_CHAT_MODEL\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "hf_rag_chain = await utils.vanilla_rag_chain_hf_embeddings(texts=chunks, \n",
    "                                                            openai_key=openai.api_key, \n",
    "                                                            embeddings = hf_embeddings,\n",
    "                                                            chat_model=BASELINE_CHAT_MODEL,\n",
    "                                                            collection_name = \"hf_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': [Document(metadata={'_id': '281e18d90ed04674a1a011582a2d12e6', '_collection_name': 'hf_collection'}, page_content='with greater ease and scale than other technologies. LLMs have been reported to generate dangerous or \\nviolent recommendations, and some models have generated actionable instructions for dangerous or \\n \\n \\n9 Confabulations of falsehoods are most commonly a problem for text-based outputs; for audio, image, or video \\ncontent, creative generation of non-factual content can be a desired behavior.  \\n10 For example, legal confabulations have been shown to be pervasive in current state-of-the-art LLMs. See also, \\ne.g.,'),\n",
      "             Document(metadata={'_id': '4f867f2288a8472eaffa3a9ba6db846a', '_collection_name': 'hf_collection'}, page_content='(as well as in tables in Appendix B) to relevant Trustworthy AI Characteristics identiﬁed in the AI RMF.  \\n \\n \\n5 These risks can be further categorized by organizations depending on their unique approaches to risk deﬁnition \\nand management. One possible way to further categorize these risks, derived in part from the UK’s International \\nScientiﬁc Report on the Safety of Advanced AI, could be: 1) Technical / Model risks (or risk from malfunction): \\nConfabulation; Dangerous or Violent Recommendations; Data Privacy; Value Chain and Component Integration; \\nHarmful Bias, and Homogenization; 2) Misuse by humans (or malicious use): CBRN Information or Capabilities; \\nData Privacy; Human-AI Conﬁguration; Obscene, Degrading, and/or Abusive Content; Information Integrity; \\nInformation Security; 3) Ecosystem / societal risks (or systemic risks): Data Privacy; Environmental; Intellectual \\nProperty. We also note that some risks are cross-cutting between these categories.'),\n",
      "             Document(metadata={'_id': '2c9c7a2990b14f378c649ffb84110784', '_collection_name': 'hf_collection'}, page_content='information reports could cause doctors to make incorrect diagnoses and/or recommend the wrong \\ntreatments. Risks of confabulated content may be especially important to monitor when integrating GAI \\ninto applications involving consequential decision making. \\nGAI outputs may also include confabulated logic or citations that purport to justify or explain the \\nsystem’s answer, which may further mislead humans into inappropriately trusting the system’s output. \\nFor instance, LLMs sometimes provide logical steps for how they arrived at an answer even when the \\nanswer itself is incorrect. Similarly, an LLM could falsely assert that it is human or has human traits, \\npotentially deceiving humans into believing they are speaking with another human. \\nThe extent to which humans can be deceived by LLMs, the mechanisms by which this may occur, and the \\npotential risks from adversarial prompting of such behavior are emerging areas of study. Given the wide \\nrange of downstream impacts of GAI, it is diﬃcult to estimate the downstream scale and impact of \\nconfabulations. \\nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Valid and Reliable, Explainable \\nand Interpretable \\n2.3. Dangerous, Violent, or Hateful Content \\nGAI systems can produce content that is inciting, radicalizing, or threatening, or that gloriﬁes violence, \\nwith greater ease and scale than other technologies. LLMs have been reported to generate dangerous or'),\n",
      "             Document(metadata={'_id': '0c68e98e74f542d5af0af325e0d9bffb', '_collection_name': 'hf_collection'}, page_content='https://www.theverge.com/2024/1/25/24050334/x-twitter-taylor-swift-ai-fake-images-trending \\nWei, J. et al. (2024) Long Form Factuality in Large Language Models. arXiv. \\nhttps://arxiv.org/pdf/2403.18802 \\nWeidinger, L. et al. (2021) Ethical and social risks of harm from Language Models. arXiv. \\nhttps://arxiv.org/pdf/2112.04359 \\nWeidinger, L. et al. (2023) Sociotechnical Safety Evaluation of Generative AI Systems. arXiv. \\nhttps://arxiv.org/pdf/2310.11986 \\nWeidinger, L. et al. (2022) Taxonomy of Risks posed by Language Models. FAccT ’22. \\nhttps://dl.acm.org/doi/pdf/10.1145/3531146.3533088 \\nWest, D. (2023) AI poses disproportionate risks to women. Brookings. \\nhttps://www.brookings.edu/articles/ai-poses-disproportionate-risks-to-women/ \\nWu, K. et al. (2024) How well do LLMs cite relevant medical references? An evaluation framework and \\nanalyses. arXiv. https://arxiv.org/pdf/2402.02008 \\nYin, L. et al. (2024) OpenAI’s GPT Is A Recruiter’s Dream Tool. Tests Show There’s Racial Bias. Bloomberg. \\nhttps://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/ \\nYu, Z. et al. (March 2024) Don’t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large \\nLanguage Models. arXiv. https://arxiv.org/html/2403.17336v1 \\nZaugg, I. et al. (2022) Digitally-disadvantaged languages. Policy Review. \\nhttps://policyreview.info/pdf/policyreview-2022-2-1654.pdf')],\n",
      " 'response': AIMessage(content=\"Modern large language models (LLMs) come with several key risks that can have significant implications for users and society. One major concern is the generation of dangerous or violent recommendations. LLMs can produce content that incites violence or promotes harmful behaviors, which can be particularly alarming given their ability to reach a wide audience quickly. Additionally, these models can sometimes generate confabulated information—essentially, falsehoods presented as facts. This is especially problematic in high-stakes areas like healthcare, where incorrect information could lead to misdiagnoses or inappropriate treatments. \\n\\nAnother risk involves the potential for harmful biases embedded within the models. These biases can manifest in various ways, such as perpetuating stereotypes or unfairly disadvantaging certain groups. This is compounded by the fact that LLMs can also produce content that is obscene, degrading, or abusive, which raises concerns about their misuse by individuals with malicious intent. Furthermore, the integration of LLMs into applications that require trust and accuracy can lead to a false sense of security, as users may be misled into believing the outputs are reliable due to the model's ability to provide seemingly logical explanations, even when those explanations are incorrect.\\n\\nLastly, there are broader societal risks associated with LLMs, including issues related to data privacy and the potential for systemic harm. As these models are used more widely, understanding and mitigating these risks becomes crucial to ensure that the benefits of AI technologies do not come at the expense of safety and ethical standards. Overall, while LLMs offer exciting possibilities, it is essential to approach their deployment with caution and a strong focus on risk management.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 326, 'prompt_tokens': 1437, 'total_tokens': 1763, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-9c9b828c-bf91-4fe1-bdc8-bf602f4cdade-0', usage_metadata={'input_tokens': 1437, 'output_tokens': 326, 'total_tokens': 1763})}\n"
     ]
    }
   ],
   "source": [
    "# Test the chain\n",
    "from pprint import pprint\n",
    "response = await hf_rag_chain.ainvoke({\"input\":\"What are some key risks associated with modern LLMs?\"})\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read test questions\n",
      "generating responses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|██████████| 30/30 [01:58<00:00,  3.95s/it]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 30/30 [00:00<00:00, 5528.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and run the RAG pipeline\n",
    "hf_response_dataset = await utils.gen_rag_responses(hf_rag_chain)\n",
    "hf_response_dataset.save_to_disk(f\"finetuned_response_dataset_{N_EVAL_QUESTIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 120/120 [01:40<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.9285, 'answer_relevancy': 0.8746, 'context_precision': 0.7009, 'context_recall': 0.9833}\n"
     ]
    }
   ],
   "source": [
    "from vars import EVALUATION_MODEL\n",
    "import pandas as pd\n",
    "\n",
    "# uncomment this line to load the responses from disk\n",
    "#response_dataset = load_from_disk(f\"finetuned_response_dataset_{N_EVAL_QUESTIONS}\") \n",
    "\n",
    "hf_results = evaluate(hf_response_dataset, \n",
    "                   RAGAS_METRICS, \n",
    "                   #run_config=RunConfig(max_workers=2), # uncomment if we need to slow it down to avoid rate limit errors\n",
    "                   llm=ChatOpenAI(model_name=EVALUATION_MODEL))\n",
    "\n",
    "# Check out the results, save them to disk\n",
    "print(hf_results)\n",
    "\n",
    "results_df = pd.DataFrame([hf_results])\n",
    "results_df.to_csv(\"finetuned_ragas_evaluation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Do we do better if we just throw more money at this by upgrading to OpenAI's \"best\" models?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created qdrant client\n",
      "created embeddings\n",
      "populated vector db\n",
      "created chain\n",
      "{'context': [Document(metadata={'_id': 'dd33382b341344dda0e2640411393a7f', '_collection_name': 'default'}, page_content='with greater ease and scale than other technologies. LLMs have been reported to generate dangerous or \\nviolent recommendations, and some models have generated actionable instructions for dangerous or \\n \\n \\n9 Confabulations of falsehoods are most commonly a problem for text-based outputs; for audio, image, or video \\ncontent, creative generation of non-factual content can be a desired behavior.  \\n10 For example, legal confabulations have been shown to be pervasive in current state-of-the-art LLMs. See also, \\ne.g.,'),\n",
      "             Document(metadata={'_id': '447169b053f040cd8ef3e19638f9d421', '_collection_name': 'default'}, page_content='information reports could cause doctors to make incorrect diagnoses and/or recommend the wrong \\ntreatments. Risks of confabulated content may be especially important to monitor when integrating GAI \\ninto applications involving consequential decision making. \\nGAI outputs may also include confabulated logic or citations that purport to justify or explain the \\nsystem’s answer, which may further mislead humans into inappropriately trusting the system’s output. \\nFor instance, LLMs sometimes provide logical steps for how they arrived at an answer even when the \\nanswer itself is incorrect. Similarly, an LLM could falsely assert that it is human or has human traits, \\npotentially deceiving humans into believing they are speaking with another human. \\nThe extent to which humans can be deceived by LLMs, the mechanisms by which this may occur, and the \\npotential risks from adversarial prompting of such behavior are emerging areas of study. Given the wide \\nrange of downstream impacts of GAI, it is diﬃcult to estimate the downstream scale and impact of \\nconfabulations. \\nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Valid and Reliable, Explainable \\nand Interpretable \\n2.3. Dangerous, Violent, or Hateful Content \\nGAI systems can produce content that is inciting, radicalizing, or threatening, or that gloriﬁes violence, \\nwith greater ease and scale than other technologies. LLMs have been reported to generate dangerous or'),\n",
      "             Document(metadata={'_id': '432b589e425e4a95abc8956714569ec0', '_collection_name': 'default'}, page_content='likelihood) of occurring and the magnitude or degree of the consequences of the corresponding event. \\nSome risks can be assessed as likely to materialize in a given context, particularly those that have been \\nempirically demonstrated in similar contexts. Other risks may be unlikely to materialize in a given \\ncontext, or may be more speculative and therefore uncertain. \\nAI risks can diﬀer from or intensify traditional software risks. Likewise, GAI can exacerbate existing AI \\nrisks, and creates unique risks. GAI risks can vary along many dimensions: \\n• \\nStage of the AI lifecycle: Risks can arise during design, development, deployment, operation, \\nand/or decommissioning. \\n• \\nScope: Risks may exist at individual model or system levels, at the application or implementation \\nlevels (i.e., for a speciﬁc use case), or at the ecosystem level – that is, beyond a single system or \\norganizational context. Examples of the latter include the expansion of “algorithmic \\nmonocultures,3” resulting from repeated use of the same model, or impacts on access to \\nopportunity, labor markets, and the creative economies.4 \\n• \\nSource of risk: Risks may emerge from factors related to the design, training, or operation of the \\nGAI model itself, stemming in some cases from GAI model or system inputs, and in other cases, \\nfrom GAI system outputs. Many GAI risks, however, originate from human behavior, including'),\n",
      "             Document(metadata={'_id': '0ef4a13141874bbd8f9e38a52757b2c1', '_collection_name': 'default'}, page_content='development, production, or use of CBRN weapons or other dangerous materials or agents. While \\nrelevant biological and chemical threat knowledge and information is often publicly accessible, LLMs \\ncould facilitate its analysis or synthesis, particularly by individuals without formal scientiﬁc training or \\nexpertise.  \\nRecent research on this topic found that LLM outputs regarding biological threat creation and attack \\nplanning provided minimal assistance beyond traditional search engine queries, suggesting that state-of-\\nthe-art LLMs at the time these studies were conducted do not substantially increase the operational \\nlikelihood of such an attack. The physical synthesis development, production, and use of chemical or \\nbiological agents will continue to require both applicable expertise and supporting materials and \\ninfrastructure. The impact of GAI on chemical or biological agent misuse will depend on what the key \\nbarriers for malicious actors are (e.g., whether information access is one such barrier), and how well GAI \\ncan help actors address those barriers.  \\nFurthermore, chemical and biological design tools (BDTs) – highly specialized AI systems trained on \\nscientiﬁc data that aid in chemical and biological design – may augment design capabilities in chemistry \\nand biology beyond what text-based LLMs are able to provide. As these models become more \\neﬃcacious, including for beneﬁcial uses, it will be important to assess their potential to be used for')],\n",
      " 'response': AIMessage(content='Modern Large Language Models (LLMs) come with several key risks that are important to understand. One significant risk is the generation of dangerous or violent content. LLMs can produce text that incites, radicalizes, or threatens individuals, and they can do this at a scale and speed that surpasses other technologies. This can lead to the spread of harmful ideologies or misinformation more rapidly and broadly.\\n\\nAnother major risk is the issue of \"confabulation,\" where the model generates false or misleading information. This is particularly concerning in contexts where accurate information is critical, such as in medical or legal advice. For example, an LLM might provide incorrect medical information that could lead to wrong diagnoses or treatments, or it might generate false legal citations that could mislead legal professionals. The model might even create logical steps or explanations that seem plausible but are entirely fabricated, which can further deceive users into trusting incorrect outputs.\\n\\nAdditionally, LLMs can sometimes falsely assert that they are human or possess human traits, which can mislead people into believing they are interacting with another person. This deception can have various negative consequences, including eroding trust in digital communications and potentially manipulating individuals\\' actions or beliefs.\\n\\nThere are also risks related to the misuse of LLMs for creating or disseminating information about dangerous materials, such as chemical or biological agents. While current research suggests that LLMs do not significantly increase the operational likelihood of such attacks compared to traditional search engines, the potential for misuse remains a concern, especially as these models become more advanced.\\n\\nOverall, the risks associated with LLMs are multifaceted and can vary depending on the context in which they are used. It is crucial to monitor and mitigate these risks to ensure the safe and responsible deployment of these powerful technologies.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 354, 'prompt_tokens': 1352, 'total_tokens': 1706, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3537616b13', 'finish_reason': 'stop', 'logprobs': None}, id='run-81f75975-2de6-4676-bae8-7f00f6e540de-0', usage_metadata={'input_tokens': 1352, 'output_tokens': 354, 'total_tokens': 1706})}\n"
     ]
    }
   ],
   "source": [
    "# Create and test the chain\n",
    "from vars import TE3_LARGE, TE3_VECTOR_LENGTH, GPT_4O\n",
    "importlib.reload(utils)\n",
    "\n",
    "expensive_rag_chain = await utils.vanilla_openai_rag_chain(texts=chunks, \n",
    "                                            openai_key=openai.api_key, \n",
    "                                            embedding_model=TE3_LARGE,\n",
    "                                            chat_model=GPT_4O,\n",
    "                                            vector_size=TE3_VECTOR_LENGTH)\n",
    "\n",
    "from pprint import pprint\n",
    "response = await expensive_rag_chain.ainvoke({\"input\":\"What are some key risks associated with modern LLMs?\"})\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read test questions\n",
      "generating responses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|██████████| 30/30 [02:34<00:00,  5.17s/it]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 30/30 [00:00<00:00, 4716.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from vars import N_EVAL_QUESTIONS\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "#  Load the dataset and run the RAG pipeline on our test data\n",
    "expensive_response_dataset = await utils.gen_rag_responses(expensive_rag_chain)\n",
    "expensive_response_dataset.save_to_disk(f\"expensive_response_dataset_{N_EVAL_QUESTIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 120/120 [01:45<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.9190, 'answer_relevancy': 0.9504, 'context_precision': 0.7556, 'context_recall': 1.0000}\n"
     ]
    }
   ],
   "source": [
    "# Use ragas to evaluate\n",
    "from datasets import load_from_disk\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from vars import N_EVAL_QUESTIONS, EVALUATION_MODEL\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.run_config import RunConfig\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# uncomment this line to load the responses from disk\n",
    "#response_dataset = load_from_disk(f\"expensive_response_dataset_{N_EVAL_QUESTIONS}\") \n",
    "\n",
    "results = evaluate(expensive_response_dataset, \n",
    "                   RAGAS_METRICS, \n",
    "                   #run_config=RunConfig(max_workers=2), # uncomment if we need to slow it down to avoid rate limit errors\n",
    "                   llm=ChatOpenAI(model_name=EVALUATION_MODEL))\n",
    "\n",
    "\n",
    "# Check out the results, save them to disk\n",
    "print(results)\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv(\"expensive_ragas_evaluation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Do something fancier and see if it helps**\n",
    "For our final effort, we are going to test out:\n",
    "- Cleaning up the docs by removing extraneous content\n",
    "- Using parent-doc retrieval (still somewhat naive, by page). To make this better, use unstructured to load the docs and chunk them by section.\n",
    "- Using our fine-tuned embedding model along with GPT-4o for the final pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dict with some info about our docs\n",
    "from vars import GPT_4O, PDF_DICT\n",
    "import importlib, utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "# Construct the chain. This assumes our hf_embeddings that we loaded earlier are still in the environment\n",
    "fancy_rag_chain = await utils.fancy_rag_chain(PDF_DICT,\n",
    "                                              openai_key=openai.api_key, \n",
    "                                              embeddings = hf_embeddings,\n",
    "                                              chat_model=GPT_4O,\n",
    "                                              collection_name = \"hf_collection_fancy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': [Document(metadata={'source': 'https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf', 'file_path': 'https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf', 'page': 9, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n6 \\n2.2. Confabulation \\n“Confabulation” refers to a phenomenon in which GAI systems generate and conﬁdently present \\nerroneous or false content in response to prompts. Confabulations also include generated outputs that \\ndiverge from the prompts or other input or that contradict previously generated statements in the same \\ncontext. These phenomena are colloquially also referred to as “hallucinations” or “fabrications.” \\nConfabulations can occur across GAI outputs and contexts.9,10 Confabulations are a natural result of the \\nway generative models are designed: they generate outputs that approximate the statistical distribution \\nof their training data; for example, LLMs predict the next token or word in a sentence or phrase. While \\nsuch statistical prediction can produce factually accurate and consistent outputs, it can also produce \\noutputs that are factually inaccurate or internally inconsistent. This dynamic is particularly relevant when \\nit comes to open-ended prompts for long-form responses and in domains which require highly \\ncontextual and/or domain expertise.  \\nRisks from confabulations may arise when users believe false content – often due to the conﬁdent nature \\nof the response – leading users to act upon or promote the false information. This poses a challenge for \\nmany real-world applications, such as in healthcare, where a confabulated summary of patient \\ninformation reports could cause doctors to make incorrect diagnoses and/or recommend the wrong \\ntreatments. Risks of confabulated content may be especially important to monitor when integrating GAI \\ninto applications involving consequential decision making. \\nGAI outputs may also include confabulated logic or citations that purport to justify or explain the \\nsystem’s answer, which may further mislead humans into inappropriately trusting the system’s output. \\nFor instance, LLMs sometimes provide logical steps for how they arrived at an answer even when the \\nanswer itself is incorrect. Similarly, an LLM could falsely assert that it is human or has human traits, \\npotentially deceiving humans into believing they are speaking with another human. \\nThe extent to which humans can be deceived by LLMs, the mechanisms by which this may occur, and the \\npotential risks from adversarial prompting of such behavior are emerging areas of study. Given the wide \\nrange of downstream impacts of GAI, it is diﬃcult to estimate the downstream scale and impact of \\nconfabulations. \\nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Valid and Reliable, Explainable \\nand Interpretable \\n2.3. Dangerous, Violent, or Hateful Content \\nGAI systems can produce content that is inciting, radicalizing, or threatening, or that gloriﬁes violence, \\nwith greater ease and scale than other technologies. LLMs have been reported to generate dangerous or \\nviolent recommendations, and some models have generated actionable instructions for dangerous or \\n \\n \\n9 Confabulations of falsehoods are most commonly a problem for text-based outputs; for audio, image, or video \\ncontent, creative generation of non-factual content can be a desired behavior.  \\n10 For example, legal confabulations have been shown to be pervasive in current state-of-the-art LLMs. See also, \\ne.g.,  \\n'),\n",
      "             Document(metadata={'source': 'https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf', 'file_path': 'https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf', 'page': 62, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n59 \\nTirrell, L. (2017) Toxic Speech: Toward an Epidemiology of Discursive Harm. Philosophical Topics, 45(2), \\n139-162. https://www.jstor.org/stable/26529441  \\nTufekci, Z. (2015) Algorithmic Harms Beyond Facebook and Google: Emergent Challenges of \\nComputational Agency. Colorado Technology Law Journal. https://ctlj.colorado.edu/wp-\\ncontent/uploads/2015/08/Tufekci-ﬁnal.pdf \\nTurri, V. et al. (2023) Why We Need to Know More: Exploring the State of AI Incident Documentation \\nPractices. AAAI/ACM Conference on AI, Ethics, and Society. \\nhttps://dl.acm.org/doi/fullHtml/10.1145/3600211.3604700 \\nUrbina, F. et al. (2022) Dual use of artiﬁcial-intelligence-powered drug discovery. Nature Machine \\nIntelligence. https://www.nature.com/articles/s42256-022-00465-9 \\nWang, X. et al. (2023) Energy and Carbon Considerations of Fine-Tuning BERT. ACL Anthology. \\nhttps://aclanthology.org/2023.ﬁndings-emnlp.607.pdf \\nWang, Y. et al. (2023) Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. arXiv. \\nhttps://arxiv.org/pdf/2308.13387 \\nWardle, C. et al. (2017) Information Disorder: Toward an interdisciplinary framework for research and \\npolicy making. Council of Europe. https://rm.coe.int/information-disorder-toward-an-interdisciplinary-\\nframework-for-researc/168076277c \\nWeatherbed, J. (2024) Trolls have ﬂooded X with graphic Taylor Swift AI fakes. The Verge. \\nhttps://www.theverge.com/2024/1/25/24050334/x-twitter-taylor-swift-ai-fake-images-trending \\nWei, J. et al. (2024) Long Form Factuality in Large Language Models. arXiv. \\nhttps://arxiv.org/pdf/2403.18802 \\nWeidinger, L. et al. (2021) Ethical and social risks of harm from Language Models. arXiv. \\nhttps://arxiv.org/pdf/2112.04359 \\nWeidinger, L. et al. (2023) Sociotechnical Safety Evaluation of Generative AI Systems. arXiv. \\nhttps://arxiv.org/pdf/2310.11986 \\nWeidinger, L. et al. (2022) Taxonomy of Risks posed by Language Models. FAccT ’22. \\nhttps://dl.acm.org/doi/pdf/10.1145/3531146.3533088 \\nWest, D. (2023) AI poses disproportionate risks to women. Brookings. \\nhttps://www.brookings.edu/articles/ai-poses-disproportionate-risks-to-women/ \\nWu, K. et al. (2024) How well do LLMs cite relevant medical references? An evaluation framework and \\nanalyses. arXiv. https://arxiv.org/pdf/2402.02008 \\nYin, L. et al. (2024) OpenAI’s GPT Is A Recruiter’s Dream Tool. Tests Show There’s Racial Bias. Bloomberg. \\nhttps://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination/ \\nYu, Z. et al. (March 2024) Don’t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large \\nLanguage Models. arXiv. https://arxiv.org/html/2403.17336v1 \\nZaugg, I. et al. (2022) Digitally-disadvantaged languages. Policy Review. \\nhttps://policyreview.info/pdf/policyreview-2022-2-1654.pdf \\n'),\n",
      "             Document(metadata={'source': 'https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf', 'file_path': 'https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf', 'page': 8, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n5 \\noperations, or other cyberattacks; increased attack surface for targeted cyberattacks, which may \\ncompromise a system’s availability or the conﬁdentiality or integrity of training data, code, or \\nmodel weights.  \\n10. Intellectual Property: Eased production or replication of alleged copyrighted, trademarked, or \\nlicensed content without authorization (possibly in situations which do not fall under fair use); \\neased exposure of trade secrets; or plagiarism or illegal replication.  \\n11. Obscene, Degrading, and/or Abusive Content: Eased production of and access to obscene, \\ndegrading, and/or abusive imagery which can cause harm, including synthetic child sexual abuse \\nmaterial (CSAM), and nonconsensual intimate images (NCII) of adults. \\n12. Value Chain and Component Integration: Non-transparent or untraceable integration of \\nupstream third-party components, including data that has been improperly obtained or not \\nprocessed and cleaned due to increased automation from GAI; improper supplier vetting across \\nthe AI lifecycle; or other issues that diminish transparency or accountability for downstream \\nusers. \\n2.1. CBRN Information or Capabilities \\nIn the future, GAI may enable malicious actors to more easily access CBRN weapons and/or relevant \\nknowledge, information, materials, tools, or technologies that could be misused to assist in the design, \\ndevelopment, production, or use of CBRN weapons or other dangerous materials or agents. While \\nrelevant biological and chemical threat knowledge and information is often publicly accessible, LLMs \\ncould facilitate its analysis or synthesis, particularly by individuals without formal scientiﬁc training or \\nexpertise.  \\nRecent research on this topic found that LLM outputs regarding biological threat creation and attack \\nplanning provided minimal assistance beyond traditional search engine queries, suggesting that state-of-\\nthe-art LLMs at the time these studies were conducted do not substantially increase the operational \\nlikelihood of such an attack. The physical synthesis development, production, and use of chemical or \\nbiological agents will continue to require both applicable expertise and supporting materials and \\ninfrastructure. The impact of GAI on chemical or biological agent misuse will depend on what the key \\nbarriers for malicious actors are (e.g., whether information access is one such barrier), and how well GAI \\ncan help actors address those barriers.  \\nFurthermore, chemical and biological design tools (BDTs) – highly specialized AI systems trained on \\nscientiﬁc data that aid in chemical and biological design – may augment design capabilities in chemistry \\nand biology beyond what text-based LLMs are able to provide. As these models become more \\neﬃcacious, including for beneﬁcial uses, it will be important to assess their potential to be used for \\nharm, such as the ideation and design of novel harmful chemical or biological agents.  \\nWhile some of these described capabilities lie beyond the reach of existing GAI tools, ongoing \\nassessments of this risk would be enhanced by monitoring both the ability of AI tools to facilitate CBRN \\nweapons planning and GAI systems’ connection or access to relevant data and tools. \\nTrustworthy AI Characteristic: Safe, Explainable and Interpretable \\n'),\n",
      "             Document(metadata={'source': 'https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf', 'file_path': 'https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf', 'page': 4, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': ''}, page_content=' \\n1 \\n1. \\nIntroduction \\nThis document is a cross-sectoral proﬁle of and companion resource for the AI Risk Management \\nFramework (AI RMF 1.0) for Generative AI,1 pursuant to President Biden’s Executive Order (EO) 14110 on \\nSafe, Secure, and Trustworthy Artiﬁcial Intelligence.2 The AI RMF was released in January 2023, and is \\nintended for voluntary use and to improve the ability of organizations to incorporate trustworthiness \\nconsiderations into the design, development, use, and evaluation of AI products, services, and systems.  \\nA proﬁle is an implementation of the AI RMF functions, categories, and subcategories for a speciﬁc \\nsetting, application, or technology – in this case, Generative AI (GAI) – based on the requirements, risk \\ntolerance, and resources of the Framework user. AI RMF proﬁles assist organizations in deciding how to \\nbest manage AI risks in a manner that is well-aligned with their goals, considers legal/regulatory \\nrequirements and best practices, and reﬂects risk management priorities. Consistent with other AI RMF \\nproﬁles, this proﬁle oﬀers insights into how risk can be managed across various stages of the AI lifecycle \\nand for GAI as a technology.  \\nAs GAI covers risks of models or applications that can be used across use cases or sectors, this document \\nis an AI RMF cross-sectoral proﬁle. Cross-sectoral proﬁles can be used to govern, map, measure, and \\nmanage risks associated with activities or business processes common across sectors, such as the use of \\nlarge language models (LLMs), cloud-based services, or acquisition. \\nThis document deﬁnes risks that are novel to or exacerbated by the use of GAI. After introducing and \\ndescribing these risks, the document provides a set of suggested actions to help organizations govern, \\nmap, measure, and manage these risks. \\n \\n \\n1 EO 14110 deﬁnes Generative AI as “the class of AI models that emulate the structure and characteristics of input \\ndata in order to generate derived synthetic content. This can include images, videos, audio, text, and other digital \\ncontent.” While not all GAI is derived from foundation models, for purposes of this document, GAI generally refers \\nto generative foundation models. The foundation model subcategory of “dual-use foundation models” is deﬁned by \\nEO 14110 as “an AI model that is trained on broad data; generally uses self-supervision; contains at least tens of \\nbillions of parameters; is applicable across a wide range of contexts.”  \\n2 This proﬁle was developed per Section 4.1(a)(i)(A) of EO 14110, which directs the Secretary of Commerce, acting \\nthrough the Director of the National Institute of Standards and Technology (NIST), to develop a companion \\nresource to the AI RMF, NIST AI 100–1, for generative AI. \\n')],\n",
      " 'response': AIMessage(content='Modern large language models (LLMs) present several key risks that are important to understand, especially as they become more integrated into various applications. One significant risk is the phenomenon known as \"confabulation,\" where these models generate content that is confidently presented but may be factually incorrect or inconsistent. This can lead users to mistakenly trust and act on false information, which is particularly concerning in critical fields like healthcare, where incorrect outputs could result in misdiagnoses or inappropriate treatments. The models can also produce misleading logical reasoning or citations, further complicating the user\\'s ability to discern the accuracy of the information provided.\\n\\nAnother major risk is the potential for LLMs to generate dangerous or harmful content. This includes outputs that could incite violence, promote radicalization, or provide instructions for harmful actions. The ease with which LLMs can produce such content raises concerns about their use in contexts where safety is paramount. Additionally, there are risks related to the generation of obscene or abusive content, including the potential for creating non-consensual intimate images or other harmful materials.\\n\\nMoreover, LLMs can inadvertently facilitate the spread of misinformation or harmful ideologies, as they may not always be able to distinguish between credible and non-credible sources of information. This can lead to a broader societal impact, where false narratives gain traction due to the persuasive nature of the outputs generated by these models. As these technologies continue to evolve, it is crucial to monitor and mitigate these risks to ensure that their deployment is safe and responsible.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 301, 'prompt_tokens': 3905, 'total_tokens': 4206, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-21e4379a-5eec-4d1e-8a64-76fdf66dc814-0', usage_metadata={'input_tokens': 3905, 'output_tokens': 301, 'total_tokens': 4206})}\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "from pprint import pprint\n",
    "response = await fancy_rag_chain.ainvoke({\"input\":\"What are some key risks associated with modern LLMs?\"})\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read test questions\n",
      "generating responses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|██████████| 30/30 [02:20<00:00,  4.70s/it]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 30/30 [00:00<00:00, 6189.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from vars import N_EVAL_QUESTIONS\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "#  Load the dataset and run the RAG pipeline on our test data\n",
    "fancy_response_dataset = await utils.gen_rag_responses(fancy_rag_chain)\n",
    "fancy_response_dataset.save_to_disk(f\"fancy_response_dataset_{N_EVAL_QUESTIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 120/120 [01:18<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.9036, 'answer_relevancy': 0.9092, 'context_precision': 0.7417, 'context_recall': 0.9889}\n"
     ]
    }
   ],
   "source": [
    "# Use ragas to evaluate\n",
    "from datasets import load_from_disk\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from vars import N_EVAL_QUESTIONS, EVALUATION_MODEL\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.run_config import RunConfig\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# uncomment this line to load the responses from disk\n",
    "#response_dataset = load_from_disk(f\"fancy_response_dataset_{N_EVAL_QUESTIONS}\") \n",
    "\n",
    "results = evaluate(fancy_response_dataset, \n",
    "                   RAGAS_METRICS, \n",
    "                   #run_config=RunConfig(max_workers=2), # uncomment if we need to slow it down to avoid rate limit errors\n",
    "                   llm=ChatOpenAI(model_name=EVALUATION_MODEL))\n",
    "\n",
    "\n",
    "# Check out the results, save them to disk\n",
    "print(results)\n",
    "results_df = pd.DataFrame([results])\n",
    "results_df.to_csv(\"fancy_ragas_evaluation_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
