{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0: Imports, constants, and API Keys!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain==0.2.16 langchain_core==0.2.38 langchain_community==0.2.16 pymupdf openai \n",
    "!pip install -q langchain_openai==0.1.23 langchain-qdrant qdrant_client asyncio ragas==0.1.14 pandas\n",
    "!pip install -q langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG constants\n",
    "CHUNK_SIZE = 1500\n",
    "OVERLAP = 150\n",
    "BASELINE_EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "BASELINE_CHAT_MODEL = \"gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "# RAGAS constants\n",
    "RAGAS_CHUNK_SIZE = 750\n",
    "RAGAS_OVERLAP = 75\n",
    "GENERATOR_LLM = \"gpt-4o-mini-2024-07-18\"\n",
    "CRITIC_LLM = \"gpt-4o-2024-08-06\"\n",
    "N_EVAL_QUESTIONS = 30 # IRL, we'd want more, and maybe a test and validation set. But set it low to accommodate low rate limits.\n",
    "TEST_DATASET_FILE = f\"test_dataset_{N_EVAL_QUESTIONS}.csv\"\n",
    "\n",
    "# Dataset\n",
    "PDFS = [\n",
    "    \"https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf\",\n",
    "    \"https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "# collect OpenAI key\n",
    "openai.api_key = getpass(\"OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Download and chunk the data**\n",
    "\n",
    "We are going to use the following docs as our knowledge base:\n",
    "1. Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People (PDF)\n",
    "2. National Institute of Standards and Technology (NIST) Artificial Intelligent Risk Management Framework \n",
    "\n",
    "Let's start with a simple fixed chunking strategy as a baseline, and later evaluate parent-doc retrieval if we have time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf...\n",
      "Chunking...\n",
      "Loading https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf...\n",
      "Chunking...\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import vanilla_rag\n",
    "\n",
    "importlib.reload(vanilla_rag)\n",
    "for pdf in PDFS:\n",
    "    chunks = await vanilla_rag.load_and_chunk_pdf(pdf,CHUNK_SIZE,OVERLAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Basic RAG Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created qdrant client\n",
      "created embeddings\n",
      "populated vector db\n",
      "created chain\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(vanilla_rag)\n",
    "rag_chain = await vanilla_rag.vanilla_openai_rag_chain(texts=chunks, \n",
    "                                            openai_key=openai.api_key, \n",
    "                                            embedding_model=BASELINE_EMBEDDING_MODEL,\n",
    "                                            chat_model=BASELINE_CHAT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': [Document(metadata={'_id': '0c56626e4e1248b8aa06650938c75b4a', '_collection_name': 'default'}, page_content='with greater ease and scale than other technologies. LLMs have been reported to generate dangerous or \\nviolent recommendations, and some models have generated actionable instructions for dangerous or \\n \\n \\n9 Confabulations of falsehoods are most commonly a problem for text-based outputs; for audio, image, or video \\ncontent, creative generation of non-factual content can be a desired behavior.  \\n10 For example, legal confabulations have been shown to be pervasive in current state-of-the-art LLMs. See also, \\ne.g.,'),\n",
      "             Document(metadata={'_id': '75e5f9bf15df4824a59305d8ed7d9e11', '_collection_name': 'default'}, page_content='development, production, or use of CBRN weapons or other dangerous materials or agents. While \\nrelevant biological and chemical threat knowledge and information is often publicly accessible, LLMs \\ncould facilitate its analysis or synthesis, particularly by individuals without formal scientiﬁc training or \\nexpertise.  \\nRecent research on this topic found that LLM outputs regarding biological threat creation and attack \\nplanning provided minimal assistance beyond traditional search engine queries, suggesting that state-of-\\nthe-art LLMs at the time these studies were conducted do not substantially increase the operational \\nlikelihood of such an attack. The physical synthesis development, production, and use of chemical or \\nbiological agents will continue to require both applicable expertise and supporting materials and \\ninfrastructure. The impact of GAI on chemical or biological agent misuse will depend on what the key \\nbarriers for malicious actors are (e.g., whether information access is one such barrier), and how well GAI \\ncan help actors address those barriers.  \\nFurthermore, chemical and biological design tools (BDTs) – highly specialized AI systems trained on \\nscientiﬁc data that aid in chemical and biological design – may augment design capabilities in chemistry \\nand biology beyond what text-based LLMs are able to provide. As these models become more \\neﬃcacious, including for beneﬁcial uses, it will be important to assess their potential to be used for'),\n",
      "             Document(metadata={'_id': 'a02c5a5926594842b44a4deff11f98fa', '_collection_name': 'default'}, page_content='information reports could cause doctors to make incorrect diagnoses and/or recommend the wrong \\ntreatments. Risks of confabulated content may be especially important to monitor when integrating GAI \\ninto applications involving consequential decision making. \\nGAI outputs may also include confabulated logic or citations that purport to justify or explain the \\nsystem’s answer, which may further mislead humans into inappropriately trusting the system’s output. \\nFor instance, LLMs sometimes provide logical steps for how they arrived at an answer even when the \\nanswer itself is incorrect. Similarly, an LLM could falsely assert that it is human or has human traits, \\npotentially deceiving humans into believing they are speaking with another human. \\nThe extent to which humans can be deceived by LLMs, the mechanisms by which this may occur, and the \\npotential risks from adversarial prompting of such behavior are emerging areas of study. Given the wide \\nrange of downstream impacts of GAI, it is diﬃcult to estimate the downstream scale and impact of \\nconfabulations. \\nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Valid and Reliable, Explainable \\nand Interpretable \\n2.3. Dangerous, Violent, or Hateful Content \\nGAI systems can produce content that is inciting, radicalizing, or threatening, or that gloriﬁes violence, \\nwith greater ease and scale than other technologies. LLMs have been reported to generate dangerous or'),\n",
      "             Document(metadata={'_id': '37556f35bb5149ec9083fefdf25826ff', '_collection_name': 'default'}, page_content=\"Models. Project Zero. https://googleprojectzero.blogspot.com/2024/06/project-naptime.html \\nGreshake, K. et al. (2023) Not what you've signed up for: Compromising Real-World LLM-Integrated \\nApplications with Indirect Prompt Injection. arXiv. https://arxiv.org/abs/2302.12173 \\nHagan, M. (2024) Good AI Legal Help, Bad AI Legal Help: Establishing quality standards for responses to \\npeople’s legal problem stories. SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4696936 \\nHaran, R. (2023) Securing LLM Systems Against Prompt Injection. NVIDIA. \\nhttps://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/ \\nInformation Technology Industry Council (2024) Authenticating AI-Generated Content. \\nhttps://www.itic.org/policy/ITI_AIContentAuthorizationPolicy_122123.pdf \\nJain, S. et al. (2023) Algorithmic Pluralism: A Structural Approach To Equal Opportunity. arXiv. \\nhttps://arxiv.org/pdf/2305.08157 \\nJi, Z. et al (2023) Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55, 12, \\nArticle 248. https://doi.org/10.1145/3571730 \\nJones-Jang, S. et al. (2022) How do people react to AI failure? Automation bias, algorithmic aversion, and \\nperceived controllability. Oxford. https://academic.oup.com/jcmc/article/28/1/zmac029/6827859] \\nJussupow, E. et al. (2020) Why Are We Averse Towards Algorithms? A Comprehensive Literature Review \\non Algorithm Aversion. ECIS 2020. https://aisel.aisnet.org/ecis2020_rp/168/\")],\n",
      " 'response': AIMessage(content='Modern large language models (LLMs) present several key risks that are important to understand, especially as they become more integrated into various applications. One significant risk is the generation of dangerous or violent content. LLMs can produce outputs that incite violence, radicalize individuals, or glorify harmful behaviors, which can have real-world consequences. This capability is concerning because it can be done with greater ease and scale than many other technologies, potentially leading to the spread of harmful ideologies or actions.\\n\\nAnother critical risk is the phenomenon of \"confabulation,\" where LLMs generate false or misleading information that appears plausible. This can be particularly dangerous in contexts like healthcare, where incorrect diagnoses or treatment recommendations could arise from LLM-generated content. The models may also provide fabricated citations or logical reasoning that mislead users into trusting incorrect outputs. This issue is compounded by the fact that LLMs can sometimes present themselves as human-like, which can further deceive users into believing they are interacting with a knowledgeable entity.\\n\\nAdditionally, LLMs can facilitate the analysis and synthesis of information related to dangerous materials, such as chemical and biological agents, even for individuals without formal training. While the actual production of such agents still requires expertise and resources, the accessibility of information through LLMs could lower barriers for malicious actors. As these models evolve, the potential for misuse in sensitive areas like public safety and security becomes a pressing concern.\\n\\nOverall, the risks associated with LLMs highlight the need for careful monitoring and regulation, especially in applications involving consequential decision-making. Understanding these risks is crucial for developing strategies to mitigate them and ensure that the benefits of LLM technology do not come at an unacceptable cost to society.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 339, 'prompt_tokens': 1439, 'total_tokens': 1778, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-f43b0de8-2dab-405d-8b7a-f3cbfaf009ec-0', usage_metadata={'input_tokens': 1439, 'output_tokens': 339, 'total_tokens': 1778})}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "response = await rag_chain.ainvoke({\"input\":\"What are some key risks associated with modern LLMs?\"})\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Generate synthetic data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context, conditional\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "generator_llm = ChatOpenAI(model=GENERATOR_LLM)\n",
    "critic_llm = ChatOpenAI(model=CRITIC_LLM)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize data generator and set up distributions\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "distributions = {\n",
    "    simple: 0.5,\n",
    "    multi_context: 0.3,\n",
    "    reasoning: 0.1,\n",
    "    conditional: 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf...\n",
      "Chunking...\n",
      "Loading https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf...\n",
      "Chunking...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ce01cd34f34f1bbaf2f8481f3f6cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139f13c59dd5438cb90b9ad37692627d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 0 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI life cycle', 'Harmful Bias', 'Fact-checking techniques', 'GAI systems', 'Information Integrity']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Organizational risk tolerance', 'GAI system outputs', 'Safety and validity review', 'Information integrity', 'Security anomalies']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Sensitive information', 'Adversarial attacks', 'Data memorization', 'Privacy risks', 'GAI models']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 0 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Sensitive information', 'Adversarial attacks', 'Data memorization', 'Privacy risks', 'GAI models']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Intellectual property risks', 'GAI systems', 'Fair use doctrine', 'Copyright infringement', 'Generated content status']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI red-teaming', 'Controlled environment', 'Adverse behavior', 'Pre-deployment contexts', 'Demographically diverse teams']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 2, 'relevance': 2, 'score': 1.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Incorrect presumptions about performance', 'Human-AI configuration', 'Anthropomorphizing GAI systems', 'Algorithmic aversion', 'Information integrity']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 2, 'relevance': 2, 'score': 1.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Incorrect presumptions about performance', 'Human-AI configuration', 'Anthropomorphizing GAI systems', 'Algorithmic aversion', 'Information integrity']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Intellectual property risks', 'GAI systems', 'Fair use doctrine', 'Copyright infringement', 'Generated content status']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Intellectual property risks', 'GAI systems', 'Fair use doctrine', 'Copyright infringement', 'Generated content status']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 0 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['TEVV metrics', 'Measurement error models', 'Construct validity', 'Hateful content', 'AI Deployment and Monitoring']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 2, 'score': 2.0}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Information Security', 'AI Development', 'Operator and practitioner proficiency', 'GAI risks', 'Digital content transparency']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI red-teaming', 'Controlled environment', 'Adverse behavior', 'Pre-deployment contexts', 'Demographically diverse teams']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are the intellectual property risks associated with GAI systems?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are the potential consequences of incorrect presumptions about performance in decision-making?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are the intellectual property risks associated with GAI systems?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are the concerns associated with harmful bias in the context of AI systems?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What privacy risks are associated with data memorization in GAI models?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What risks do adversarial attacks pose in relation to sensitive information in GAI models?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are the key processes defined for operator and practitioner proficiency in AI development?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What measures should be taken to handle and recover from security anomalies in GAI system architecture?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What is the significance of identifying adverse behavior in AI models during the red-teaming process?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What is the purpose of AI red-teaming in relation to identifying adverse behavior in AI models?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What is the purpose of creating measurement error models for pre-deployment metrics in the context of TEVV processes?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What is the significance of the fair use doctrine in relation to intellectual property risks from GAI systems?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are the potential consequences of algorithmic aversion in human-AI interactions?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 2, 'relevance': 2, 'score': 1.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI Actors', 'GAI system performance', 'Content provenance data tracking', 'Human-AI Configuration', 'Continual improvements in AI system updates']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 2, 'relevance': 2, 'score': 1.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Data privacy', 'Intellectual property', 'GAI risks', 'Content provenance management', 'Service level agreements (SLAs)']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['TEVV practices', 'Content provenance', 'GAI risks', 'Misinformation and disinformation', 'Deepfakes']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are the responsibilities of AI Actors in the context of Human-AI Configuration?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are the potential harms associated with GAI risks, and how can they be addressed?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about concerns related to harmful bias within AI systems. It is clear and specific in its intent, seeking information on the potential issues or risks associated with bias in AI. The question is independent and does not rely on external references or context, making it understandable and answerable based on the details provided. It effectively communicates its purpose without ambiguity.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the risks posed by adversarial attacks concerning sensitive information in GAI (General Artificial Intelligence) models. It is clear in its intent, seeking information on the specific risks associated with adversarial attacks in the context of sensitive information within GAI models. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: \"What risks do adversarial attacks pose in relation to sensitive information in GAI models?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What is the purpose of service level agreements (SLAs) in managing content ownership and usage rights?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the intellectual property risks associated with GAI (General Artificial Intelligence) systems. It is clear in its intent, seeking information on potential risks related to intellectual property in the context of GAI systems. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge. However, to enhance clarity, it could specify whether it is interested in risks related to the creation, use, or distribution of GAI systems, or any specific aspect of intellectual property (e.g., patents, copyrights).', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What are the intellectual property risks associated with GAI systems?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the significance of the fair use doctrine in relation to intellectual property risks from Generative AI (GAI) systems. It is clear in its intent, seeking to understand the role or impact of fair use within the specific context of intellectual property risks associated with GAI systems. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge about intellectual property law and GAI systems.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the potential consequences of algorithmic aversion in human-AI interactions. It is clear and specific, as it focuses on a particular phenomenon (algorithmic aversion) and its impact within a defined context (human-AI interactions). The intent is to understand the implications or outcomes of this aversion, making it straightforward and answerable with sufficient domain knowledge. No additional context or external references are required to comprehend or respond to the question.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about privacy risks related to data memorization in Generative AI (GAI) models. It is clear in its intent, seeking information on potential privacy issues that arise when GAI models memorize data. The question is specific and does not rely on external references or context, making it understandable and answerable for someone with knowledge in the field of AI and privacy. However, to enhance clarity, the question could specify whether it is interested in general risks, specific types of data, or particular use cases of GAI models.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the significance of identifying adverse behavior in AI models during the red-teaming process. It is clear and specific, as it seeks to understand the importance or impact of this identification within a particular context (red-teaming). The question does not rely on external references or unspecified contexts, making it independent and self-contained. The intent is clear, aiming for an explanation of the significance, which can be addressed with domain knowledge about AI model evaluation and red-teaming practices.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking about the purpose of AI red-teaming in the context of identifying adverse behavior in AI models. It does not rely on external references or unspecified contexts, making it independent and self-contained. The intent is clear, seeking an explanation of the role or objective of AI red-teaming in this particular area.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What is the purpose of AI red-teaming in relation to identifying adverse behavior in AI models?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the purpose of creating measurement error models for pre-deployment metrics within the context of TEVV processes. It is specific in its focus on measurement error models and pre-deployment metrics, and it clearly seeks to understand the rationale behind their creation in the TEVV (Test, Evaluation, Verification, and Validation) processes. The question is self-contained and does not rely on external references or unspecified contexts, making it clear and answerable for someone with domain knowledge in TEVV processes.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What is the purpose of creating measurement error models for pre-deployment metrics in the context of TEVV processes?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the potential consequences of incorrect presumptions regarding performance in decision-making. It is clear in its intent, seeking information on the outcomes or effects of such presumptions. The question is independent and does not rely on external references or specific contexts, making it understandable and answerable based on general knowledge of decision-making processes. To enhance clarity, the question could specify the type of performance (e.g., employee performance, system performance) or the context of decision-making (e.g., business, personal, strategic), but it is sufficiently clear as it stands.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: \"What are the potential consequences of incorrect presumptions about performance in decision-making?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for measures to handle and recover from security anomalies in GAI (General Artificial Intelligence) system architecture. It is clear in its intent, seeking specific strategies or actions related to security anomaly management and recovery within a GAI context. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge. However, to enhance clarity, it could specify whether it is interested in technical measures, policy measures, or both, and whether it focuses on prevention, detection, or response strategies.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What measures should be taken to handle and recover from security anomalies in GAI system architecture?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the key processes defined for operator and practitioner proficiency in AI development. It is clear in its intent to understand the processes related to proficiency in AI development for specific roles (operators and practitioners). However, the question could be considered somewhat broad as it does not specify which aspects of AI development (e.g., model training, deployment, ethical considerations) or which industry standards or frameworks it refers to. To improve clarity and answerability, the question could specify the context or framework within which these processes are defined, or the particular areas of proficiency it is interested in.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What are the key processes defined for operator and practitioner proficiency in AI development?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the responsibilities of AI Actors within the context of Human-AI Configuration. While it specifies the topic of interest (AI Actors and Human-AI Configuration), it lacks clarity on what 'Human-AI Configuration' specifically refers to, as this could vary widely depending on the field or context (e.g., technical setup, ethical guidelines, collaborative frameworks). To improve clarity and answerability, the question could benefit from defining or describing what is meant by 'Human-AI Configuration' or specifying the particular responsibilities or roles of interest (e.g., ethical, operational, technical).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What are the responsibilities of AI Actors in the context of Human-AI Configuration?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the potential harms associated with General Artificial Intelligence (GAI) risks and how these harms can be addressed. It is clear in its intent, seeking information on both the risks and mitigation strategies related to GAI. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge. However, it could be improved by specifying particular types of harms or areas of concern (e.g., ethical, societal, economic) to narrow down the scope and provide a more focused answer.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What are the potential harms associated with GAI risks, and how can they be addressed?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What protocols should be implemented to ensure the GAI system can effectively monitor, recover from, and document security anomalies while considering the involvement of AI actors in risk identification?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the purpose of service level agreements (SLAs) in the context of managing content ownership and usage rights. It is clear in its intent, seeking to understand the role or function of SLAs specifically related to content management. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided. It specifies the focus on SLAs and their relation to content ownership and usage rights, which is a sufficiently clear and specific inquiry.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about privacy risks associated with Generative AI (GAI) models inferring sensitive information during adversarial attacks. It is clear in its intent, specifying the context (adversarial attacks) and the focus (privacy risks and inference of sensitive information). The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge about GAI models and adversarial attacks.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What potential copyright infringements and cybersecurity vulnerabilities could arise from the use of GAI systems, particularly concerning the unauthorized use of copyrighted materials and the risks of data poisoning?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the key processes defined for operator and practitioner proficiency in AI development. It is clear in its intent to seek information about processes related to proficiency in AI development for specific roles (operators and practitioners). However, it lacks specificity regarding the context or framework within which these processes are defined, which could vary significantly across different organizations or standards. To improve clarity and answerability, the question could specify the context, such as a particular industry standard, organization, or framework, or clarify what aspects of proficiency (e.g., skills, training, evaluation) are of interest.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What role does the expertise of diverse AI red teams play in uncovering potential adverse behaviors in AI models during pre-deployment assessments?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What role do measurement error models play in validating pre-deployment metrics for TEVV processes, particularly in addressing biases and assessing risks in AI systems?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What types of harms related to GAI, such as misinformation or vulnerabilities, can be identified through regular adversarial testing, and what measures can be implemented to mitigate these risks?\"\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"What privacy risks arise from GAI models inferring sensitive info during adversarial attacks?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"The significance of the fair use doctrine in relation to intellectual property risks from GAI systems lies in determining whether the use of copyrighted works in training data is considered fair use. If a GAI system's training data includes copyrighted material and the outputs display instances of training data memorization, it could potentially infringe on copyright. The fair use doctrine is crucial in assessing these risks and the legal status of generated content that resembles copyrighted work.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the responsibilities of AI Actors within the context of Human-AI Configuration. While it specifies the topic of interest (responsibilities of AI Actors) and the context (Human-AI Configuration), it lacks clarity on what 'Human-AI Configuration' specifically refers to. This term could encompass various frameworks, models, or scenarios, making the question potentially ambiguous for those not familiar with the specific context intended by the asker. To improve clarity and answerability, the question could benefit from a brief explanation or definition of 'Human-AI Configuration', or by specifying the particular framework or scenario being referred to.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about potential biases that could result from flawed performance assumptions in decision-making. It is clear in its intent, seeking information on biases, and does not rely on external references or context, making it independent. However, the question is somewhat broad as it does not specify the type of decision-making context or the nature of the performance assumptions, which could vary widely across different fields. To improve clarity and specificity, the question could specify a particular domain (e.g., business, healthcare, education) or type of decision-making process (e.g., hiring, policy-making) to narrow down the scope of potential biases.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is clear in its intent, asking for protocols related to monitoring, recovering from, and documenting security anomalies in a GAI system, with a specific focus on the involvement of AI actors in risk identification. It specifies the context (GAI system) and the aspects of interest (monitoring, recovery, documentation, AI actors), making it understandable and answerable without needing additional context. However, it could be improved by specifying what 'GAI' stands for, as it might not be immediately clear to all readers. Overall, the question is specific and self-contained.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 2, 'relevance': 2, 'score': 1.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Incorrect presumptions about performance', 'Human-AI configuration', 'Anthropomorphizing GAI systems', 'Algorithmic aversion', 'Information integrity']\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The significance of identifying adverse behavior in AI models during the red-teaming process lies in the ability to uncover potential flaws and adverse outcomes of a GAI model or system, which can then be addressed to improve the safety and effectiveness of the AI before it is made available to the public.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI system incidents', 'Organizational risk management', 'Incident response processes', 'Remediation plan', 'Deactivation criteria']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI red teams', 'Domain expertise', 'Socio-cultural aspects', 'Organizational governance', 'AI risk management']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking about the role of diverse AI red teams in identifying potential adverse behaviors in AI models during pre-deployment assessments. It specifies the context (pre-deployment assessments) and the focus (expertise of diverse AI red teams), making the intent clear. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Data memorization in GAI models poses privacy risks by potentially leaking, generating, or correctly inferring sensitive information about individuals. This includes revealing sensitive information from the public domain that was included in their training data, which may exacerbate privacy risks even for data present only in a small number of training samples.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear in its intent, asking about potential copyright infringements and cybersecurity vulnerabilities associated with the use of Generative AI (GAI) systems. It specifically focuses on the unauthorized use of copyrighted materials and the risks of data poisoning, making it specific and independent. The question does not rely on external references or unspecified contexts, allowing for a direct and relevant response from someone with domain knowledge in copyright law and cybersecurity. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the intellectual property risks associated with GAI (General Artificial Intelligence) systems. It is clear in its intent, seeking information on potential risks related to intellectual property in the context of GAI systems. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge. However, to enhance clarity, it could specify whether it is interested in risks related to the creation, use, or distribution of GAI systems, or any specific aspect of intellectual property (e.g., patents, copyrights).', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ConditionalEvolution] simple question generated: \"What are the intellectual property risks associated with GAI systems?\"\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"What biases might arise from flawed performance assumptions in decision-making?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What protocols ensure GAI can monitor, recover, and document security issues with AI involvement?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI systems', 'Accessibility and reasonable accommodations', 'AI actor credentials', 'Alignment to organizational values', 'Data provenance']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear in its intent, asking about the role of measurement error models in the context of validating pre-deployment metrics for TEVV (Testing, Evaluation, Verification, and Validation) processes. It specifically focuses on how these models address biases and assess risks in AI systems. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge. However, it could be improved by briefly explaining what TEVV processes entail for those unfamiliar with the term, although this is not strictly necessary for those with relevant expertise.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions address the risks associated with sensitive information in GAI models, specifically focusing on adversarial attacks and privacy risks. They share a similar depth and breadth of inquiry regarding the potential threats to sensitive information.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] evolution_filter failed, retrying with 1\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How do diverse AI red teams help spot issues in models before deployment?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What copyright and cybersecurity risks might GAI systems pose?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are the potential consequences of incorrect presumptions about performance in decision-making?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What role does organizational governance play in the incorporation of AI red-teaming results?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is clear in its intent, asking about the types of harms related to General Artificial Intelligence (GAI) that can be identified through regular adversarial testing, and the measures that can be implemented to mitigate these risks. It specifies the context (GAI) and the method of identification (adversarial testing), making it understandable and answerable for someone with domain knowledge. However, it could be improved by specifying what is meant by 'regular adversarial testing' (e.g., frequency, scope) and by providing examples of 'measures' to clarify the expected type of response. Overall, the question is specific and independent.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What procedures should be established for escalating GAI system incidents to the organizational risk management authority?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI system incidents', 'Organizational risk management', 'Incident response processes', 'Remediation plan', 'Deactivation criteria']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI incidents', 'Legal and regulatory requirements', 'HIPAA breach reporting', 'NHTSA autonomous vehicle crash reporting', 'Information Security and Data Privacy']\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How do measurement error models help validate pre-deployment metrics in TEVV for AI risks and biases?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 2, 'score': 2.0}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Information Security', 'AI Development', 'Operator and practitioner proficiency', 'GAI risks', 'Digital content transparency']\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What harms from GAI, like misinformation, can regular adversarial testing reveal, and how can we mitigate them?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What is the significance of data provenance in the context of GAI systems?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI system validity', 'Reliability in testing', 'Measurement gaps', 'Laboratory vs. real-world settings', 'Benchmark test datasets']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are the legal and regulatory requirements for reporting GAI incidents?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks about the consequences of incorrect presumptions, while the second question focuses on biases that could arise from faulty assumptions. Although related, they differ in scope and depth, with one focusing on consequences and the other on biases.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are the specific criteria that warrant the deactivation of GAI systems according to the established procedures?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question broadly addresses intellectual property risks, while the second question specifically focuses on copyright and cybersecurity risks, indicating different constraints and depth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on measures for handling and recovering from security anomalies, while the second question emphasizes protocols for monitoring, recovering, and documenting security issues. Although related, they differ in scope and focus, leading to different depths of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the role of organizational governance in the incorporation of AI red-teaming results. It is clear in its intent, seeking to understand the influence or function of governance structures in the context of AI red-teaming. The question is specific and does not rely on external references or unspecified contexts, making it independent and self-contained. It is understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What role does organizational governance play in the incorporation of AI red-teaming results?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the purpose of AI red-teaming specifically in identifying adverse behavior, while the second question is about the role of diverse AI red teams in spotting issues before deployment. The scope and focus differ, leading to different depths and breadths of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the potential consequences of incorrect presumptions regarding performance in decision-making. It is clear in its intent, seeking information on the outcomes or effects of such presumptions. The question is independent and does not rely on external references or specific contexts, making it understandable and answerable based on general knowledge of decision-making processes. To enhance clarity, the question could specify the type of performance (e.g., employee performance, system performance) or the context of decision-making (e.g., business, personal, strategic), but it is sufficiently clear as it stands.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What are the potential consequences of incorrect presumptions about performance in decision-making?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What is the significance of digital content transparency in the context of information integrity?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the procedures that should be established for escalating GAI (General Artificial Intelligence) system incidents to an organizational risk management authority. It is clear in its intent, seeking specific procedural guidelines for incident escalation. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge. However, to enhance clarity, it could specify the types of incidents or the context in which these procedures are to be applied, such as the industry or specific organizational settings.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ConditionalEvolution] simple question generated: \"What procedures should be established for escalating GAI system incidents to the organizational risk management authority?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks about the purpose of creating measurement error models, while the second question focuses on how these models help validate metrics specifically for AI risks and biases. The scope and focus differ, leading to different depths and requirements.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What challenges are associated with ensuring reliability in testing for GAI systems?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear in its intent, asking about the intellectual property risks associated with Generative AI (GAI) systems using copyrighted material without fair use, with a focus on data memorization and identity emulation. It specifies the context (GAI systems and copyrighted material) and the particular concerns (data memorization and identity emulation), making it understandable and answerable for someone with knowledge in intellectual property law and AI. The question does not rely on external references or unspecified contexts, thus meeting the criteria for independence and clear intent.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What significance does the expertise and diversity of AI red teams hold in shaping organizational governance and decision-making processes regarding the integration of their findings?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the significance of data provenance specifically in the context of GAI (General Artificial Intelligence) systems. It is clear in its intent, seeking an explanation or discussion on the importance or impact of data provenance within this specific domain. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge about GAI systems and data provenance.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks about potential harms associated with GAI risks in general and how they can be addressed, while the second question specifically focuses on harms like misinformation that can be revealed through adversarial testing and how to mitigate them. The second question has a narrower scope and specific method mentioned, leading to different depths and breadths of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What adverse effects might arise from flawed assumptions regarding performance in decision-making processes, particularly in relation to biases and human-AI interactions?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the legal and regulatory requirements for reporting GAI (General Artificial Intelligence) incidents. It is clear in its intent, seeking specific information about legal and regulatory frameworks. However, the term 'GAI incidents' might be ambiguous without further context, as it could refer to a wide range of scenarios involving artificial intelligence. To improve clarity and answerability, the question could specify the type of incidents (e.g., data breaches, ethical violations) and the jurisdiction or industry context (e.g., healthcare, finance) it is concerned with, as legal and regulatory requirements can vary significantly across different regions and sectors.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What are the legal and regulatory requirements for reporting GAI incidents?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks for specific criteria that justify the deactivation of GAI systems according to established procedures. It is clear in its intent, seeking detailed information about the criteria and procedures related to the deactivation of GAI systems. However, the question assumes familiarity with 'GAI systems' and 'established procedures' without providing context or definitions for these terms. To improve clarity and answerability, the question could include a brief explanation of what GAI systems are and what the established procedures refer to, or specify the context in which these procedures are applied (e.g., a particular organization or industry).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What are the specific criteria that warrant the deactivation of GAI systems according to the established procedures?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Faulty performance assumptions can lead to erroneous outputs, ill-founded decision-making, and the amplification of harmful biases.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ConditionalEvolution] question compressed: \"What intellectual property risks arise if GAI systems utilize copyrighted material without fair use, particularly regarding data memorization and identity emulation?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the significance of digital content transparency in relation to information integrity. It is clear in its intent, seeking an explanation of the importance or impact of transparency on maintaining the integrity of information. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge. However, to enhance clarity, the question could specify particular aspects of digital content transparency or information integrity it is interested in, such as accuracy, trustworthiness, or verification processes.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 2, 'score': 1.25}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 0 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the challenges associated with ensuring reliability in testing for GAI (General Artificial Intelligence) systems. It is clear in its intent, seeking information on the specific difficulties or obstacles faced in this area. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge. The use of 'GAI systems' is specific enough for those familiar with the field of artificial intelligence, and the focus on 'ensuring reliability in testing' provides a clear direction for the response.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: \"What challenges are associated with ensuring reliability in testing for GAI systems?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Diverse AI red teams help spot issues in models before deployment by bringing a range of backgrounds and expertise, which allows them to identify flaws in the varying contexts where General AI (GAI) will be used. Their demographic and interdisciplinary diversity enhances the identification of potential adverse behaviors or outcomes, ensuring a more comprehensive evaluation of the AI models.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"GAI systems may pose copyright risks if their training data includes copyrighted material, leading to potential infringement through outputs that display memorization of training data. Additionally, there are cybersecurity risks such as indirect prompt injections that can exploit vulnerabilities, steal proprietary data, or run malicious code. Data poisoning is another risk, where an adversary compromises a training dataset to manipulate a model's outputs or operation.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the criteria required for escalating GAI (General Artificial Intelligence) system incidents to risk management, specifically when deactivation is considered necessary. It is clear in its intent, seeking specific criteria for escalation and involving risk management processes. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge about GAI systems and risk management protocols.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear in its intent, asking about the adverse effects of flawed assumptions in decision-making processes, with a focus on biases and human-AI interactions. It is specific in identifying the areas of interest (biases and human-AI interactions) and seeks information on potential negative outcomes. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The context does not provide information on how measurement error models help validate pre-deployment metrics in TEVV for AI risks and biases.', 'verdict': -1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Information integrity', 'High-integrity information', 'Trustworthy information', 'Accurate and reliable', 'Chain of custody']\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the legal and regulatory requirements for reporting GAI (General Artificial Intelligence) incidents. It is clear in its intent, seeking specific information about legal and regulatory frameworks. However, the term 'GAI incidents' might be ambiguous without further context, as it could refer to various types of incidents involving artificial intelligence. To improve clarity and answerability, the question could specify the type of incidents (e.g., data breaches, ethical violations) and the jurisdiction or industry context (e.g., healthcare, finance) for which the requirements are sought.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the significance of the expertise and diversity of AI red teams in influencing organizational governance and decision-making processes, particularly in relation to integrating their findings. It is clear in its intent, specifying the focus on expertise, diversity, and the impact on governance and decision-making. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks for specific criteria that justify the deactivation of GAI systems according to established procedures. It is clear in its intent, seeking detailed information about the criteria and procedures related to the deactivation of GAI systems. However, the question assumes familiarity with 'GAI systems' and 'established procedures' without providing any context or definitions. To improve clarity and answerability, the question could benefit from a brief explanation of what GAI systems are and a reference to the specific procedures or guidelines being referred to, if applicable.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The protocols ensuring GAI can monitor, recover, and document security issues with AI involvement include verifying that the GAI system architecture can monitor outputs and performance, handle, recover from, and repair errors when security anomalies, threats, and impacts are detected. Additionally, it involves including relevant AI Actors in the GAI system risk identification process and ensuring that downstream GAI system impacts, such as the use of third-party plugins, are included in the impact documentation process.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ConditionalEvolution] question compressed: \"What criteria must be met for escalating GAI system incidents to risk management if deactivation is deemed necessary?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What issues could stem from poor assumptions in decision-making, especially with biases and human-AI interactions?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Harmful bias', 'Generated content', 'Information security', 'Misinformation analysis', 'Dangerous content']\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question broadly asks about intellectual property risks associated with GAI systems, while the second question specifies particular IP risks related to the use of copyrighted material, fair use, data memorization, and identity emulation, indicating a different depth and breadth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How do AI red teams' expertise and diversity impact governance and decision-making?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The context mentions that potential content provenance harms of GAI include misinformation or disinformation, deepfakes, and tampered content. Regular adversarial testing can help identify vulnerabilities and understand potential misuse scenarios and unintended outputs, which can aid in mitigating these harms by mapping and measuring GAI risks.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What characteristics define information that is considered accurate and reliable?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI incidents', 'Legal and regulatory requirements', 'HIPAA breach reporting', 'NHTSA autonomous vehicle crash reporting', 'Information Security and Data Privacy']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 0 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 0 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 2, 'score': 2.0}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI-generated content', 'Feedback mechanisms', 'Real-time auditing tools', 'Information integrity', 'Synthetic data']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI systems', 'Accessibility and reasonable accommodations', 'AI actor credentials', 'Alignment to organizational values', 'Data provenance']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are the concerns associated with harmful bias in generated content?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks for procedures for escalation, while the second question focuses on criteria for escalation specifically when deactivation is needed. They differ in scope and requirements.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are the requirements for reporting incidents related to NHTSA autonomous vehicle crash reporting?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 0 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the factors contributing to measurement gaps affecting the reliability of General Artificial Intelligence (GAI) in real-world versus laboratory settings. It is clear in its intent, seeking an explanation of the differences in reliability due to measurement gaps between these two environments. The question is specific and does not rely on external references, making it understandable and answerable with sufficient domain knowledge. However, it could be improved by specifying what aspects of GAI reliability are of interest (e.g., accuracy, consistency, robustness) to provide a more focused response.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 2, 'score': 1.25}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 0 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the characteristics that define information as accurate and reliable. It is clear in its intent, seeking specific attributes or criteria that contribute to the accuracy and reliability of information. The question is independent and does not rely on external references or context, making it understandable and answerable based on general knowledge about information quality. No improvements are necessary as the question is well-structured and clear.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Generative AI Public Working Group', 'Stakeholder feedback', 'GAI risk management', 'Governance', 'Content Provenance']\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the consequences of incorrect presumptions about performance in decision-making, while the second question specifically addresses issues from poor assumptions, with an emphasis on biases and human-AI interactions. The second question has a broader scope and additional constraints, leading to different depths of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What role do accessibility and reasonable accommodations play in the application of systems to GAI systems?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What measures can be taken to ensure information integrity in AI-generated content?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 2, 'relevance': 2, 'score': 1.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI resources', 'Organizational risk tolerances', 'Third-party models', 'Value chain risks', 'Data privacy and information security']\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"What factors contribute to measurement gaps affecting GAI reliability in real-world vs lab settings?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the role of organizational governance in incorporating AI red-teaming results, while the second question explores how the expertise and diversity of AI red teams impact governance and decision-making. These questions have different focuses and depths of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Mapping AI technology', 'Legal risks', 'Third-party data', 'Intellectual property infringement', 'Data privacy']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI system security features', 'Content provenance methods', 'User satisfaction surveys', 'Content authenticity perceptions', 'Effectiveness of security measures']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the requirements for reporting incidents related to NHTSA autonomous vehicle crash reporting. It is clear in its intent, specifying the topic of interest (NHTSA autonomous vehicle crash reporting) and the type of information sought (reporting requirements). The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about concerns associated with harmful bias in generated content. It is clear and specific, seeking information on the potential issues or risks related to bias in content generated by AI or other automated systems. The question is independent and does not rely on external references or context, making it understandable and answerable based on the details provided. To further enhance clarity, the question could specify the type of generated content (e.g., text, images) or the context in which the bias is being considered (e.g., social media, news articles), but it is sufficiently clear as is.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What role does governance play in the context of the Generative AI Public Working Group (GAI PWG)?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What considerations should be taken into account when applying organizational risk tolerances to third-party models?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Intellectual property risks from GAI systems may arise where the use of copyrighted works is not a fair use under the fair use doctrine. If a GAI system’s training data included copyrighted material, GAI outputs displaying instances of training data memorization could infringe on copyright. Additionally, there are discussions regarding the use or emulation of personal identity, likeness, or voice without permission.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['CBRN weapons', 'Biological and chemical threat knowledge', 'LLMs analysis and synthesis', 'Attack planning', 'Operational likelihood']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about measures to ensure information integrity in AI-generated content. It is clear and specific, focusing on strategies or actions that can be implemented to maintain the accuracy and reliability of content produced by AI systems. The question is independent and does not rely on external references or context, making it understandable and answerable based on the details provided. The intent is clear, seeking practical solutions or methods to address a well-defined issue.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What measures can be taken to ensure information integrity in AI-generated content?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The specific criteria for escalating GAI system incidents to the organizational risk management authority when deactivation or disengagement is needed are not detailed in the provided context.', 'verdict': -1}\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the challenges of ensuring reliability in testing GAI systems, while the second question specifically addresses the causes of measurement gaps in GAI reliability between real-world and lab settings. These questions have different constraints and depths of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What processes should be implemented to respond to potential intellectual property infringement claims?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the role of accessibility and reasonable accommodations in the application of systems to GAI (General Artificial Intelligence) systems. While it specifies the focus on accessibility and accommodations, the question is somewhat unclear due to the ambiguous reference to 'systems' and 'GAI systems'. It is not clear what specific systems are being referred to or how they relate to GAI. To improve clarity and answerability, the question could specify the type of systems (e.g., software, hardware) and the context in which they are applied to GAI. Additionally, clarifying what is meant by 'reasonable accommodations' in this context would help in providing a more precise answer.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What role do accessibility and reasonable accommodations play in the application of systems to GAI systems?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What methods can be used to analyze user perceptions of content authenticity?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What role do LLMs play in the analysis and synthesis of information related to biological threats?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI system security', 'Vulnerabilities and threats', 'Data breaches', 'Information security', 'Content provenance']\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What strategies can be employed to enhance the authenticity and societal alignment of AI-generated content through user feedback and auditing?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Poor assumptions in decision-making can lead to erroneous outputs, ill-founded decision-making, and the amplification of harmful biases. Additionally, human-AI interactions may result in inappropriate anthropomorphizing of AI systems, algorithmic aversion, automation bias, over-reliance, or emotional entanglement with these systems.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Information that is considered accurate and reliable is high-integrity information that can be trusted, distinguishes fact from fiction, opinion, and inference, acknowledges uncertainties, and is transparent about its level of vetting. It can be linked to the original source(s) with appropriate evidence, is verifiable and authenticatable, has a clear chain of custody, and creates reasonable expectations about when its validity may expire.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the role of governance within the context of the Generative AI Public Working Group (GAI PWG). It is clear in specifying the topic of interest (governance) and the context (GAI PWG), making the intent understandable. However, it assumes familiarity with the GAI PWG without providing any background or context about what it is or its objectives. To improve clarity and answerability, the question could include a brief description of the GAI PWG or specify the aspects of governance it is interested in (e.g., decision-making processes, policy development).', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What role does governance play in the context of the Generative AI Public Working Group (GAI PWG)?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The concerns associated with harmful bias in generated content include the potential for misinformation, the presence of obscene, degrading, and abusive content, and the risk of dangerous, violent, or hateful content.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What measures are suggested to evaluate and enhance AI system security and resilience?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The context mentions that AI red teams should demonstrate domain expertise and awareness of socio-cultural aspects, which are important for the quality of AI red-teaming outputs. Additionally, it states that demographically and interdisciplinarily diverse AI red teams can be used to identify flaws, suggesting that their expertise and diversity positively impact governance and decision-making.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the processes that should be implemented to respond to potential intellectual property infringement claims. It is clear in its intent, seeking specific procedural information related to handling such claims. The question is independent and does not rely on external references or context, making it understandable and answerable based on the details provided. It could be improved by specifying the type of intellectual property (e.g., patents, trademarks, copyrights) if the question aims to address a particular area, but it is sufficiently clear as it stands.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about methods for analyzing user perceptions of content authenticity. It is clear in its intent, seeking information on analytical methods, and does not rely on external references or unspecified contexts. The question is specific enough to be understood and answered by someone with domain knowledge in user perception analysis or content authenticity. To further enhance clarity, the question could specify the type of content (e.g., social media, news articles) or the context in which authenticity is being assessed, but it is still sufficiently clear and answerable as is.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the role of Large Language Models (LLMs) in the analysis and synthesis of information related to biological threats. It is clear in specifying the subject of interest (LLMs) and the context (biological threats), and it seeks information on their role, making the intent clear. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: \"What role do LLMs play in the analysis and synthesis of information related to biological threats?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the role of accessibility and reasonable accommodations in the application of systems to GAI (General Artificial Intelligence) systems. While it specifies the focus on accessibility and accommodations, the question is somewhat ambiguous due to the lack of clarity on what 'systems' refers to and how they relate to GAI systems. Additionally, the term 'application of systems to GAI systems' is vague and could benefit from further specification. To improve clarity and answerability, the question could specify the types of systems being referred to (e.g., software, hardware, organizational processes) and clarify the context or scenarios in which these systems are applied to GAI systems.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear in its intent, asking for considerations when applying organizational risk tolerances to third-party models. It specifies the context (organizational risk tolerances and third-party models) and seeks information on considerations, making it understandable and answerable. The question does not rely on external references or unspecified contexts, thus meeting the criteria for independence. However, to enhance clarity, it could specify the type of third-party models (e.g., financial, AI models) or the industry context, if relevant.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is clear in its intent, asking for strategies to improve the authenticity and societal alignment of AI-generated content using user feedback and auditing. It specifies the focus on 'authenticity' and 'societal alignment', and the methods of interest ('user feedback' and 'auditing'), making it understandable and answerable without needing additional context. The question is self-contained and does not rely on external references, meeting the criteria for independence and clear intent.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Measurement gaps in GAI reliability arise from mismatches between laboratory and real-world settings. Current testing approaches often focus on laboratory conditions or are restricted to benchmark test datasets and in silico techniques, which may not extrapolate well to or directly assess GAI impacts in real-world conditions.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the role of governance within the Generative AI Public Working Group (GAI PWG). It is clear in specifying the topic of interest (governance) and the context (GAI PWG), making the intent understandable. However, it assumes familiarity with the GAI PWG without providing any background or context about what this group is or its objectives. To improve clarity and answerability, the question could include a brief description of the GAI PWG or specify particular aspects of governance it is interested in (e.g., decision-making processes, policy development).', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for measures to evaluate and enhance AI system security and resilience. It is clear in its intent, seeking specific strategies or methods related to AI security and resilience. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge. However, to further improve clarity, the question could specify the type of AI systems or the context in which these measures are to be applied (e.g., enterprise systems, consumer applications).', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What measures are suggested to evaluate and enhance AI system security and resilience?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How can user feedback improve AI content authenticity?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['TEVV metrics', 'Measurement error models', 'Construct validity', 'Hateful content', 'AI Deployment and Monitoring']\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Processes for responding to potential intellectual property infringement claims should be implemented as suggested in the action MP-4.1-002, which involves addressing such claims or other rights.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What actions are recommended to assess and improve the security and resilience of AI systems while ensuring ongoing effectiveness against identified vulnerabilities?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Human-AI Configuration', 'Obscene and Abusive Content', 'Harmful Bias', 'Data Privacy', 'Intellectual Property']\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'User surveys can be conducted to gather user satisfaction with AI-generated content and user perceptions of content authenticity. Analyzing user feedback can help identify concerns and current literacy levels related to content provenance and understanding of labels on content.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on measures for ensuring information integrity in AI-generated content, while the second question is about using user feedback to improve AI content authenticity. These questions have different focuses and requirements.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for insights that large language models (LLMs) provide in analyzing biological threats compared to traditional search engines. It is clear in specifying the topic of interest (LLMs vs. traditional search engines) and the context (analyzing biological threats). The intent is to understand the comparative advantages or unique insights offered by LLMs in this specific application. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What considerations should be taken into account when measuring hateful content in the context of TEVV metrics?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are the considerations involved in Human-AI Configuration regarding harmful content?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The considerations that should be taken into account when applying organizational risk tolerances to third-party models include reassessing risk measurements after fine-tuning third-party GAI models, testing GAI system value chain risks such as data poisoning, malware, software and hardware vulnerabilities, labor practices, data privacy and localization compliance, and geopolitical alignment.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"What insights do LLMs offer for analyzing biological threats compared to traditional search engines?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear in its intent, asking for recommended actions to assess and improve the security and resilience of AI systems while maintaining effectiveness against vulnerabilities. It specifies the focus on security, resilience, and effectiveness, making it understandable and answerable for someone with domain knowledge in AI security. The question is self-contained and does not rely on external references or unspecified contexts, meeting the criteria for independence.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What steps can enhance AI security and resilience?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about considerations for measuring hateful content using TEVV metrics. It specifies the context (TEVV metrics) and the subject (hateful content), making the intent relatively clear. However, it assumes familiarity with 'TEVV metrics' without explaining what they are or providing any context about them. To improve clarity and answerability, the question could include a brief description of TEVV metrics or specify the aspects of hateful content measurement it is interested in (e.g., accuracy, bias, context sensitivity).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What considerations should be taken into account when measuring hateful content in the context of TEVV metrics?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the role of LLMs in analyzing and synthesizing information about biological threats, while the second question compares what LLMs reveal about bio threats to search engines. The scope and requirements differ, leading to different depths and breadths of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the considerations involved in Human-AI Configuration concerning harmful content. It is clear in its intent to understand the factors or considerations in this specific context. However, the term 'Human-AI Configuration' is somewhat broad and could benefit from further specification or context to ensure clarity. Additionally, 'harmful content' could refer to various types of content across different platforms or applications, so specifying the context or type of harmful content could improve the question's clarity and answerability. Overall, the question is mostly clear but could be more specific.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'User feedback can improve AI content authenticity by using structured feedback mechanisms to solicit and capture user input about AI-generated content, which helps detect subtle shifts in quality or alignment with community and societal values. Additionally, real-time auditing tools can aid in tracking and validating the lineage and authenticity of AI-generated data.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions focus on improving AI system security and resilience, asking for measures or steps to achieve this. They share the same depth and breadth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about considerations for measuring hateful content using TEVV metrics. It specifies the context (TEVV metrics) and the subject (hateful content), making the intent relatively clear. However, it assumes familiarity with 'TEVV metrics' without explaining what they are or providing any context about them. To improve clarity and answerability, the question could include a brief description of TEVV metrics or specify the aspects of hateful content measurement it is interested in (e.g., accuracy, bias, context sensitivity).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Ongoing monitoring', 'Risk management process', 'Organizational roles and responsibilities', 'Periodic review', 'GAI systems incident response']\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'LLMs provide minimal assistance regarding biological threat creation and attack planning compared to traditional search engine queries, suggesting that they do not substantially increase the operational likelihood of such an attack.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI Risk Management Framework', 'Generative AI', 'Trustworthiness considerations', 'Executive Order 14110', 'Implementation profile']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What is the significance of defining organizational roles and responsibilities in the risk management process?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What is the purpose of the AI Risk Management Framework (AI RMF) for Generative AI?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Considerations involved in Human-AI Configuration regarding harmful content include assessing the existence or levels of harmful bias, intellectual property infringement, data privacy violations, obscenity, extremism, violence, or CBRN information in system training data. Additionally, it involves re-evaluating safety features of fine-tuned models when the negative risk exceeds organizational risk tolerance, particularly concerning dangerous, violent, or hateful content.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the significance of defining organizational roles and responsibilities within the risk management process. It is clear in its intent, seeking an explanation of the importance or impact of this aspect of risk management. The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided. It is specific enough to allow for a focused response on the role of organizational structure in risk management.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What is the significance of defining organizational roles and responsibilities in the risk management process?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What role do clearly defined responsibilities play in the ongoing oversight and evaluation of GAI systems within the risk management framework?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the purpose of the AI Risk Management Framework (AI RMF) specifically for Generative AI. It is clear and specific, as it identifies the framework of interest (AI RMF) and the context (Generative AI), and seeks information about its purpose. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ConditionalEvolution] simple question generated: \"What is the purpose of the AI Risk Management Framework (AI RMF) for Generative AI?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the role of clearly defined responsibilities in the oversight and evaluation of GAI (General Artificial Intelligence) systems within a risk management framework. It is specific in its focus on responsibilities, oversight, evaluation, and the context of risk management, making the intent clear. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable with sufficient domain knowledge. However, it could be improved by briefly explaining what is meant by 'GAI systems' for those unfamiliar with the term, although this does not significantly impact its clarity or answerability.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How do clear roles impact GAI oversight in risk management?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the role of the AI RMF (Risk Management Framework) in ensuring trustworthiness for Generative AI, particularly in the context of voluntary adoption by organizations. It is clear in specifying the framework (AI RMF) and the technology of interest (Generative AI), and it seeks to understand the impact of voluntary adoption on trustworthiness. The question is self-contained and does not rely on external references, making it understandable and answerable with sufficient domain knowledge about AI RMF and Generative AI. However, to enhance clarity, the question could briefly define or describe what AI RMF entails, as not all readers may be familiar with this framework.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the general significance of defining roles and responsibilities in the risk management process, while the second question specifically addresses the impact of clear roles on GAI oversight within risk management. These questions differ in both scope and specific focus.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] [ConditionalEvolution] question compressed: \"What role does the AI RMF play in ensuring trustworthiness for Generative AI, especially if organizations choose to adopt it voluntarily?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks about the purpose of the AI RMF for Generative AI, while the second question focuses on how the AI RMF supports trust in its adoption. These questions have different focuses and depths of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Clear roles and responsibilities are essential for effective oversight of GAI systems in risk management, as they help define and differentiate the responsibilities for human-AI configurations and ensure that policies and procedures are in place to bolster oversight through independent evaluations or assessments proportional to identified risks.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n"
     ]
    }
   ],
   "source": [
    "# re-chunk the data using a different size, then generate the synthetic test set\n",
    "importlib.reload(vanilla_rag)\n",
    "for pdf in PDFS:\n",
    "    ragas_chunks = await vanilla_rag.load_and_chunk_pdf(pdf,RAGAS_CHUNK_SIZE,RAGAS_OVERLAP)\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(ragas_chunks, N_EVAL_QUESTIONS, distributions, with_debugging_logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Generating the test data costs money, time, and compute, so make sure to save it for later re-use\n",
    "test_df = testset.to_pandas().to_csv(TEST_DATASET_FILE,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Evaluate baseline RAG system**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  13%|█▎        | 4/30 [00:20<02:15,  5.23s/it]"
     ]
    }
   ],
   "source": [
    "# Load the dataset and run the RAG pipeline\n",
    "import pandas as pd\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "test_df = pd.read_csv(TEST_DATASET_FILE)\n",
    "\n",
    "test_questions = test_df[\"question\"].to_list()\n",
    "test_gt = test_df[\"ground_truth\"].to_list()\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for question in tqdm_asyncio(test_questions,desc=\"Processing Questions\"):\n",
    "  response = await rag_chain.ainvoke({\"input\" : question})\n",
    "  answers.append(response[\"response\"].content)\n",
    "  contexts.append([context.page_content for context in response[\"context\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13b2f4165dc426ab59214dc7c3056f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Put in huggingface dataset format\n",
    "response_dataset = Dataset.from_dict({\n",
    "    \"question\" : test_questions,\n",
    "    \"answer\" : answers,\n",
    "    \"contexts\" : contexts,\n",
    "    \"ground_truth\" : test_gt\n",
    "})\n",
    "response_dataset.save_to_disk(f\"baseline_response_dataset_{N_EVAL_QUESTIONS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3355186ad046449a9667bbc5ffcde743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[377]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[383]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[397]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[399]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[332]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[328]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[390]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[324]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[326]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[380]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[317]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[319]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[330]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[334]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[381]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[329]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[398]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[389]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[331]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[318]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[378]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[321]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[340]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[395]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[333]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[339]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[393]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[392]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[388]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[325]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[341]: APIConnectionError(Connection error.)\n",
      "Exception raised in Job[345]: APIConnectionError(Connection error.)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qk/rnr0wf6j3xj452rpryymzb200000gs/T/ipykernel_28902/4040212259.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mfaithfulness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_relevancy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_recall\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/ragas/_analytics.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIsCompleteEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_completed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIsCompleteEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_completed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/ragas/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, token_usage_parser, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;31m# get the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mExceptionInRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/ragas/executor.py\u001b[0m in \u001b[0;36mresults\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_aresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0msorted_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_destroy_pending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 scheduled[0]._when - self.time(), 0), 86400) if scheduled\n\u001b[1;32m    114\u001b[0m             else None)\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mevent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m                 \u001b[0mkev_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use ragas to evaluate\n",
    "from datasets import load_from_disk\n",
    "response_dataset = load_from_disk(\"baseline_response_dataset\")\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall\n",
    ")\n",
    "\n",
    "metrics = [ faithfulness, answer_relevancy, context_precision, context_recall ]\n",
    "results = evaluate(response_dataset, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': nan, 'answer_relevancy': nan, 'context_precision': nan, 'context_recall': nan}\n"
     ]
    }
   ],
   "source": [
    "# Check out the results\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
